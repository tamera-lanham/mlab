{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch as t\n",
    "import torchvision\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EleutherAI/gpt-j-6B were not used when initializing GPTJForSequenceClassification: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing GPTJForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTJForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPTJForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-j-6B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "gpt_model = transformers.AutoModelForSequenceClassification.from_pretrained(\"EleutherAI/gpt-j-6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the class\n",
    "# Have self.model = model\n",
    "# self.forward\n",
    "\n",
    "\n",
    "class SplitModelWrapper(t.nn.Module):\n",
    "    def __init__(self, s_model):\n",
    "        super().__init__()\n",
    "        self.model = s_model\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        print(self.model)\n",
    "        result = self.model(inputs)\n",
    "        print(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpus = 2\n",
    "\n",
    "def split_model(model, n_gpus):\n",
    "    starts = t.linspace(0, 28, n_gpus + 1).int()[:-1] # Starting index of each section\n",
    "    ends = t.linspace(0, 28, n_gpus + 1).int()[1:]\n",
    "    gpt_block_sections = [model.transformer.h[start:end] for start, end in zip(starts, ends)]\n",
    "\n",
    "    first = t.nn.Sequential(\n",
    "        model.transformer.wte,\n",
    "        model.transformer.drop,\n",
    "        gpt_block_sections[0]\n",
    "    )\n",
    "\n",
    "    last = t.nn.Sequential(\n",
    "        gpt_block_sections[-1],\n",
    "        model.transformer.ln_f,\n",
    "        model.score\n",
    "    )\n",
    "\n",
    "    models = [first] + gpt_block_sections[1:-1] + [last]\n",
    "    return models\n",
    "\n",
    "models = split_model(gpt_model, n_gpus)\n",
    "for i, model in enumerate(models):\n",
    "    t.save(model, 'gpt-j-%d.pt' % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "391482d31be846a79a4e3505636a9e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/619 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54400e5c6120471ebac3f47b7fcc25ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/779k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5780f854916744178ca08327edcdc645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8e672bbad54be2aeffa62d802cb2ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636eb96aa7ee47e69ef0959de350de42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa970b2562354ac785ba8ce816631351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/357 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = t.Tensor(tokenizer(\"Should Tamera refactor the code? Answer: \")[\"input_ids\"]).int()\n",
    "inputs = inputs.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Embedding(50400, 4096)\n",
      "  (1): Dropout(p=0.0, inplace=False)\n",
      "  (2): ModuleList(\n",
      "    (0): GPTJBlock(\n",
      "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPTJAttention(\n",
      "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      )\n",
      "      (mlp): GPTJMLP(\n",
      "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): GPTJBlock(\n",
      "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPTJAttention(\n",
      "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      )\n",
      "      (mlp): GPTJMLP(\n",
      "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): GPTJBlock(\n",
      "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPTJAttention(\n",
      "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      )\n",
      "      (mlp): GPTJMLP(\n",
      "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): GPTJBlock(\n",
      "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPTJAttention(\n",
      "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      )\n",
      "      (mlp): GPTJMLP(\n",
      "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): GPTJBlock(\n",
      "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPTJAttention(\n",
      "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      )\n",
      "      (mlp): GPTJMLP(\n",
      "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): GPTJBlock(\n",
      "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPTJAttention(\n",
      "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      )\n",
      "      (mlp): GPTJMLP(\n",
      "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): GPTJBlock(\n",
      "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPTJAttention(\n",
      "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      )\n",
      "      (mlp): GPTJMLP(\n",
      "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): GPTJBlock(\n",
      "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPTJAttention(\n",
      "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      )\n",
      "      (mlp): GPTJMLP(\n",
      "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): GPTJBlock(\n",
      "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPTJAttention(\n",
      "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      )\n",
      "      (mlp): GPTJMLP(\n",
      "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): GPTJBlock(\n",
      "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPTJAttention(\n",
      "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      )\n",
      "      (mlp): GPTJMLP(\n",
      "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): GPTJBlock(\n",
      "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPTJAttention(\n",
      "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      )\n",
      "      (mlp): GPTJMLP(\n",
      "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): GPTJBlock(\n",
      "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPTJAttention(\n",
      "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      )\n",
      "      (mlp): GPTJMLP(\n",
      "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (12): GPTJBlock(\n",
      "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPTJAttention(\n",
      "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      )\n",
      "      (mlp): GPTJMLP(\n",
      "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (13): GPTJBlock(\n",
      "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPTJAttention(\n",
      "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      )\n",
      "      (mlp): GPTJMLP(\n",
      "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-a0eee171d197>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtorso\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSplitModelWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-46e0bf8696c3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \"\"\"\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torso = SplitModelWrapper(models[0])\n",
    "torso(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Embedding(50400, 4096)\n",
       "  (1): Dropout(p=0.0, inplace=False)\n",
       "  (2): ModuleList(\n",
       "    (0): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (12): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (13): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_output = gpt_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3227, 1.1226]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_model = t.nn.Sequential(*models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-944be768fc85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mour_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mour_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \"\"\"\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "our_output = our_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0010, -0.0453,  0.0014,  ...,  0.0157, -0.0010,  0.0138],\n",
       "         [-0.0095, -0.0071, -0.0048,  ...,  0.0296,  0.0011,  0.0059],\n",
       "         [-0.0159, -0.0139, -0.0133,  ..., -0.0032, -0.0170,  0.0362],\n",
       "         ...,\n",
       "         [-0.0242, -0.0049,  0.0158,  ...,  0.0052,  0.0274, -0.0107],\n",
       "         [ 0.0019,  0.0064,  0.0051,  ...,  0.0023, -0.0058,  0.0081],\n",
       "         [ 0.0013, -0.0045, -0.0043,  ...,  0.0041,  0.0029, -0.0003]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_model.transformer.wte(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2dp.gin        pipelineparallel.gin                   pp_nominibatch.py\n",
      "2dparallel.py  pipelineparallel.py                    save_model.py\n",
      "dist_basic.py  pipelineparallel_gptj_imdb.gin         w3d1.ipynb\n",
      "gpt-j-0.pt     pipelineparallel_resnet50_cifar10.gin\n",
      "gpt-j-1.pt     pp_naive.gin\n"
     ]
    }
   ],
   "source": [
    "def init_processes(rank, size, fn, backend='nccl'):\n",
    "    \"\"\" Initialize the distributed environment. \"\"\"\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "    print('I am initiating the process at:', rank, ' of ', size)\n",
    "    fn(rank, size)\n",
    "    \n",
    "def main():\n",
    "    size = n_gpus\n",
    "    processes = []\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "    for rank in range(size):\n",
    "        p = mp.Process(target=init_processes, args=(rank, size, run))\n",
    "        print(\"I am starting!\", rank, size)\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    for p in processes:\n",
    "        p.join()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
