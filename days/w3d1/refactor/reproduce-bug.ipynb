{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b00bf707-6037-4dcf-b966-569ee99e0ed2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [128, 16]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gg/8gtq763d1dx59dkqcq87mlh00000gn/T/ipykernel_47417/2749878056.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/mlab/env/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/mlab/env/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [128, 16]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "\n",
    "model = t.nn.Sequential(t.nn.Linear(16, 128), t.nn.ReLU(), t.nn.Linear(128, 16))\n",
    "\n",
    "batch_size, n_batches = 10, 2\n",
    "data_batches = [t.randn((batch_size, 16)) for _ in range(n_batches)]\n",
    "\n",
    "############### Forward pass ###############\n",
    "\n",
    "stored_outputs = {}\n",
    "\n",
    "for i, inputs in enumerate(data_batches):\n",
    "    outputs = model(inputs)\n",
    "    stored_outputs[i] = outputs\n",
    "\n",
    "############## Backwards pass ##############\n",
    "\n",
    "loss_fn = t.nn.MSELoss()\n",
    "optimizer = t.optim.SGD(model.parameters(), 0.1)\n",
    "\n",
    "for i, targets in enumerate(data_batches):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs = stored_outputs[i]\n",
    "\n",
    "    loss = loss_fn(targets, outputs)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8a2229-b4fe-4609-8e89-06cc43a0bebc",
   "metadata": {},
   "source": [
    "I think the problem is related to the saved tensors on the graph. The best source of info I've found about this is here:\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/autograd_saved_tensors_hooks_tutorial.html\n",
    "\n",
    "The hacky fix is just to re-run the forward pass each time before running the backward pass to repopulate the saved tensors correctly. There's probably a better fix that involves packing, saving, and unpacking the saved tensors appropriately for each microbatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d31bd18-ca24-4df6-9029-0ae8cbc8f27a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7427,  0.5930,  2.7530,  0.3157,  0.5558, -0.7625, -0.5604, -0.6728,\n",
       "         -0.0853, -1.8455,  1.6673,  0.1490,  1.0437, -2.1890, -0.3083,  0.9335],\n",
       "        [-0.1113, -1.7596, -0.4557, -0.2848,  1.0087,  1.6762, -0.7310,  0.1322,\n",
       "          0.5212,  0.5044, -0.4175, -0.2241,  1.2632,  0.5341,  0.5677, -1.6662],\n",
       "        [-0.6822, -0.1219, -0.9987, -1.1098,  1.2427, -0.2316, -1.2185,  0.1468,\n",
       "          0.8322,  0.7007,  0.0628,  0.6061,  0.9081,  1.2711, -2.2495, -0.2998],\n",
       "        [-1.4726, -0.1904,  1.7050,  0.6522,  2.2435,  0.7128,  0.8931,  1.0558,\n",
       "          0.4479, -2.5197, -1.5794,  2.7752, -1.0689, -1.6108, -1.3693,  2.0114],\n",
       "        [-0.5615,  1.5408, -1.4485, -0.0782,  0.3234,  1.0927, -0.0450, -1.7949,\n",
       "          0.1155,  0.7487,  0.0355,  0.8411,  1.1565, -1.7849,  0.4582,  1.1205],\n",
       "        [-0.0702,  0.2253,  0.6500,  0.1752,  0.3533,  0.6881, -2.0929,  1.7572,\n",
       "         -0.1277,  0.6316,  0.1293,  0.1294, -0.5409, -1.0919,  0.6151,  0.1582],\n",
       "        [-1.1471, -0.4757, -0.6866,  0.9279, -2.0948, -0.8965, -1.3044,  0.5543,\n",
       "          1.7604,  0.1549, -0.7478, -0.2974, -1.6250,  0.4646,  1.4656, -1.1008],\n",
       "        [ 0.0893,  2.0673, -0.7007,  0.5720, -0.4250, -0.5282, -0.2534, -0.3857,\n",
       "          0.5237,  0.4390, -0.0097,  0.9875,  0.9941,  0.0585,  3.4317, -1.9413],\n",
       "        [ 0.2339,  0.9495, -2.2275, -1.3417, -0.2901,  0.4404,  0.7483,  0.5723,\n",
       "         -2.1783, -2.7214, -1.3678, -1.4850,  0.8385, -0.0330, -0.1069, -0.2221],\n",
       "        [ 0.7127,  1.9035, -2.4773, -0.3971,  0.0149, -1.8393,  0.1025,  0.7094,\n",
       "          0.8117, -0.4660, -0.3898, -0.9734,  0.3622, -0.4241, -1.0147, -0.3178]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data_batches[0]\n",
    "outputs = model(x)\n",
    "loss = loss_fn(x, outputs)\n",
    "\n",
    "# This is one of the secret stateful tensors that stick around in the computation graph\n",
    "loss.grad_fn._saved_self "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ce7a694-0d03-404e-98e8-28b849d40c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 128)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's some more lingering saved data\n",
    "f = loss.grad_fn.next_functions[1][0]\n",
    "f._saved_mat2_strides"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
