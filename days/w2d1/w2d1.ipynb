{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import einsum\n",
    "from einops import rearrange, reduce, repeat\n",
    "import bert_tests\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_attention_scores(\n",
    "    token_activations: t.Tensor, #[batch_size, seq_length, hidden_size], \n",
    "    num_heads: int, \n",
    "    project_query, #: function(Tensor[..., 768] -> Tensor[..., num_heads*head_size]), \n",
    "    project_key, #: function((Tensor[..., 768]) -> Tensor[..., num_heads*head_size]) -> \n",
    "    #Tensor[batch_size, head_num, key_token: seq_length, query_token: seq_length]\n",
    "):\n",
    "    #key * query\n",
    "    batch_s, s_l, hidden_s = token_activations.size()\n",
    "    query = project_query(token_activations)\n",
    "    query = rearrange(query, 'b s (n h) -> b n s h', n=num_heads)\n",
    "    \n",
    "    key = project_key(token_activations)\n",
    "    # print(key.size())\n",
    "    key = rearrange(key, 'b s (n h) -> b n s h', n=num_heads)\n",
    "    _,_,_,head_s = key.size()\n",
    "\n",
    "    raw_scores = einsum('bnsh,bnzh->bnzs', query, key)\n",
    "    print(raw_scores.size())\n",
    "    print(num_heads)\n",
    "    return raw_scores / math.sqrt(head_s)\n",
    "#     t.nn.Softmax(\n",
    "    \n",
    "#     break\n",
    "    \n",
    "    # return attention_score\n",
    "\n",
    "#ok, batch size 2, seq_length 3 and hidden size 768, 12 heads\n",
    "bert_tests.test_attention_pattern_fn(raw_attention_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_attention(\n",
    "token_activations: t.Tensor, #[batch_size, seq_length, hidden_size (768)], \n",
    "num_heads: int, \n",
    "attention_pattern: t.Tensor, #[batch_size,num_heads, seq_length, seq_length], \n",
    "project_value,#func( (Tensor[..., hidden_size (768)]) -> Tensor[..., 12*64] ), \n",
    "project_output #func( (Tensor[..., 12*64]) -> Tensor[..., 768] )\n",
    "):\n",
    "#-> Tensor[batch_size, seq_length, hidden_size]\n",
    "    #attention_score: t[batch_size,num_heads, seq_length, seq_length], \n",
    "    #Naive attempt\n",
    "    values = project_value(token_activations)\n",
    "    values = rearrange(values, \\\n",
    "                       'b sql (num_h vec_length) -> b num_h sql vec_length',\\\n",
    "                      vec_length=64)\n",
    "    \n",
    "    soft_max = t.nn.Softmax(dim=-2)\n",
    "    #These are the scores me think\n",
    "    scores = soft_max(attention_pattern)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(scores.size())\n",
    "    print(values.size())\n",
    "    \n",
    "    #now multiply score into value\n",
    "    score_times_value = einsum('bnsz,bnhs -> bs', scores, values)\n",
    "    print(score_times_value.size())\n",
    "    outputs = project_output(score_times_value)\n",
    "    return outputs\n",
    "    \n",
    "bert_tests.test_attention_fn(bert_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
