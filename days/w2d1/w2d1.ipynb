{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import einsum\n",
    "from einops import rearrange, reduce, repeat\n",
    "import bert_tests\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 12, 3, 3])\n",
      "12\n",
      "attention pattern raw MATCH!!!!!!!!\n",
      " SHAPE (2, 12, 3, 3) MEAN: -0.01231 STD: 0.1219 VALS [-0.1191 0.1944 0.007139 0.03108 -0.1979 -0.3329 0.06184 0.1247 0.126 -0.1378...]\n"
     ]
    }
   ],
   "source": [
    "def raw_attention_scores(\n",
    "    token_activations: t.Tensor, #[batch_size, seq_length, hidden_size], \n",
    "    num_heads: int, \n",
    "    project_query, #: function(Tensor[..., 768] -> Tensor[..., num_heads*head_size]), \n",
    "    project_key, #: function((Tensor[..., 768]) -> Tensor[..., num_heads*head_size]) -> \n",
    "    #Tensor[batch_size, head_num, key_token: seq_length, query_token: seq_length]\n",
    "):\n",
    "    #key * query\n",
    "    batch_s, s_l, hidden_s = token_activations.size()\n",
    "    query = project_query(token_activations)\n",
    "    query = rearrange(query, 'b s (n h) -> b n s h', n=num_heads)\n",
    "    \n",
    "    key = project_key(token_activations)\n",
    "    # print(key.size())\n",
    "    key = rearrange(key, 'b s (n h) -> b n s h', n=num_heads)\n",
    "    _,_,_,head_s = key.size()\n",
    "\n",
    "    raw_scores = einsum('bnsh,bnzh->bnzs', query, key)\n",
    "    print(raw_scores.size())\n",
    "    print(num_heads)\n",
    "    return raw_scores / math.sqrt(head_s)\n",
    "#     t.nn.Softmax(\n",
    "    \n",
    "#     break\n",
    "    \n",
    "    # return attention_score\n",
    "\n",
    "#ok, batch size 2, seq_length 3 and hidden size 768, 12 heads\n",
    "bert_tests.test_attention_pattern_fn(raw_attention_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 12, 3, 3])\n",
      "torch.Size([2, 12, 3, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): operands do not broadcast with remapped shapes [original->remapped]: [2, 12, 3, 3]->[2, 3, 1, 12, 3] [2, 12, 3, 64]->[2, 64, 3, 12, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-f2e18e69e6a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mbert_tests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_attention_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_attention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/tamera_mlab/days/w2d1/bert_tests.py\u001b[0m in \u001b[0;36mtest_attention_fn\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mdropout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     allclose(\n\u001b[0;32m---> 40\u001b[0;31m         fn(\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mtoken_activations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_activations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-115-f2e18e69e6a3>\u001b[0m in \u001b[0;36mbert_attention\u001b[0;34m(token_activations, num_heads, attention_pattern, project_value, project_output)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m#now multiply score into value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mscore_times_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bnsz,bnhs -> bs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_times_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproject_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_times_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_operands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: einsum(): operands do not broadcast with remapped shapes [original->remapped]: [2, 12, 3, 3]->[2, 3, 1, 12, 3] [2, 12, 3, 64]->[2, 64, 3, 12, 1]"
     ]
    }
   ],
   "source": [
    "def bert_attention(\n",
    "token_activations: t.Tensor, #[batch_size, seq_length, hidden_size (768)], \n",
    "num_heads: int, \n",
    "attention_pattern: t.Tensor, #[batch_size,num_heads, seq_length, seq_length], \n",
    "project_value,#func( (Tensor[..., hidden_size (768)]) -> Tensor[..., 12*64] ), \n",
    "project_output #func( (Tensor[..., 12*64]) -> Tensor[..., 768] )\n",
    "):\n",
    "#-> Tensor[batch_size, seq_length, hidden_size]\n",
    "    #attention_score: t[batch_size,num_heads, seq_length, seq_length], \n",
    "    #Naive attempt\n",
    "    values = project_value(token_activations)\n",
    "    values = rearrange(values, \\\n",
    "                       'b sql (num_h vec_length) -> b num_h sql vec_length',\\\n",
    "                      vec_length=64)\n",
    "    \n",
    "    soft_max = t.nn.Softmax(dim=-2)\n",
    "    #These are the scores me think\n",
    "    scores = soft_max(attention_pattern)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(scores.size())\n",
    "    print(values.size())\n",
    "    \n",
    "    #now multiply score into value\n",
    "    score_times_value = einsum('bnsz,bnhs -> bs', scores, values)\n",
    "    print(score_times_value.size())\n",
    "    outputs = project_output(score_times_value)\n",
    "    return outputs\n",
    "    \n",
    "bert_tests.test_attention_fn(bert_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
