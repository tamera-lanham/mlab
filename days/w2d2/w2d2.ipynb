{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import bert_tests\n",
    "import bert_tao\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/bert-base-cased\n",
    "\n",
    "class Bert(nn.Module):\n",
    "    def __init__(self, config={}):\n",
    "        super(Bert, self).__init__()\n",
    "        \n",
    "        self.model = transformers.BertForMaskedLM.from_pretrained(\"bert-base-cased\")\n",
    "        self.model.config.update(config) # Modifies self.pretrained_model.config in-place\n",
    "\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        return self.model(**inputs) \n",
    "\n",
    "class BertEmbedded(Bert):\n",
    "    def forward(self, **inputs):\n",
    "        embedded = self.model.cls.predictions.transform(self.model.bert(**inputs).last_hidden_state)\n",
    "        logits = self.model.cls.predictions.decoder(embedded)\n",
    "        return embedded, unembedded\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = Bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/bert-base-cased\n",
    "\n",
    "# Same as above, just fresh weights (not pretrained)\n",
    "\n",
    "class Bert(nn.Module):\n",
    "    def __init__(self, pretrained=True, config={}):\n",
    "        super(Bert, self).__init__()\n",
    "        \n",
    "        default_config = {\n",
    "            \"attention_probs_dropout_prob\": 0.1,\n",
    "            \"classifier_dropout\": None,\n",
    "            \"gradient_checkpointing\": False,\n",
    "            \"hidden_act\": \"gelu\",\n",
    "            \"hidden_dropout_prob\": 0.1,\n",
    "            \"hidden_size\": 768,\n",
    "            \"initializer_range\": 0.02,\n",
    "            \"intermediate_size\": 3072,\n",
    "            \"layer_norm_eps\": 1e-12,\n",
    "            \"max_position_embeddings\": 512,\n",
    "            \"model_type\": \"bert\",\n",
    "            \"num_attention_heads\": 12,\n",
    "            \"num_hidden_layers\": 12,\n",
    "            \"pad_token_id\": 0,\n",
    "            \"position_embedding_type\": \"absolute\",\n",
    "            \"transformers_version\": \"4.16.2\",\n",
    "            \"type_vocab_size\": 2,\n",
    "            \"vocab_size\": 28996\n",
    "        }\n",
    "        \n",
    "        if pretrained:\n",
    "            self.model = transformers.BertForMaskedLM.from_pretrained(\"bert-base-cased\")\n",
    "            self.model.config.update(config) # Modifies self.pretrained_model.config in-place\n",
    "\n",
    "        else:\n",
    "            config = transformers.PretrainedConfig.from_dict({**default_config, **config})\n",
    "            self.model = transformers.BertForMaskedLM(config)\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        return self.model(**inputs) \n",
    "\n",
    "class BertEmbedded(Bert):\n",
    "    def forward(self, **inputs):\n",
    "        embedded = self.model.cls.predictions.transform(self.model.bert(**inputs).last_hidden_state)\n",
    "        unembedded = self.model.cls.predictions.decoder(embedded)\n",
    "        return embedded, unembedded\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = Bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The firetruck was painted a bright ___.\n",
      "\n",
      "48%\tred\n",
      "15%\tyellow\n",
      "10%\tblue\n",
      "8%\tpink\n",
      "6%\torange\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def ascii_art_probs(text, k=5):\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    mask_indices, = t.where(inputs[\"input_ids\"][0] == tokenizer.mask_token_id)\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    logits = t.nn.functional.softmax(outputs.logits, dim=2)\n",
    "    \n",
    "    top_k_masks = t.topk(logits, k, dim = 2)[1][0][mask_indices]\n",
    "\n",
    "    candidate_words = [tokenizer.decode(candidate_list).split() for candidate_list in top_k_masks]\n",
    "    candidate_percents = [logits[:,mask_index,top_k_masks[i]][0] for i, mask_index in enumerate(mask_indices)]\n",
    "    logits = logits.argmax(dim=2)\n",
    "    tokenizer.decode(logits[0])\n",
    "\n",
    "    s = text.replace('[MASK]', '___') + '\\n\\n'\n",
    "    for i, (words, percents) in enumerate(zip(candidate_words, candidate_percents)):\n",
    "        candidates = ['%d%%\\t%s' % (round(float(percent*100)), word)  for word, percent in zip(words, percents)]\n",
    "        s += '\\n'.join(candidates) + '\\n\\n'\n",
    "    print(s)\n",
    "\n",
    "text = \"The firetruck was painted a bright [MASK].\"\n",
    "ascii_art_probs(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert_tests\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, pretrained=True, **config):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        bert_config = {'attention_probs_dropout_prob': config['dropout'], **config}\n",
    "        self.bert = BertEmbedded(pretrained, bert_config)\n",
    "        self.classifier_dropout = nn.Dropout(p=config['dropout'])\n",
    "        self.classifier = nn.Linear(config['hidden_size'], config['num_classes'])\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded, unembedded = self.bert(input_ids=input_ids)\n",
    "        #logits = t.nn.functional.softmax(unembedded, dim=2)\n",
    "        classifications = self.classifier(self.classifier_dropout(embedded[:,0]))\n",
    "        return unembedded, classifications\n",
    "\n",
    "# Everything is working more or less correctly; the problem is that the random initialization of our Bert weights is just slightly different than the way they want it to be :(\n",
    "#bert_tests.test_bert_classification(BertClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Tao's implementation :(\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, **config):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = bert_tao.Bert(config)\n",
    "        self.sigmoid = t.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.bert(input_ids=input_ids)\n",
    "        logits = self.sigmoid(outputs.classification)\n",
    "        return outputs.logits, logits\n",
    "    \n",
    "#bert_tests.test_bert_classification(BertClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from tqdm import tqdm\n",
    "\n",
    "def batch(data, batch_size):\n",
    "    batches, batch = [], []\n",
    "    for i, sample in enumerate(data, 1):\n",
    "        if i % batch_size == 0:\n",
    "            batches.append(batch)\n",
    "            batch = []\n",
    "        batch.append(sample)\n",
    "\n",
    "    batches.append(batch)\n",
    "    return batches\n",
    "\n",
    "def tokenize_batch(batch, tokenizer, max_seq_len):\n",
    "    sentiments, texts = zip(*batch)\n",
    "    outputs = tokenizer(texts, return_tensors=\"pt\", padding='longest', max_length=max_seq_len, truncation=True)\n",
    "    return list(zip(sentiments, outputs['input_ids']))\n",
    "\n",
    "def tokenize(batches, max_seq_len=512):\n",
    "    tokenizer = transformers.BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "    return [tokenize_batch(batch, tokenizer, max_seq_len) for batch in tqdm(batches)]\n",
    "\n",
    "def convert_to_int(batches):\n",
    "    conv_dict = {\n",
    "        \"pos\": 1, \n",
    "        \"neg\": 0\n",
    "    }\n",
    "    return [\n",
    "            [(conv_dict[sentiment], text) for sentiment,text in batch] \n",
    "            for batch in batches\n",
    "            ]\n",
    "\n",
    "def preprocess(data, batch_size, max_seq_len=512):\n",
    "    \n",
    "    batched_data = batch(data, batch_size)\n",
    "    random.shuffle(batched_data)\n",
    "    tokenized = tokenize(batched_data, max_seq_len)\n",
    "    preprocessed = convert_to_int(tokenized)\n",
    "    \n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = torchtext.datasets.IMDB(root='.data', split=('train', 'test'))\n",
    "\n",
    "data_train_list = list(data_train)\n",
    "data_test_list = list(data_test)\n",
    "\n",
    "tokenized_train_batches = preprocess(data_train_list, 32)\n",
    "tokenized_test_batches = preprocess(data_test_list, 32)\n",
    "\n",
    "train, test = tokenized_train_batches, tokenized_test_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "default_config = {\n",
    "        \"vocab_size\": 28996,\n",
    "        \"intermediate_size\": 3072,\n",
    "        \"hidden_size\": 768,\n",
    "        \"num_classes\": 1,\n",
    "        \"num_layers\": 12,\n",
    "        \"num_heads\": 12,\n",
    "        \"max_position_embeddings\": 512,\n",
    "        \"dropout\": 0.1,\n",
    "        \"type_vocab_size\": 2,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertClassifier(**default_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "pretrained_bert = transformers.BertForMaskedLM.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One question for Max: why should we expect GPT2 to produce the next token in a string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mem = lambda: t.cuda.memory_allocated() / 2**(30) # Current GPU memory usage in GiB\n",
    "acc = lambda outputs, targets: sum((outputs > 0.5).squeeze().int() == targets) / len(outputs) # Accuracy score\n",
    "\n",
    "gpu = t.device('cuda') \n",
    "\n",
    "pretrained_bert = BertClassifier(**default_config)\n",
    "pretrained_bert.cuda()\n",
    "\n",
    "optimizer = t.optim.Adam(pretrained_bert.parameters(), lr=1e-5)\n",
    "loss_func = t.nn.BCELoss()\n",
    "\n",
    "n_epochs = 5\n",
    "batches_per_epoch = None # None means use all the batches\n",
    "record_loss_batches = 10\n",
    "\n",
    "loss_vals, accs = [], []\n",
    "running_loss = 0\n",
    "\n",
    "def batch_generator(batched_data, n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        for i, batch in enumerate(batched_data):\n",
    "            yield i, batch\n",
    "\n",
    "batches = batch_generator(list(zip(train, test))[:batches_per_epoch], n_epochs)\n",
    "batches_tqdm = tqdm(batches, total = n_epochs * (batches_per_epoch if batches_per_epoch else len(train)))\n",
    "\n",
    "for i, (train_batch, test_batch) in batches_tqdm:\n",
    "\n",
    "    targets, inputs = zip(*train_batch)\n",
    "    targets = t.Tensor(targets).to(gpu)\n",
    "    inputs = t.stack(inputs).to(gpu)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    _, outputs = pretrained_bert(inputs)\n",
    "    loss = loss_func(outputs, targets[:, None])\n",
    "    running_loss += loss.detach()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % record_loss_batches == 0:\n",
    "        \n",
    "        with t.no_grad():\n",
    "            \n",
    "            test_targets, test_inputs = zip(*train_batch)\n",
    "            test_targets = t.Tensor(test_targets).to(gpu)\n",
    "            test_inputs = t.stack(test_inputs).to(gpu)\n",
    "            \n",
    "            _, test_outputs = pretrained_bert(test_inputs)\n",
    "            accs.append(float(acc(test_outputs, test_targets).cpu().detach()))\n",
    "            loss_vals.append(running_loss / record_loss_batches)\n",
    "            running_loss = 0.0 \n",
    "            \n",
    "            batches_tqdm.set_postfix({'acc': sum(accs[-5:]) / min(len(accs), 5)})\n",
    "        \n",
    "t.save(pretrained_bert.state_dict(), './bert-classifier-2-18.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot([loss_tensor.cpu().detach().numpy() for loss_tensor in loss_vals])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved model and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = t.device('cuda')\n",
    "\n",
    "\n",
    "model = t.load('./bert-classifier-2-17.pt', device)\n",
    "\n",
    "model_weights = model\n",
    "pretrained_bert = BertClassifier(**default_config)\n",
    "pretrained_bert\n",
    "\n",
    "pretrained_bert.load_state_dict(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-0bb72197a7c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "device = t.device('cuda')\n",
    "bert = pretrained_bert.to(device)\n",
    "\n",
    "acc = lambda outputs, targets: sum((outputs > 0.5).squeeze().int() == targets) / len(inputs)\n",
    "accs = []\n",
    "\n",
    "with t.no_grad():\n",
    "    for i, data in enumerate(test[0:10]):\n",
    "        targets, inputs = zip(*data)\n",
    "        targets = t.Tensor(targets).to(device)\n",
    "        inputs = t.stack(inputs).to(device)\n",
    "        _, outputs = bert(inputs)\n",
    "\n",
    "        accs.append(acc(outputs, targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
