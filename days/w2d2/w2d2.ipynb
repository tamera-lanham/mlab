{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import bert_tests\n",
    "import bert_tao\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/bert-base-cased\n",
    "\n",
    "class Bert(nn.Module):\n",
    "    def __init__(self, config={}):\n",
    "        super(Bert, self).__init__()\n",
    "        \n",
    "        self.model = transformers.BertForMaskedLM.from_pretrained(\"bert-base-cased\")\n",
    "        self.model.config.update(config) # Modifies self.pretrained_model.config in-place\n",
    "\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        return self.model(**inputs) \n",
    "\n",
    "class BertEmbedded(Bert):\n",
    "    def forward(self, **inputs):\n",
    "        embedded = self.model.cls.predictions.transform(self.model.bert(**inputs).last_hidden_state)\n",
    "        logits = self.model.cls.predictions.decoder(embedded)\n",
    "        return embedded, unembedded\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = Bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/bert-base-cased\n",
    "\n",
    "# Same as above, just fresh weights (not pretrained)\n",
    "\n",
    "class Bert(nn.Module):\n",
    "    def __init__(self, pretrained=True, config={}):\n",
    "        super(Bert, self).__init__()\n",
    "        \n",
    "        default_config = {\n",
    "            \"attention_probs_dropout_prob\": 0.1,\n",
    "            \"classifier_dropout\": None,\n",
    "            \"gradient_checkpointing\": False,\n",
    "            \"hidden_act\": \"gelu\",\n",
    "            \"hidden_dropout_prob\": 0.1,\n",
    "            \"hidden_size\": 768,\n",
    "            \"initializer_range\": 0.02,\n",
    "            \"intermediate_size\": 3072,\n",
    "            \"layer_norm_eps\": 1e-12,\n",
    "            \"max_position_embeddings\": 512,\n",
    "            \"model_type\": \"bert\",\n",
    "            \"num_attention_heads\": 12,\n",
    "            \"num_hidden_layers\": 12,\n",
    "            \"pad_token_id\": 0,\n",
    "            \"position_embedding_type\": \"absolute\",\n",
    "            \"transformers_version\": \"4.16.2\",\n",
    "            \"type_vocab_size\": 2,\n",
    "            \"vocab_size\": 28996\n",
    "        }\n",
    "        \n",
    "        if pretrained:\n",
    "            self.model = transformers.BertForMaskedLM.from_pretrained(\"bert-base-cased\")\n",
    "            self.model.config.update(config) # Modifies self.pretrained_model.config in-place\n",
    "\n",
    "        else:\n",
    "            config = transformers.PretrainedConfig.from_dict({**default_config, **config})\n",
    "            self.model = transformers.BertForMaskedLM(config)\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        return self.model(**inputs) \n",
    "\n",
    "class BertEmbedded(Bert):\n",
    "    def forward(self, **inputs):\n",
    "        embedded = self.model.cls.predictions.transform(self.model.bert(**inputs).last_hidden_state)\n",
    "        unembedded = self.model.cls.predictions.decoder(embedded)\n",
    "        return embedded, unembedded\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = Bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The firetruck was painted a bright ___.\n",
      "\n",
      "48%\tred\n",
      "15%\tyellow\n",
      "10%\tblue\n",
      "8%\tpink\n",
      "6%\torange\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def ascii_art_probs(text, k=5):\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    mask_indices, = t.where(inputs[\"input_ids\"][0] == tokenizer.mask_token_id)\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    logits = t.nn.functional.softmax(outputs.logits, dim=2)\n",
    "    \n",
    "    top_k_masks = t.topk(logits, k, dim = 2)[1][0][mask_indices]\n",
    "\n",
    "    candidate_words = [tokenizer.decode(candidate_list).split() for candidate_list in top_k_masks]\n",
    "    candidate_percents = [logits[:,mask_index,top_k_masks[i]][0] for i, mask_index in enumerate(mask_indices)]\n",
    "    logits = logits.argmax(dim=2)\n",
    "    tokenizer.decode(logits[0])\n",
    "\n",
    "    s = text.replace('[MASK]', '___') + '\\n\\n'\n",
    "    for i, (words, percents) in enumerate(zip(candidate_words, candidate_percents)):\n",
    "        candidates = ['%d%%\\t%s' % (round(float(percent*100)), word)  for word, percent in zip(words, percents)]\n",
    "        s += '\\n'.join(candidates) + '\\n\\n'\n",
    "    print(s)\n",
    "\n",
    "text = \"The firetruck was painted a bright [MASK].\"\n",
    "ascii_art_probs(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "error in bert\n SHAPE (1, 4, 28996) MEAN: -4.229 STD: 2.874 VALS [-7.459 -7.382 -7.54 -7.476 -7.382 -7.434 -7.522 -7.545 -7.549 -7.458...] \n!=\n SHAPE (1, 4, 28996) MEAN: 0.003031 STD: 0.5765 VALS [-0.5742 -0.432 0.1186 -0.7165 -0.5261 0.4967 1.223 0.3165 -0.3247 -0.5716...]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-56d50d632b61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Everything is working more or less correctly; the problem is that the random initialization of our Bert weights is just slightly different than the way they want it to be :(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mbert_tests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_bert_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBertClassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/mlab/days/w2d2/bert_tests.py\u001b[0m in \u001b[0;36mtest_bert_classification\u001b[0;34m(your_module)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hello there\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     allclose(\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mreference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlab/days/w2d2/bert_tests.py\u001b[0m in \u001b[0;36mallclose\u001b[0;34m(my_out, their_out, name, tol)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheir_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0merrstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'error in {name}\\n{tpeek(\"\", my_out, ret=True)} \\n!=\\n{tpeek(\"\", their_out, ret=True)}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtpeek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{name} MATCH!!!!!!!!\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: error in bert\n SHAPE (1, 4, 28996) MEAN: -4.229 STD: 2.874 VALS [-7.459 -7.382 -7.54 -7.476 -7.382 -7.434 -7.522 -7.545 -7.549 -7.458...] \n!=\n SHAPE (1, 4, 28996) MEAN: 0.003031 STD: 0.5765 VALS [-0.5742 -0.432 0.1186 -0.7165 -0.5261 0.4967 1.223 0.3165 -0.3247 -0.5716...]"
     ]
    }
   ],
   "source": [
    "import bert_tests\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, pretrained=True, **config):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        bert_config = {'attention_probs_dropout_prob': config['dropout'], **config}\n",
    "        self.bert = BertEmbedded(pretrained, bert_config)\n",
    "        self.classifier_dropout = nn.Dropout(p=config['dropout'])\n",
    "        self.classifier = nn.Linear(config['hidden_size'], config['num_classes'])\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded, unembedded = self.bert(input_ids=input_ids)\n",
    "        #logits = t.nn.functional.softmax(unembedded, dim=2)\n",
    "        classifications = self.classifier(self.classifier_dropout(embedded[:,0]))\n",
    "        return unembedded, classifications\n",
    "\n",
    "# Everything is working more or less correctly; the problem is that the random initialization of our Bert weights is just slightly different than the way they want it to be :(\n",
    "bert_tests.test_bert_classification(BertClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 4, 28996) MEAN: 0.003031 STD: 0.5765 VALS [-0.5742 -0.432 0.1186 -0.7165 -0.5261 0.4967 1.223 0.3165 -0.3247 -0.5716...]\n",
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 2) MEAN: 0.09479 STD: 1.411 VALS [-0.903 1.093]\n"
     ]
    }
   ],
   "source": [
    "# Using Tao's implementation :(\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, **config):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = bert_tao.Bert(config)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.bert(input_ids=input_ids)\n",
    "        return outputs.logits, outputs.classification\n",
    "    \n",
    "bert_tests.test_bert_classification(BertClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from tqdm import tqdm\n",
    "\n",
    "def batch(data, batch_size):\n",
    "    batches, batch = [], []\n",
    "    for i, sample in enumerate(data, 1):\n",
    "        if i % batch_size == 0:\n",
    "            batches.append(batch)\n",
    "            batch = []\n",
    "        batch.append(sample)\n",
    "\n",
    "    batches.append(batch)\n",
    "    return batches\n",
    "\n",
    "def tokenize_batch(batch, tokenizer, max_seq_len):\n",
    "    sentiments, texts = zip(*batch)\n",
    "    outputs = tokenizer(texts, return_tensors=\"pt\", padding='longest', max_length=max_seq_len, truncation=True)\n",
    "    return list(zip(sentiments, outputs['input_ids']))\n",
    "\n",
    "def tokenize(batches, max_seq_len=512):\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')\n",
    "    return [tokenize_batch(batch, tokenizer, max_seq_len) for batch in tqdm(batches)]\n",
    "\n",
    "def convert_to_int(batches):\n",
    "    conv_dict = {\n",
    "        \"pos\": 1, \n",
    "        \"neg\": 0\n",
    "    }\n",
    "    return [\n",
    "            [(conv_dict[sentiment], text) for sentiment,text in batch] \n",
    "            for batch in batches\n",
    "            ]\n",
    "\n",
    "def preprocess(data, batch_size, max_seq_len=512):\n",
    "    \n",
    "    batched_data = batch(data, batch_size)\n",
    "    random.shuffle(batched_data)\n",
    "    tokenized = tokenize(batched_data, max_seq_len)\n",
    "    preprocessed = convert_to_int(tokenized)\n",
    "    \n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [01:38<00:00,  7.95it/s]\n",
      "100%|██████████| 782/782 [01:35<00:00,  8.15it/s]\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test = torchtext.datasets.IMDB(root='.data', split=('train', 'test'))\n",
    "\n",
    "data_train_list = list(data_train)\n",
    "data_test_list = list(data_test)\n",
    "\n",
    "tokenized_train_batches = preprocess(data_train_list, 32)\n",
    "tokenized_test_batches = preprocess(data_test_list, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [1:06:20<00:00, 398.01s/it]\n"
     ]
    }
   ],
   "source": [
    "default_config = {\n",
    "        \"vocab_size\": 28996,\n",
    "        \"intermediate_size\": 3072,\n",
    "        \"hidden_size\": 768,\n",
    "        \"num_classes\": 1,\n",
    "        \"num_layers\": 12,\n",
    "        \"num_heads\": 12,\n",
    "        \"max_position_embeddings\": 512,\n",
    "        \"dropout\": 0.1,\n",
    "        \"type_vocab_size\": 2,\n",
    "    }\n",
    "\n",
    "\n",
    "# t.nn.CrossEntropyLoss()\n",
    "gpu = t.device('cuda') \n",
    "\n",
    "pretrained_bert = BertClassifier(**default_config)\n",
    "pretrained_bert.cuda()\n",
    "\n",
    "optimizer = t.optim.Adam(pretrained_bert.parameters())\n",
    "loss_func = t.nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)): \n",
    "    running_loss = 0.0 #wait we are keep track of this for...?\n",
    "    for i, data in enumerate(tokenized_train_batches):\n",
    "        labels, inputs = zip(*data)\n",
    "        stacked_labels = t.Tensor(labels).to(gpu)\n",
    "        stacked_inputs = t.stack(inputs).to(gpu)\n",
    "        optimizer.zero_grad()\n",
    "        _, outputs = pretrained_bert(stacked_inputs)\n",
    "        outputs = outputs.to(gpu)\n",
    "        loss = loss_func(outputs, stacked_labels[:, None])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.save(pretrained_bert.state_dict(), './bert-classifier.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    5161 MB |   31710 MB |  647173 GB |  647168 GB |\n",
      "|       from large pool |    5157 MB |   31702 MB |  647071 GB |  647066 GB |\n",
      "|       from small pool |       3 MB |      26 MB |     102 GB |     102 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    5161 MB |   31710 MB |  647173 GB |  647168 GB |\n",
      "|       from large pool |    5157 MB |   31702 MB |  647071 GB |  647066 GB |\n",
      "|       from small pool |       3 MB |      26 MB |     102 GB |     102 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   34830 MB |   34830 MB |   37304 MB |    2474 MB |\n",
      "|       from large pool |   34822 MB |   34822 MB |   37272 MB |    2450 MB |\n",
      "|       from small pool |       8 MB |      32 MB |      32 MB |      24 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  485856 KB |    7318 MB |   34514 GB |   34514 GB |\n",
      "|       from large pool |  481674 KB |    7316 MB |   34411 GB |   34411 GB |\n",
      "|       from small pool |    4182 KB |       8 MB |     102 GB |     102 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1422    |    1678    |   12343 K  |   12341 K  |\n",
      "|       from large pool |     529    |     728    |    7389 K  |    7389 K  |\n",
      "|       from small pool |     893    |    1022    |    4953 K  |    4952 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1422    |    1678    |   12343 K  |   12341 K  |\n",
      "|       from large pool |     529    |     728    |    7389 K  |    7389 K  |\n",
      "|       from small pool |     893    |    1022    |    4953 K  |    4952 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     338    |     338    |     420    |      82    |\n",
      "|       from large pool |     334    |     334    |     404    |      70    |\n",
      "|       from small pool |       4    |      16    |      16    |      12    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |     149    |     341    |    3795 K  |    3795 K  |\n",
      "|       from large pool |     110    |     302    |    1718 K  |    1718 K  |\n",
      "|       from small pool |      39    |      71    |    2076 K  |    2076 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(t.cuda.memory_summary(device=gpu, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
