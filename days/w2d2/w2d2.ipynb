{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import bert_tests\n",
    "import bert_tao\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/bert-base-cased\n",
    "\n",
    "class Bert(nn.Module):\n",
    "    def __init__(self, config={}):\n",
    "        super(Bert, self).__init__()\n",
    "        \n",
    "        self.model = transformers.BertForMaskedLM.from_pretrained(\"bert-base-cased\")\n",
    "        self.model.config.update(config) # Modifies self.pretrained_model.config in-place\n",
    "\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        return self.model(**inputs) \n",
    "\n",
    "class BertEmbedded(Bert):\n",
    "    def forward(self, **inputs):\n",
    "        embedded = self.model.cls.predictions.transform(self.model.bert(**inputs).last_hidden_state)\n",
    "        logits = self.model.cls.predictions.decoder(embedded)\n",
    "        return embedded, unembedded\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = Bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/bert-base-cased\n",
    "\n",
    "# Same as above, just fresh weights (not pretrained)\n",
    "\n",
    "class Bert(nn.Module):\n",
    "    def __init__(self, pretrained=True, config={}):\n",
    "        super(Bert, self).__init__()\n",
    "        \n",
    "        default_config = {\n",
    "            \"attention_probs_dropout_prob\": 0.1,\n",
    "            \"classifier_dropout\": None,\n",
    "            \"gradient_checkpointing\": False,\n",
    "            \"hidden_act\": \"gelu\",\n",
    "            \"hidden_dropout_prob\": 0.1,\n",
    "            \"hidden_size\": 768,\n",
    "            \"initializer_range\": 0.02,\n",
    "            \"intermediate_size\": 3072,\n",
    "            \"layer_norm_eps\": 1e-12,\n",
    "            \"max_position_embeddings\": 512,\n",
    "            \"model_type\": \"bert\",\n",
    "            \"num_attention_heads\": 12,\n",
    "            \"num_hidden_layers\": 12,\n",
    "            \"pad_token_id\": 0,\n",
    "            \"position_embedding_type\": \"absolute\",\n",
    "            \"transformers_version\": \"4.16.2\",\n",
    "            \"type_vocab_size\": 2,\n",
    "            \"vocab_size\": 28996\n",
    "        }\n",
    "        \n",
    "        if pretrained:\n",
    "            self.model = transformers.BertForMaskedLM.from_pretrained(\"bert-base-cased\")\n",
    "            self.model.config.update(config) # Modifies self.pretrained_model.config in-place\n",
    "\n",
    "        else:\n",
    "            config = transformers.PretrainedConfig.from_dict({**default_config, **config})\n",
    "            self.model = transformers.BertForMaskedLM(config)\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        return self.model(**inputs) \n",
    "\n",
    "class BertEmbedded(Bert):\n",
    "    def forward(self, **inputs):\n",
    "        embedded = self.model.cls.predictions.transform(self.model.bert(**inputs).last_hidden_state)\n",
    "        unembedded = self.model.cls.predictions.decoder(embedded)\n",
    "        return embedded, unembedded\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = Bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The firetruck was painted a bright ___.\n",
      "\n",
      "48%\tred\n",
      "15%\tyellow\n",
      "10%\tblue\n",
      "8%\tpink\n",
      "6%\torange\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def ascii_art_probs(text, k=5):\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    mask_indices, = t.where(inputs[\"input_ids\"][0] == tokenizer.mask_token_id)\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    logits = t.nn.functional.softmax(outputs.logits, dim=2)\n",
    "    \n",
    "    top_k_masks = t.topk(logits, k, dim = 2)[1][0][mask_indices]\n",
    "\n",
    "    candidate_words = [tokenizer.decode(candidate_list).split() for candidate_list in top_k_masks]\n",
    "    candidate_percents = [logits[:,mask_index,top_k_masks[i]][0] for i, mask_index in enumerate(mask_indices)]\n",
    "    logits = logits.argmax(dim=2)\n",
    "    tokenizer.decode(logits[0])\n",
    "\n",
    "    s = text.replace('[MASK]', '___') + '\\n\\n'\n",
    "    for i, (words, percents) in enumerate(zip(candidate_words, candidate_percents)):\n",
    "        candidates = ['%d%%\\t%s' % (round(float(percent*100)), word)  for word, percent in zip(words, percents)]\n",
    "        s += '\\n'.join(candidates) + '\\n\\n'\n",
    "    print(s)\n",
    "\n",
    "text = \"The firetruck was painted a bright [MASK].\"\n",
    "ascii_art_probs(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "error in bert\n SHAPE (1, 4, 28996) MEAN: -4.229 STD: 2.874 VALS [-7.459 -7.382 -7.54 -7.476 -7.382 -7.434 -7.522 -7.545 -7.549 -7.458...] \n!=\n SHAPE (1, 4, 28996) MEAN: 0.003031 STD: 0.5765 VALS [-0.5742 -0.432 0.1186 -0.7165 -0.5261 0.4967 1.223 0.3165 -0.3247 -0.5716...]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-56d50d632b61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Everything is working more or less correctly; the problem is that the random initialization of our Bert weights is just slightly different than the way they want it to be :(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mbert_tests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_bert_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBertClassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/mlab/days/w2d2/bert_tests.py\u001b[0m in \u001b[0;36mtest_bert_classification\u001b[0;34m(your_module)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hello there\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     allclose(\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mreference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlab/days/w2d2/bert_tests.py\u001b[0m in \u001b[0;36mallclose\u001b[0;34m(my_out, their_out, name, tol)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheir_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0merrstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'error in {name}\\n{tpeek(\"\", my_out, ret=True)} \\n!=\\n{tpeek(\"\", their_out, ret=True)}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtpeek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{name} MATCH!!!!!!!!\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: error in bert\n SHAPE (1, 4, 28996) MEAN: -4.229 STD: 2.874 VALS [-7.459 -7.382 -7.54 -7.476 -7.382 -7.434 -7.522 -7.545 -7.549 -7.458...] \n!=\n SHAPE (1, 4, 28996) MEAN: 0.003031 STD: 0.5765 VALS [-0.5742 -0.432 0.1186 -0.7165 -0.5261 0.4967 1.223 0.3165 -0.3247 -0.5716...]"
     ]
    }
   ],
   "source": [
    "import bert_tests\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, pretrained=True, **config):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        bert_config = {'attention_probs_dropout_prob': config['dropout'], **config}\n",
    "        self.bert = BertEmbedded(pretrained, bert_config)\n",
    "        self.classifier_dropout = nn.Dropout(p=config['dropout'])\n",
    "        self.classifier = nn.Linear(config['hidden_size'], config['num_classes'])\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded, unembedded = self.bert(input_ids=input_ids)\n",
    "        #logits = t.nn.functional.softmax(unembedded, dim=2)\n",
    "        classifications = self.classifier(self.classifier_dropout(embedded[:,0]))\n",
    "        return unembedded, classifications\n",
    "\n",
    "# Everything is working more or less correctly; the problem is that the random initialization of our Bert weights is just slightly different than the way they want it to be :(\n",
    "bert_tests.test_bert_classification(BertClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 4, 28996) MEAN: 0.003031 STD: 0.5765 VALS [-0.5742 -0.432 0.1186 -0.7165 -0.5261 0.4967 1.223 0.3165 -0.3247 -0.5716...]\n",
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 2) MEAN: 0.09479 STD: 1.411 VALS [-0.903 1.093]\n"
     ]
    }
   ],
   "source": [
    "# Using Tao's implementation :(\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, **config):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = bert_tao.Bert(config)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.bert(input_ids=input_ids)\n",
    "        return outputs.logits, outputs.classification\n",
    "    \n",
    "bert_tests.test_bert_classification(BertClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from tqdm import tqdm\n",
    "\n",
    "def batch(data, batch_size):\n",
    "    batches, batch = [], []\n",
    "    for i, sample in enumerate(data, 1):\n",
    "        if i % batch_size == 0:\n",
    "            batches.append(batch)\n",
    "            batch = []\n",
    "        batch.append(sample)\n",
    "\n",
    "    batches.append(batch)\n",
    "    return batches\n",
    "\n",
    "def tokenize_batch(batch, tokenizer, max_seq_len):\n",
    "    sentiments, texts = zip(*batch)\n",
    "    outputs = tokenizer(texts, return_tensors=\"pt\", padding='longest', max_length=max_seq_len, truncation=True)\n",
    "    return list(zip(sentiments, outputs['input_ids']))\n",
    "\n",
    "def tokenize(batches, max_seq_len=512):\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')\n",
    "    return [tokenize_batch(batch, tokenizer, max_seq_len) for batch in tqdm(batches)]\n",
    "\n",
    "def convert_to_int(batches):\n",
    "    conv_dict = {\n",
    "        \"pos\": 1, \n",
    "        \"neg\": 0\n",
    "    }\n",
    "    return [\n",
    "            [(conv_dict[sentiment], text) for sentiment,text in batch] \n",
    "            for batch in batches\n",
    "            ]\n",
    "\n",
    "def preprocess(data, batch_size, max_seq_len=512):\n",
    "    \n",
    "    batched_data = batch(data, batch_size)\n",
    "    random.shuffle(batched_data)\n",
    "    tokenized = tokenize(batched_data, max_seq_len)\n",
    "    preprocessed = convert_to_int(tokenized)\n",
    "    \n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [21:15<?, ?it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 83.53it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 81.40it/s]\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test = torchtext.datasets.IMDB(root='.data', split=('train', 'test'))\n",
    "\n",
    "data_train_list = list(data_train)\n",
    "data_test_list = list(data_test)\n",
    "\n",
    "tokenized_train_batches = preprocess(data_train_list[:100], 3)\n",
    "tokenized_test_batches = preprocess(data_test_list[:100], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"nll_loss_forward_reduce_cuda_kernel_2d_index\" not implemented for 'Float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-aab287832336>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstacked_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacked_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"nll_loss_forward_reduce_cuda_kernel_2d_index\" not implemented for 'Float'"
     ]
    }
   ],
   "source": [
    "default_config = {\n",
    "        \"vocab_size\": 28996,\n",
    "        \"intermediate_size\": 3072,\n",
    "        \"hidden_size\": 768,\n",
    "        \"num_classes\": 2,\n",
    "        \"num_layers\": 12,\n",
    "        \"num_heads\": 12,\n",
    "        \"max_position_embeddings\": 512,\n",
    "        \"dropout\": 0.1,\n",
    "        \"type_vocab_size\": 2,\n",
    "    }\n",
    "\n",
    "\n",
    "# t.nn.CrossEntropyLoss()\n",
    "pretrained_bert = BertClassifier(**default_config)\n",
    "pretrained_bert.cuda()\n",
    "\n",
    "optimizer = t.optim.Adam(pretrained_bert.parameters())\n",
    "loss_func = t.nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs): \n",
    "    running_loss = 0.0 #wait we are keep track of this for...?\n",
    "    for i, data in tqdm(enumerate(tokenized_train_batches)): # enumerate\n",
    "        labels, inputs = zip(*data) # zip\n",
    "        # print(list(inputs))\n",
    "        print(labels)\n",
    "        stacked_labels = t.Tensor(labels).cuda()\n",
    "        stacked_inputs = t.stack(inputs).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        _, outputs = pretrained_bert(stacked_inputs.cuda())\n",
    "        outputs = outputs.cuda()\n",
    "        loss = loss_func(outputs, stacked_labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 47.54 GiB total capacity; 45.01 GiB already allocated; 3.62 MiB free; 45.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-1bf3776700f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 47.54 GiB total capacity; 45.01 GiB already allocated; 3.62 MiB free; 45.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "t.randn(1500,2500).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 4         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   46093 MB |   46094 MB |   84137 MB |   38043 MB |\n",
      "|       from large pool |   46084 MB |   46085 MB |   84123 MB |   38039 MB |\n",
      "|       from small pool |       9 MB |       9 MB |      14 MB |       4 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   46093 MB |   46094 MB |   84137 MB |   38043 MB |\n",
      "|       from large pool |   46084 MB |   46085 MB |   84123 MB |   38039 MB |\n",
      "|       from small pool |       9 MB |       9 MB |      14 MB |       4 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   46324 MB |   46324 MB |   46460 MB |  139264 KB |\n",
      "|       from large pool |   46314 MB |   46314 MB |   46450 MB |  139264 KB |\n",
      "|       from small pool |      10 MB |      10 MB |      10 MB |       0 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  235946 KB |  702698 KB |    5628 MB |    5397 MB |\n",
      "|       from large pool |  235513 KB |  700680 KB |    5613 MB |    5383 MB |\n",
      "|       from small pool |     433 KB |    2176 KB |      14 MB |      14 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    2724    |    2731    |    5290    |    2566    |\n",
      "|       from large pool |    1617    |    1617    |    3131    |    1514    |\n",
      "|       from small pool |    1107    |    1114    |    2159    |    1052    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    2724    |    2731    |    5290    |    2566    |\n",
      "|       from large pool |    1617    |    1617    |    3131    |    1514    |\n",
      "|       from small pool |    1107    |    1114    |    2159    |    1052    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |    1140    |    1140    |    1143    |       3    |\n",
      "|       from large pool |    1135    |    1135    |    1138    |       3    |\n",
      "|       from small pool |       5    |       5    |       5    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |     104    |     108    |     548    |     444    |\n",
      "|       from large pool |      97    |     101    |     381    |     284    |\n",
      "|       from small pool |       7    |      11    |     167    |     160    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(t.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('active.all.allocated', 5290),\n",
       "             ('active.all.current', 2724),\n",
       "             ('active.all.freed', 2566),\n",
       "             ('active.all.peak', 2731),\n",
       "             ('active.large_pool.allocated', 3131),\n",
       "             ('active.large_pool.current', 1617),\n",
       "             ('active.large_pool.freed', 1514),\n",
       "             ('active.large_pool.peak', 1617),\n",
       "             ('active.small_pool.allocated', 2159),\n",
       "             ('active.small_pool.current', 1107),\n",
       "             ('active.small_pool.freed', 1052),\n",
       "             ('active.small_pool.peak', 1114),\n",
       "             ('active_bytes.all.allocated', 88224369664),\n",
       "             ('active_bytes.all.current', 48332625408),\n",
       "             ('active_bytes.all.freed', 39891744256),\n",
       "             ('active_bytes.all.peak', 48333485056),\n",
       "             ('active_bytes.large_pool.allocated', 88209381888),\n",
       "             ('active_bytes.large_pool.current', 48322583040),\n",
       "             ('active_bytes.large_pool.freed', 39886798848),\n",
       "             ('active_bytes.large_pool.peak', 48323694592),\n",
       "             ('active_bytes.small_pool.allocated', 14987776),\n",
       "             ('active_bytes.small_pool.current', 10042368),\n",
       "             ('active_bytes.small_pool.freed', 4945408),\n",
       "             ('active_bytes.small_pool.peak', 10045952),\n",
       "             ('allocated_bytes.all.allocated', 88224369664),\n",
       "             ('allocated_bytes.all.current', 48332625408),\n",
       "             ('allocated_bytes.all.freed', 39891744256),\n",
       "             ('allocated_bytes.all.peak', 48333485056),\n",
       "             ('allocated_bytes.large_pool.allocated', 88209381888),\n",
       "             ('allocated_bytes.large_pool.current', 48322583040),\n",
       "             ('allocated_bytes.large_pool.freed', 39886798848),\n",
       "             ('allocated_bytes.large_pool.peak', 48323694592),\n",
       "             ('allocated_bytes.small_pool.allocated', 14987776),\n",
       "             ('allocated_bytes.small_pool.current', 10042368),\n",
       "             ('allocated_bytes.small_pool.freed', 4945408),\n",
       "             ('allocated_bytes.small_pool.peak', 10045952),\n",
       "             ('allocation.all.allocated', 5290),\n",
       "             ('allocation.all.current', 2724),\n",
       "             ('allocation.all.freed', 2566),\n",
       "             ('allocation.all.peak', 2731),\n",
       "             ('allocation.large_pool.allocated', 3131),\n",
       "             ('allocation.large_pool.current', 1617),\n",
       "             ('allocation.large_pool.freed', 1514),\n",
       "             ('allocation.large_pool.peak', 1617),\n",
       "             ('allocation.small_pool.allocated', 2159),\n",
       "             ('allocation.small_pool.current', 1107),\n",
       "             ('allocation.small_pool.freed', 1052),\n",
       "             ('allocation.small_pool.peak', 1114),\n",
       "             ('inactive_split.all.allocated', 548),\n",
       "             ('inactive_split.all.current', 104),\n",
       "             ('inactive_split.all.freed', 444),\n",
       "             ('inactive_split.all.peak', 108),\n",
       "             ('inactive_split.large_pool.allocated', 381),\n",
       "             ('inactive_split.large_pool.current', 97),\n",
       "             ('inactive_split.large_pool.freed', 284),\n",
       "             ('inactive_split.large_pool.peak', 101),\n",
       "             ('inactive_split.small_pool.allocated', 167),\n",
       "             ('inactive_split.small_pool.current', 7),\n",
       "             ('inactive_split.small_pool.freed', 160),\n",
       "             ('inactive_split.small_pool.peak', 11),\n",
       "             ('inactive_split_bytes.all.allocated', 5901452800),\n",
       "             ('inactive_split_bytes.all.current', 241609216),\n",
       "             ('inactive_split_bytes.all.freed', 5659843584),\n",
       "             ('inactive_split_bytes.all.peak', 719563264),\n",
       "             ('inactive_split_bytes.large_pool.allocated', 5886095360),\n",
       "             ('inactive_split_bytes.large_pool.current', 241165824),\n",
       "             ('inactive_split_bytes.large_pool.freed', 5644929536),\n",
       "             ('inactive_split_bytes.large_pool.peak', 717496320),\n",
       "             ('inactive_split_bytes.small_pool.allocated', 15357440),\n",
       "             ('inactive_split_bytes.small_pool.current', 443392),\n",
       "             ('inactive_split_bytes.small_pool.freed', 14914048),\n",
       "             ('inactive_split_bytes.small_pool.peak', 2228736),\n",
       "             ('max_split_size', -1),\n",
       "             ('num_alloc_retries', 4),\n",
       "             ('num_ooms', 4),\n",
       "             ('oversize_allocations.allocated', 0),\n",
       "             ('oversize_allocations.current', 0),\n",
       "             ('oversize_allocations.freed', 0),\n",
       "             ('oversize_allocations.peak', 0),\n",
       "             ('oversize_segments.allocated', 0),\n",
       "             ('oversize_segments.current', 0),\n",
       "             ('oversize_segments.freed', 0),\n",
       "             ('oversize_segments.peak', 0),\n",
       "             ('reserved_bytes.all.allocated', 48716840960),\n",
       "             ('reserved_bytes.all.current', 48574234624),\n",
       "             ('reserved_bytes.all.freed', 142606336),\n",
       "             ('reserved_bytes.all.peak', 48574234624),\n",
       "             ('reserved_bytes.large_pool.allocated', 48706355200),\n",
       "             ('reserved_bytes.large_pool.current', 48563748864),\n",
       "             ('reserved_bytes.large_pool.freed', 142606336),\n",
       "             ('reserved_bytes.large_pool.peak', 48563748864),\n",
       "             ('reserved_bytes.small_pool.allocated', 10485760),\n",
       "             ('reserved_bytes.small_pool.current', 10485760),\n",
       "             ('reserved_bytes.small_pool.freed', 0),\n",
       "             ('reserved_bytes.small_pool.peak', 10485760),\n",
       "             ('segment.all.allocated', 1143),\n",
       "             ('segment.all.current', 1140),\n",
       "             ('segment.all.freed', 3),\n",
       "             ('segment.all.peak', 1140),\n",
       "             ('segment.large_pool.allocated', 1138),\n",
       "             ('segment.large_pool.current', 1135),\n",
       "             ('segment.large_pool.freed', 3),\n",
       "             ('segment.large_pool.peak', 1135),\n",
       "             ('segment.small_pool.allocated', 5),\n",
       "             ('segment.small_pool.current', 5),\n",
       "             ('segment.small_pool.freed', 0),\n",
       "             ('segment.small_pool.peak', 5)])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.cuda.memory_stats(device=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = t.optim.Adam(pretrained_bert.parameters(), lr=1e-5)\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# for epoch in range(n_epochs):\n",
    "#     for batch in batches:\n",
    "#         y, X = zip(*batch)\n",
    "#         y_pred = model(X)\n",
    "#         loss = loss_fn(y, y_pred)\n",
    "#\n",
    "#         loss_fn.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "#     running_loss = 0.0\n",
    "#     for i, data in enumerate(trainloader, 0):\n",
    "#         # get the inputs; data is a list of [inputs, labels]\n",
    "#         inputs, labels = data\n",
    "\n",
    "#         # zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # forward + backward + optimize\n",
    "#         outputs = net(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # print statistics\n",
    "#         running_loss += loss.item()\n",
    "#         if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "#             print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "#             running_loss = 0.0\n",
    "\n",
    "# print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
