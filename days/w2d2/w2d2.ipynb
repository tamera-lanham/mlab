{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import bert_tests\n",
    "import bert_tao\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/bert-base-cased\n",
    "\n",
    "class Bert(nn.Module):\n",
    "    def __init__(self, config={}):\n",
    "        super(Bert, self).__init__()\n",
    "        \n",
    "        self.model = transformers.BertForMaskedLM.from_pretrained(\"bert-base-cased\")\n",
    "        self.model.config.update(config) # Modifies self.pretrained_model.config in-place\n",
    "\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        return self.model(**inputs) \n",
    "\n",
    "class BertEmbedded(Bert):\n",
    "    def forward(self, **inputs):\n",
    "        embedded = self.model.cls.predictions.transform(self.model.bert(**inputs).last_hidden_state)\n",
    "        logits = self.model.cls.predictions.decoder(embedded)\n",
    "        return embedded, unembedded\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = Bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/bert-base-cased\n",
    "\n",
    "# Same as above, just fresh weights (not pretrained)\n",
    "\n",
    "class Bert(nn.Module):\n",
    "    def __init__(self, pretrained=True, config={}):\n",
    "        super(Bert, self).__init__()\n",
    "        \n",
    "        default_config = {\n",
    "            \"attention_probs_dropout_prob\": 0.1,\n",
    "            \"classifier_dropout\": None,\n",
    "            \"gradient_checkpointing\": False,\n",
    "            \"hidden_act\": \"gelu\",\n",
    "            \"hidden_dropout_prob\": 0.1,\n",
    "            \"hidden_size\": 768,\n",
    "            \"initializer_range\": 0.02,\n",
    "            \"intermediate_size\": 3072,\n",
    "            \"layer_norm_eps\": 1e-12,\n",
    "            \"max_position_embeddings\": 512,\n",
    "            \"model_type\": \"bert\",\n",
    "            \"num_attention_heads\": 12,\n",
    "            \"num_hidden_layers\": 12,\n",
    "            \"pad_token_id\": 0,\n",
    "            \"position_embedding_type\": \"absolute\",\n",
    "            \"transformers_version\": \"4.16.2\",\n",
    "            \"type_vocab_size\": 2,\n",
    "            \"vocab_size\": 28996\n",
    "        }\n",
    "        \n",
    "        if pretrained:\n",
    "            self.model = transformers.BertForMaskedLM.from_pretrained(\"bert-base-cased\")\n",
    "            self.model.config.update(config) # Modifies self.pretrained_model.config in-place\n",
    "\n",
    "        else:\n",
    "            config = transformers.PretrainedConfig.from_dict({**default_config, **config})\n",
    "            self.model = transformers.BertForMaskedLM(config)\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        return self.model(**inputs) \n",
    "\n",
    "class BertEmbedded(Bert):\n",
    "    def forward(self, **inputs):\n",
    "        embedded = self.model.cls.predictions.transform(self.model.bert(**inputs).last_hidden_state)\n",
    "        unembedded = self.model.cls.predictions.decoder(embedded)\n",
    "        return embedded, unembedded\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = Bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The firetruck was painted a bright ___.\n",
      "\n",
      "48%\tred\n",
      "15%\tyellow\n",
      "10%\tblue\n",
      "8%\tpink\n",
      "6%\torange\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def ascii_art_probs(text, k=5):\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    mask_indices, = t.where(inputs[\"input_ids\"][0] == tokenizer.mask_token_id)\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    logits = t.nn.functional.softmax(outputs.logits, dim=2)\n",
    "    \n",
    "    top_k_masks = t.topk(logits, k, dim = 2)[1][0][mask_indices]\n",
    "\n",
    "    candidate_words = [tokenizer.decode(candidate_list).split() for candidate_list in top_k_masks]\n",
    "    candidate_percents = [logits[:,mask_index,top_k_masks[i]][0] for i, mask_index in enumerate(mask_indices)]\n",
    "    logits = logits.argmax(dim=2)\n",
    "    tokenizer.decode(logits[0])\n",
    "\n",
    "    s = text.replace('[MASK]', '___') + '\\n\\n'\n",
    "    for i, (words, percents) in enumerate(zip(candidate_words, candidate_percents)):\n",
    "        candidates = ['%d%%\\t%s' % (round(float(percent*100)), word)  for word, percent in zip(words, percents)]\n",
    "        s += '\\n'.join(candidates) + '\\n\\n'\n",
    "    print(s)\n",
    "\n",
    "text = \"The firetruck was painted a bright [MASK].\"\n",
    "ascii_art_probs(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert_tests\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, pretrained=True, **config):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        bert_config = {'attention_probs_dropout_prob': config['dropout'], **config}\n",
    "        self.bert = BertEmbedded(pretrained, bert_config)\n",
    "        self.classifier_dropout = nn.Dropout(p=config['dropout'])\n",
    "        self.classifier = nn.Linear(config['hidden_size'], config['num_classes'])\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded, unembedded = self.bert(input_ids=input_ids)\n",
    "        #logits = t.nn.functional.softmax(unembedded, dim=2)\n",
    "        classifications = self.classifier(self.classifier_dropout(embedded[:,0]))\n",
    "        return unembedded, classifications\n",
    "\n",
    "# Everything is working more or less correctly; the problem is that the random initialization of our Bert weights is just slightly different than the way they want it to be :(\n",
    "#bert_tests.test_bert_classification(BertClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Tao's implementation :(\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, **config):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = bert_tao.Bert(config)\n",
    "        self.sigmoid = t.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.bert(input_ids=input_ids)\n",
    "        logits = self.sigmoid(outputs.classification)\n",
    "        return outputs.logits, logits\n",
    "    \n",
    "#bert_tests.test_bert_classification(BertClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from tqdm import tqdm\n",
    "\n",
    "def batch(data, batch_size):\n",
    "    batches, batch = [], []\n",
    "    for i, sample in enumerate(data, 1):\n",
    "        if i % batch_size == 0:\n",
    "            batches.append(batch)\n",
    "            batch = []\n",
    "        batch.append(sample)\n",
    "\n",
    "    batches.append(batch)\n",
    "    return batches\n",
    "\n",
    "def tokenize_batch(batch, tokenizer, max_seq_len):\n",
    "    sentiments, texts = zip(*batch)\n",
    "    outputs = tokenizer(texts, return_tensors=\"pt\", padding='longest', max_length=max_seq_len, truncation=True)\n",
    "    return list(zip(sentiments, outputs['input_ids']))\n",
    "\n",
    "def tokenize(batches, max_seq_len=512):\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')\n",
    "    return [tokenize_batch(batch, tokenizer, max_seq_len) for batch in tqdm(batches)]\n",
    "\n",
    "def convert_to_int(batches):\n",
    "    conv_dict = {\n",
    "        \"pos\": 1, \n",
    "        \"neg\": 0\n",
    "    }\n",
    "    return [\n",
    "            [(conv_dict[sentiment], text) for sentiment,text in batch] \n",
    "            for batch in batches\n",
    "            ]\n",
    "\n",
    "def preprocess(data, batch_size, max_seq_len=512):\n",
    "    \n",
    "    batched_data = batch(data, batch_size)\n",
    "    random.shuffle(batched_data)\n",
    "    tokenized = tokenize(batched_data, max_seq_len)\n",
    "    preprocessed = convert_to_int(tokenized)\n",
    "    \n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [01:37<00:00,  8.06it/s]\n",
      "100%|██████████| 782/782 [01:34<00:00,  8.31it/s]\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test = torchtext.datasets.IMDB(root='.data', split=('train', 'test'))\n",
    "\n",
    "data_train_list = list(data_train)\n",
    "data_test_list = list(data_test)\n",
    "\n",
    "tokenized_train_batches = preprocess(data_train_list, 32)\n",
    "tokenized_test_batches = preprocess(data_test_list, 32)\n",
    "\n",
    "train, test = tokenized_train_batches, tokenized_test_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "default_config = {\n",
    "        \"vocab_size\": 28996,\n",
    "        \"intermediate_size\": 3072,\n",
    "        \"hidden_size\": 768,\n",
    "        \"num_classes\": 1,\n",
    "        \"num_layers\": 12,\n",
    "        \"num_heads\": 12,\n",
    "        \"max_position_embeddings\": 512,\n",
    "        \"dropout\": 0.1,\n",
    "        \"type_vocab_size\": 2,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One question for Max: why should we expect GPT2 to produce the next token in a string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 272/782 [03:18<06:15,  1.36it/s, acc=0.6]   "
     ]
    }
   ],
   "source": [
    "\n",
    "mem = lambda: t.cuda.memory_allocated() / 2**(30) # Current GPU memory usage in GiB\n",
    "acc = lambda outputs, targets: sum((outputs > 0.5).squeeze().int() == targets) / len(outputs) # Accuracy score\n",
    "\n",
    "gpu = t.device('cuda') \n",
    "\n",
    "pretrained_bert = BertClassifier(**default_config)\n",
    "pretrained_bert.cuda()\n",
    "\n",
    "optimizer = t.optim.Adam(pretrained_bert.parameters(), lr=1e-5)\n",
    "loss_func = t.nn.BCELoss()\n",
    "\n",
    "n_epochs = 1\n",
    "batches_per_epoch = None # None means use all the batches\n",
    "record_loss_batches = 1\n",
    "\n",
    "loss_vals, accs = [], []\n",
    "running_loss = 0\n",
    "\n",
    "def batch_generator(batched_data, n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        for i, batch in enumerate(batched_data):\n",
    "            yield i, batch\n",
    "\n",
    "batches = batch_generator(list(zip(train, test))[:batches_per_epoch], n_epochs)\n",
    "batches_tqdm = tqdm(batches, total = n_epochs * (batches_per_epoch if batches_per_epoch else len(train)))\n",
    "\n",
    "for i, (train_batch, test_batch) in batches_tqdm:\n",
    "\n",
    "    targets, inputs = zip(*train_batch)\n",
    "    targets = t.Tensor(targets).to(gpu)\n",
    "    inputs = t.stack(inputs).to(gpu)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    _, outputs = pretrained_bert(inputs)\n",
    "    loss = loss_func(outputs, targets[:, None])\n",
    "    running_loss += loss.detach()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % record_loss_batches == 0:\n",
    "        \n",
    "        with t.no_grad():\n",
    "            \n",
    "            test_targets, test_inputs = zip(*train_batch)\n",
    "            test_targets = t.Tensor(test_targets).to(gpu)\n",
    "            test_inputs = t.stack(test_inputs).to(gpu)\n",
    "            \n",
    "            _, test_outputs = pretrained_bert(test_inputs)\n",
    "            accs.append(float(acc(test_outputs, test_targets).cpu().detach()))\n",
    "            loss_vals.append(running_loss / record_loss_batches)\n",
    "            running_loss = 0.0 \n",
    "            \n",
    "            batches_tqdm.set_postfix({'acc': sum(accs[-5:]) / min(len(accs), 5)})\n",
    "        \n",
    "t.save(pretrained_bert.state_dict(), './bert-classifier-2-18.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd73758ebb0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot([loss_tensor.cpu().detach().numpy() for loss_tensor in loss_vals])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved model and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = t.device('cuda') \n",
    "model = t.load('./bert-classifier-2-17.pt', gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weights = model\n",
    "pretrained_bert = BertClassifier(**default_config)\n",
    "pretrained_bert\n",
    "\n",
    "pretrained_bert.load_state_dict(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_cpu = pretrained_bert.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d0f2ba7a846e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_cpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0maccs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-fe1ad81d5a84>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlab/days/w2d2/bert_tao.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids)\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         )\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencodings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         classification = self.classification_head(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlab/days/w2d2/bert_tao.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, token_activations, attention_masks)\u001b[0m\n\u001b[1;32m    165\u001b[0m         attention_output = self.layer_norm(\n\u001b[1;32m    166\u001b[0m             \u001b[0mtoken_activations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_activations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         )\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlab/days/w2d2/bert_tao.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, token_activations, attention_masks)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_activations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_masks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         return multi_head_self_attention(\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0mtoken_activations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;31m# attention_masks,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlab/days/w2d2/bert_tao.py\u001b[0m in \u001b[0;36mmulti_head_self_attention\u001b[0;34m(token_activations, num_heads, attention_pattern, project_value, project_out, dropout)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m#     attention_raw = attention_raw * attention_masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mattention_patterns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mattention_patterns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_patterns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproject_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_activations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[0;34m\"but got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "acc = lambda outputs, targets: sum((outputs > 0.5).squeeze().int() == targets) / len(inputs)\n",
    "\n",
    "accs = []\n",
    "for i, data in enumerate(test[0:10]):\n",
    "    targets, inputs = zip(*data)\n",
    "    targets = t.Tensor(targets)\n",
    "    inputs = t.stack(inputs)\n",
    "    _, outputs = bert_cpu(inputs)\n",
    "    \n",
    "    accs.append(acc(outputs, targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
