{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTWithValueHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = transformers.GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        d_model = self.model.transformer.wte.weight.shape[-1]\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(d_model, 4*d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*d_model, 1)\n",
    "        )\n",
    "        self.generate = self.model.generate\n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.model(input_ids, output_hidden_states = True)\n",
    "        logits = outputs.logits\n",
    "        values = self.value_head(outputs.hidden_states[-1]).squeeze(-1)\n",
    "        return logits, values\n",
    "\n",
    "ref_model = GPTWithValueHead().to(device)\n",
    "ref_model.eval()\n",
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(model, input_ids, batch_size=50, gen_len=10):\n",
    "    with torch.no_grad():\n",
    "        samples = model.generate(input_ids, max_length=input_ids.shape[-1]+gen_len, min_length=input_ids.shape[-1]+gen_len, do_sample=True, temperature=0.6, top_k=len(tokenizer), top_p=1.0, num_return_sequences=batch_size)\n",
    "        gen_samples = samples[:, input_ids.shape[-1]:]\n",
    "        sample_ids = copy.deepcopy(samples)\n",
    "        samples = tokenizer.batch_decode(samples)\n",
    "        gen_samples = tokenizer.batch_decode(gen_samples)\n",
    "    return sample_ids, samples, gen_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode('Testing', return_tensors='pt').to(device)\n",
    "sample_ids, samples, gen_samples =  get_samples(model=ref_model, input_ids=input_ids, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_fn(gen_sample):\n",
    "    if isinstance(gen_sample, list):\n",
    "        return [reward_fn(item) for item in gen_sample]\n",
    "    else:\n",
    "        return gen_sample.count('.')\n",
    "\n",
    "def reward_fn_test():\n",
    "    A = 'This is a test.'\n",
    "    assert reward_fn(A) == 1\n",
    "    B = '......'\n",
    "    assert reward_fn(B) ==6\n",
    "    C = 'Whatever'\n",
    "    assert reward_fn(C) == 0\n",
    "    assert reward_fn([A, B, C]) == [1, 6, 0]\n",
    "\n",
    "    print('Passed test.')\n",
    "    return\n",
    "\n",
    "reward_fn_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logprobs(input_ids, logits):\n",
    "    logprobs = F.log_softmax(logits, dim=-1)\n",
    "    logprobs = torch.gather(logprobs, -1, input_ids[:,:,None])[:,:,0]\n",
    "    return logprobs\n",
    "\n",
    "def logprobs_test(logprobs_fn):\n",
    "    input_ids = torch.randint(0, 100, (10, 10))\n",
    "    logits = torch.randn(10, 10, 100)\n",
    "    ref_logprobs = get_logprobs(input_ids, logits)\n",
    "    logprobs = logprobs_fn(input_ids, logits)\n",
    "    assert torch.allclose(logprobs, ref_logprobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kl(logits, ref_logits, eps=1e-4):\n",
    "    ref_probs = torch.nn.functional.softmax(ref_logits, dim=-1)\n",
    "    return (ref_probs * (torch.log(ref_probs)-F.log_softmax(logits, dim=-1))).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(logits):\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    log_probs = torch.log(probs)\n",
    "    entropy = -(probs * log_probs).sum(dim=-1)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_scheduler(warmup_steps, total_steps, final_scale):\n",
    "    def lr_scheduler(step):\n",
    "        if step < warmup_steps:\n",
    "            return step / warmup_steps\n",
    "        else:\n",
    "            return 1-(1-final_scale)*(step-warmup_steps)/(total_steps-warmup_steps)\n",
    "    return lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whiten(t, eps=1e-5):\n",
    "    t = t - t.mean()\n",
    "    t = t/(t.std()+eps)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatches(sample_ids, old_logprobs, ref_logits, old_values, rewards):\n",
    "    sample_ids = einops.rearrange(sample_ids,'(m b) t -> m b t', b=minibatch_size)\n",
    "    old_logprobs = einops.rearrange(old_logprobs, '(m b) t -> m b t', b=minibatch_size)\n",
    "    ref_logits = einops.rearrange(ref_logits, '(m b) t d -> m b t d', b=minibatch_size)\n",
    "    old_values = einops.rearrange(old_values, '(m b) t -> m b t', b=minibatch_size)\n",
    "    rewards = einops.rearrange(rewards, '(m b) -> m b', b=minibatch_size)\n",
    "    for i in range(batch_size//minibatch_size):\n",
    "        yield {'sample_ids': sample_ids[i], 'old_logprobs': old_logprobs[i], 'ref_logits': ref_logits[i], 'old_values': old_values[i], 'rewards': rewards[i]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMET_API_KEY = \"your key goes here!\"\n",
    "\n",
    "n_minibatches_per_epoch = 4\n",
    "minibatch_size=20\n",
    "n_epochs = 40\n",
    "ent_coef = .001\n",
    "kl_coef = .2\n",
    "vf_coef = .3\n",
    "n_steps = 300\n",
    "warmup_steps = 10\n",
    "lr = 3e-5\n",
    "gen_len=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(\n",
    "        api_key=COMET_API_KEY,\n",
    "        project_name='hf-rl',\n",
    "        workspace=\"redwood\",\n",
    "        log_env_cpu=False,\n",
    "        log_env_gpu=False,\n",
    "    )\n",
    "\n",
    "model = GPTWithValueHead().to(device)\n",
    "prefix = 'This is'\n",
    "input_ids = tokenizer(prefix, return_tensors='pt').input_ids.to(device)\n",
    "\n",
    "batch_size = minibatch_size*n_minibatches_per_epoch\n",
    "prefix_len=input_ids.shape[-1]\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = get_lr_scheduler(5, n_steps, 0.1)\n",
    "lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=scheduler)\n",
    "\n",
    "def get_loss(sample_ids, old_logprobs, ref_logits, old_values, rewards, prefix_len, clip_range=.2):\n",
    "    logits, est_values = model(sample_ids)\n",
    "    logprobs = get_logprobs(sample_ids[:,prefix_len:], logits[:,prefix_len-1:-1])\n",
    "    \n",
    "    entropy = get_entropy(logits[:,prefix_len-1:-1])\n",
    "    ent_loss = -entropy.mean()\n",
    "\n",
    "    kl = get_kl(logits, ref_logits)[:,prefix_len-1:-1]\n",
    "    kl_loss = kl.mean()\n",
    "    \n",
    "    def get_advantages(values, prefix_len):\n",
    "        one_step_q_est = torch.cat((est_values[:,prefix_len:-1].detach(), rewards[:,None]), dim=-1)\n",
    "        # s_0a_0r_0s_1a_1r_1s_2a_2r_2s_3a_3r_3s_4\n",
    "        #          v_1 ---- v_2 ---- v_3 ---- 0\n",
    "        #        + 0   ---- 0   ---- 0   ---- r_3\n",
    "    \n",
    "        zero_step_value_est = est_values[:,prefix_len-1:-1]\n",
    "        # s_0a_0r_0s_1a_1r_1s_2a_2r_2s_3a_3r_3s_4\n",
    "        # v_0 ---- v_1 ---- v_2 ---- v_3\n",
    "    \n",
    "        advantages = one_step_q_est - zero_step_value_est\n",
    "        \n",
    "        return advantages\n",
    "\n",
    "    advantages = get_advantages(est_values, prefix_len)\n",
    "\n",
    "    vf_loss = (advantages**2).mean()\n",
    "\n",
    "\n",
    "    ratio = torch.exp(logprobs - old_logprobs)\n",
    "    pg_losses1 = -advantages.detach() * ratio\n",
    "    pg_losses2 = -advantages.detach() * torch.clamp(ratio,\n",
    "                                            1.0 - clip_range,\n",
    "                                            1.0 + clip_range)\n",
    "    pg_loss = torch.max(pg_losses1, pg_losses2).mean()\n",
    "    pg_clipfrac = torch.mean(torch.gt(pg_losses2, pg_losses1).double())\n",
    "\n",
    "    loss = pg_loss + vf_coef*vf_loss + kl_coef*kl_loss + ent_coef*ent_loss\n",
    "    \n",
    "\n",
    "    experiment.log_metric('pg_clipfrac', pg_clipfrac.item())\n",
    "    experiment.log_metric('vf_loss', vf_loss.item())\n",
    "    experiment.log_metric('mean kl', kl_loss.item())\n",
    "    experiment.log_metric('total_loss', loss.item())\n",
    "    experiment.log_metric('lr', lr_scheduler.get_last_lr()[0])\n",
    "    experiment.log_metric('pg_loss', pg_loss.item())\n",
    "    experiment.log_metric('mean entropy', -ent_loss.item())\n",
    "\n",
    "    return loss\n",
    "    \n",
    "\n",
    "for batch_idx in range(n_steps):\n",
    "    sample_ids, samples, gen_samples = get_samples(model, input_ids=input_ids, batch_size=batch_size, gen_len=gen_len)\n",
    "    sample_ids = sample_ids.to(device)\n",
    "    experiment.log_text(gen_samples[0])\n",
    "    old_logits, old_values = model(sample_ids)\n",
    "    old_logits, old_values = old_logits.detach(), old_values.detach()\n",
    "    old_logprobs = get_logprobs(sample_ids[:,input_ids.shape[-1]:], old_logits[:,input_ids.shape[-1]-1:-1]).detach()\n",
    "    ref_logits, _ = ref_model(sample_ids)\n",
    "    ref_logits = ref_logits.detach()\n",
    "\n",
    "    rewards = torch.tensor(reward_fn(samples), dtype=torch.float32).to(device)\n",
    "    experiment.log_metric('mean_reward', rewards.mean())\n",
    "    rewards = whiten(rewards)  \n",
    "    \n",
    "    for epoch in range(1):\n",
    "        for minibatch in get_minibatches(sample_ids, old_logprobs, ref_logits, old_values, rewards):\n",
    "            loss = get_loss(**minibatch, prefix_len=2, clip_range=.2)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0, norm_type=2.0, error_if_nonfinite=True)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "experiment.end()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_model = transformers.AutoModelForSequenceClassification.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment').to(device)\n",
    "cls_tokenizer = transformers.AutoTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment')\n",
    "\n",
    "def reward_fn(gen_sample):\n",
    "    if isinstance(gen_sample, list):\n",
    "        return [reward_fn(item) for item in gen_sample]\n",
    "    else:\n",
    "        logits = cls_model(cls_tokenizer(gen_sample, return_tensors='pt')['input_ids'].to(device)).logits[0]\n",
    "        logprobs = F.log_softmax(logits, dim=0)\n",
    "        assert logprobs.shape == (3,)\n",
    "        return float(logprobs[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
