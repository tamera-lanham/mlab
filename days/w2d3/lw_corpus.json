[{"text": "This is a transcript of a conversation between Paul Christiano and Eliezer Yudkowsky, with comments by Rohin Shah, Beth Barnes, Richard Ngo, and Holden Karnofsky, continuing the Late 2021 MIRI Conversations.Color key:\u00a0Chat by Paul and Eliezer\u00a0\u00a0Other chat\u00a0\u00a015. October 19 comment\u00a0[Yudkowsky][11:01]\u00a0thing that struck me as an iota of evidence for Paul over Eliezer: https://twitter.com/tamaybes/status/1450514423823560706?s=20\u00a0\u00a016. November 3 conversation\u00a016.1. EfficientZero\u00a0[Yudkowsky][9:30]\u00a0Thing that (if true) strikes me as... straight-up falsifying Paul's view as applied to modern-day AI, at the frontier of the most AGI-ish part of it and where Deepmind put in substantial effort on their project? \u00a0EfficientZero (allegedly) learns Atari in 100,000 frames. \u00a0Caveat: I'm not having an easy time figuring out how many frames MuZero would've required to achieve the same performance level. \u00a0MuZero was trained on 200,000,000 frames but reached what looks like an allegedly higher high; the EfficientZero paper compares their performance to MuZero on 100,000 frames, and claims theirs is much better than MuZero given only that many frames.https://arxiv.org/pdf/2111.00210.pdf \u00a0CC: @paulfchristiano.(I would further argue that this case is important because it's about the central contemporary model for approaching AGI, at least according to Eliezer, rather than any number of random peripheral AI tasks.)[Shah][14:46] \u00a0I only looked at the front page, so might be misunderstanding, but the front figure says \"Our proposed method EfficientZero is 170% and 180% better than the previous SoTA performance in mean and median human normalized score [...] on the Atari 100k benchmark\", which does not seem like a huge leap?Oh, I incorrectly thought that was 1.7x and 1.8x, but it is actually 2.7x and 2.8x, which is a bigger deal (though still feels not crazy to me)[Yudkowsky][15:28] \u00a0the question imo is how many frames the previous SoTA would require to catch up to EfficientZero(I've tried emailing an author to ask about this, no response yet)like, perplexity on GPT-3 vs GPT-2 and \"losses decreased by blah%\" would give you a pretty meaningless concept of how far ahead GPT-3 was from GPT-2, and I think the \"2.8x performance\" figure in terms of scoring is equally meaningless as a metric of how much EfficientZero improves if anywhat you want is a notion like \"previous SoTA would have required 10x the samples\" or \"previous SoTA would have required 5x the computation\" to achieve that performance level[Shah][15:38] \u00a0I see. Atari curves are not nearly as nice and stable as GPT curves and often have the problem that they plateau rather than making steady progress with more training time, so that will make these metrics noisier, but it does seem like a reasonable metric to track(Not that I have recommendations about how to track it; I doubt the authors can easily get these metrics)[Christiano][18:01]\u00a0If you think our views are making such starkly different predictions then I'd be happy to actually state any of them in advance, including e.g. about future ML benchmark results.I don't think this falsifies my view, and we could continue trying to hash out what my view is but it seems like slow going and I'm inclined to give up.Relevant questions on my view are things like: is MuZero optimized at all for performance in the tiny-sample regime? (I think not, I don't even think it set SoTA on that task and I haven't seen any evidence.) What's the actual rate of improvements since people started studying this benchmark ~2 years ago, and how much work has gone into it? And I totally agree with your comments that \"# of frames\" is the natural unit for measuring and that would be the starting point for any discussion.[Barnes][18:22]\u00a0In previous MCTS RL algorithms, the environment model is either given or only trained with rewards, values, and policies, which cannot provide sufficient training signals due to their scalar nature. The problem is more severe when the reward is sparse or the bootstrapped value is not accurate. The MCTS policy improvement operator heavily relies on the environment model. Thus, it is vital to have an accurate one.We notice that the output\u00a0^st+1\u00a0from the dynamic function\u00a0G\u00a0should be the same as\u00a0st+1, i.e. the output of the representation function\u00a0H\u00a0with input of the next observation\u00a0ot+1\u00a0(Fig. 2). This can help to supervise the predicted next state\u00a0^st+1\u00a0using the actual\u00a0st+1, which is a tensor with at least a few hundred dimensions. This provides\u00a0^st+1\u00a0with much more training signals than the default scalar reward and value.This seems like a super obvious thing to do and I'm confused why DM didn't already try this. It was definitely being talked about in ~2018Will ask a DM friend about it[Yudkowsky][22:45]\u00a0I... don't think I want to take all of the blame for misunderstanding Paul's views; I think I also want to complain at least a little that Paul spends an insufficient quantity of time pointing at extremely concrete specific possibilities, especially real ones, and saying how they do or don't fit into the scheme.Am I rephrasing correctly that, in this case, if Efficient Zero was actually a huge (3x? 5x? 10x?) jump in RL sample efficiency over previous SOTA, measured in 1 / frames required to train to a performance level, then that means the Paul view doesn't apply to the present world; but this could be because MuZero wasn't the real previous SOTA, or maybe because nobody really worked on pushing out this benchmark for 2 years and therefore on the Paul view it's fine for there to still be huge jumps? \u00a0In other words, this is something Paul's worldview has to either defy or excuse, and not just, \"well, sure, why wouldn't it do that, you have misunderstood which kinds of AI-related events Paul is even trying to talk about\"?In the case where, \"yes it's a big jump and that shouldn't happen later, but it could happen now because it turned out nobody worked hard on pushing past MuZero over the last 2 years\", I wish to register that my view permits it to be the case that, when the world begins to end, the frontier that enters into AGI is similarly something that not a lot of people spent a huge effort on since a previous prototype from 2 years earlier. \u00a0It's just not very surprising to me if the future looks a lot like the past, or if human civilization neglects to invest a ton of effort in a research frontier.Gwern guesses that getting to EfficientZero's performance level would require around 4x the samples for MuZero-Reanalyze (the more efficient version of MuZero which replayed past frames), which is also apparently the only version of MuZero the paper's authors were considering in the first place - without replays, MuZero requires 20 billion frames to achieve its performance, not the figure of 200 million. https://www.lesswrong.com/posts/jYNT3Qihn2aAYaaPb/efficientzero-human-ale-sample-efficiency-w-muzero-self?commentId=JEHPQa7i8Qjcg7TW6\u00a017. November 4 conversation\u00a017.1. EfficientZero (continued)\u00a0[Christiano][7:42]\u00a0I think it's possible the biggest misunderstanding is that you somehow think of my view as a \"scheme\" and your view as a normal view where probability distributions over things happen.Concretely, this is a paper that adds a few techniques to improve over MuZero in a domain that (it appears) wasn't a significant focus of MuZero. I don't know how much it improves but I can believe gwern's estimates of 4x.I'd guess MuZero itself is a 2x improvement over the baseline from a year ago, which was maybe a 4x improvement over the algorithm from a year before that.If that's right, then no it's not mindblowing on my view to have 4x progress one year, 2x progress the next, and 4x progress the next.If other algorithms were better than MuZero, then the 2019-2020 progress would be >2x and the 2020-2021 progress would be <4x.I think it's probably >4x sample efficiency though (I don't totally buy gwern's estimate there), which makes it at least possibly surprising.But it's never going to be that surprising. It's a benchmark that people have been working on for a few years that has been seeing relatively rapid improvement over that whole period.The main innovation is how quickly you can learn to predict future frames of Atari games, which has tiny economic relevance and calling it the most AGI-ish direction seems like it's a very Eliezer-ish view, this isn't the kind of domain where I'm either most surprised to see rapid progress at all nor is the kind of thing that seems like a key update re: transformative AIyeah, SoTA in late 2020 was SPR, published by a much smaller academic group: https://arxiv.org/pdf/2007.05929.pdfMuZero wasn't even setting sota on this task at the time it was publishedmy \"schemes\" are that (i) if a bunch of people are trying on a domain and making steady slow progress, I'm surprised to see giant jumps and I don't expect most absolute progress to occur in such jumps, (ii) if a domain is worth a lot of $, generally a bunch of people will be trying. Those aren't claims about what is always true, they are claims about what is typically true and hence what I'm guessing will be true for transformative AI.Maybe you think those things aren't even good general predictions, and that I don't have long enough tails in my distributions or whatever. But in that case it seems we can settle it quickly by prediction.I think this result is probably significant (>30% absolute improvement) + faster-than-trend (>50% faster than previous increment) progress relative to prior trend on 8 of the 27 atari games (from table 1, treating SimPL->{max of MuZero, SPR}->EfficientZero as 3 equally spaced datapoints): Asterix, Breakout, almost ChopperCMD, almost CrazyClimber, Gopher, Kung Fu Master, Pong, QBert, SeaQuest. My guess is that they thought a lot about a few of those games in particular because they are very influential on the mean/median. Note that this paper is a giant grab bag and that simply stapling together the prior methods would have already been a significant improvement over prior SoTA. (ETA: I don't think saying \"its only 8 of 27 games\" is an update against it being big progress or anything. I do think saying \"stapling together 2 previous methods without any complementarity at all would already have significantly beaten SoTA\" is fairly good evidence that it's not a hard-to-beat SoTA.)and even fewer people working on the ultra-low-sample extremely-low-dimensional DM control environments (this is the subset of problems where the state space is 4 dimensions, people are just not trying to publish great results on cartpole), so I think the most surprising contribution is the atari stuffOK, I now also understand what the result is I think?I think the quick summary is: the prior SoTA is SPR, which learns to predict the domain and then does Q-learning. MuZero instead learns to predict the domain and does MCTS, but it predicts the domain in a slightly less sophisticated way than SPR (basically just predicts rewards, whereas SPR predicts all of the agent's latent state in order to get more signal from each frame). If you combine MCTS with more sophisticated prediction, you do better.I think if you told me that DeepMind put in significant effort in 2020 (say, at least as much post-MuZero effort as the new paper?) trying to get great sample efficiency on the easy-exploration atari games, and failed to make significant progress, then I'm surprised.I don't think that would \"falsify\" my view, but it would be an update against? Like maybe if DM put in that much effort I'd maybe have given only a 10-20% probability to a new project of similar size putting in that much effort making big progress, and even conditioned on big progress this is still >>median (ETA: and if DeepMind put in much more effort I'd be more surprised than 10-20% by big progress from the new project)Without DM putting in much effort, it's significantly less surprising and I'll instead be comparing to the other academic efforts. But it's just not surprising that you can beat them if you are willing to put in the effort to reimplement MCTS and they aren't, and that's a step that is straightforwardly going to improve performance.(not sure if that's the situation)And then to see how significant updates against are, you have to actually contrast them with all the updates in the other direction where people don't crush previous benchmark resultsand instead just make modest progressI would guess that if you had talked to an academic about this question (what happens if you combine SPR+MCTS) they would have predicted significant wins in sample efficiency (at the expense of compute efficiency) and cited the difficulty of implementing MuZero compared to any of the academic results. That's another way I could be somewhat surprised (or if there were academics with MuZero-quality MCTS implementations working on this problem, and they somehow didn't set SoTA, then I'm even more surprised). But I'm not sure if you'll trust any of those judgments in hindsight.Repeating the main point:I don't really think a 4x jump over 1 year is something I have to \"defy or excuse\", it's something that I think becomes more or less likely depending on facts about the world, like (i) how fast was previous progress, (ii) how many people were working on previous projects and how targeted were they at this metric, (iii) how many people are working in this project and how targeted was it at this metricit becomes continuously less likely as those parameters move in the obvious directionsit never becomes 0 probability, and you just can't win that much by citing isolated events that I'd give say a 10% probability to, unless you actually say something about how you are giving >10% probabilities to those events without losing a bunch of probability mass on what I see as the 90% of boring stuff[Ngo: \ud83d\udc4d]and then separately I have a view about lots of people working on important problems, which doesn't say anything about this case(I actually don't think this event is as low as 10%, though it depends on what background facts about the project you are conditioning on---obviously I gave <<10% probability to someone publishing this particular result, but something like \"what fraction of progress in this field would come down to jumps like this\" or whatever is probably >10% until you tell me that DeepMind actually cared enough to have already tried)[Ngo][8:48]\u00a0I expect Eliezer to say something like: DeepMind believes that both improving RL sample efficiency, and benchmarking progress on games like Atari, are important parts of the path towards AGI. So insofar as your model predicts that smooth progress will be caused by people working directly towards AGI, DeepMind not putting effort into this is a hit to that model. Thoughts?[Christiano][9:06]\u00a0I don't think that learning these Atari games in 2 hours is a very interesting benchmark even for deep RL sample efficiency, and it's totally unrelated to the way in which humans learn such games quickly. It seems pretty likely totally plausible (50%?) to me that DeepMind feels the same way, and then the question is about other random considerations like how they are making some PR calculation.[Ngo][9:18]\u00a0If Atari is not a very interesting benchmark, then why did DeepMind put a bunch of effort into making Agent57 and applying MuZero to Atari?Also, most of the effort they've spent on games in general has been on methods very unlike the way humans learn those games, so that doesn't seem like a likely reason for them to overlook these methods for increasing sample efficiency.[Shah][9:32]\u00a0It seems pretty likely totally plausible (50%?) to me that DeepMind feels the same way, and then the question is about other random considerations like how they are making some PR calculation.Not sure of the exact claim, but DeepMind is big enough and diverse enough that I'm pretty confident at least some people working on relevant problems don't feel the same way[...] This seems like a super obvious thing to do and I'm confused why DM didn't already try this. It was definitely being talked about in ~2018Speculating without my DM hat on: maybe it kills performance in board games, and they want one algorithm for all settings?[Christiano][10:29]\u00a0Atari games in the tiny sample regime are a different beastthere are just a lot of problems you can state about Atari some of which are more or less interesting (e.g. jointly learning to play 57 Atari games is a more interesting problem than learning how to play one of them absurdly quickly, and there are like 10 other problems about Atari that are more interesting than this one)That said, Agent57 also doesn't seem interesting except that it's an old task people kind of care about. I don't know about the take within DeepMind but outside I don't think anyone would care about it other than historical significance of the benchmark / obviously-not-cherrypickedness of the problem.I'm sure that some people at DeepMind care about getting the super low sample complexity regime. I don't think that really tells you how large the DeepMind effort is compared to some random academics who care about it.[Shah: \ud83d\udc4d]I think the argument for working on deep RL is fine and can be based on an analogy with humans while you aren't good at the task. Then once you are aiming for crazy superhuman performance on Atari games you naturally start asking \"what are we doing here and why are we still working on atari games?\"[Ngo: \ud83d\udc4d]and correspondingly they are a smaller and smaller slice of DeepMind's work over time[Ngo: \ud83d\udc4d](e.g. Agent57 and MuZero are the only DeepMind blog posts about Atari in the last 4 years, it's not the main focus of MuZero and I don't think Agent57 is a very big DM project)Reaching this level of performance in Atari games is largely about learning perception, and doing that from 100k frames of an Atari game just doesn't seem very analogous to anything humans do or that is economically relevant from any perspective. I totally agree some people are into it, but I'm totally not surprised if it's not going to be a big DeepMind project.[Yudkowsky][10:51]\u00a0would you agree it's a load-bearing assumption of your worldview - where I also freely admit to having a worldview/scheme, this is not meant to be a prejudicial term at all - that the line of research which leads into world-shaking AGI must be in the mainstream and not in a weird corner where a few months earlier there were more profitable other ways of doing all the things that weird corner did?\u00a0eg, the tech line leading into world-shaking AGI must be at the profitable forefront of non-world-shaking tasks. \u00a0as otherwise, afaict, your worldview permits that if counterfactually we were in the Paul-forbidden case where the immediate precursor to AGI was something like EfficientZero (whose motivation had been beating an old SOTA metric rather than, say, market-beating self-driving cars), there might be huge capability leaps there just as EfficientZero represents a large leap, because there wouldn't have been tons of investment in that line.[Christiano][10:54]\u00a0Something like that is definitely a load-bearing assumptionLike there's a spectrum with e.g. EfficientZero --> 2016 language modeling --> 2014 computer vision --> 2021 language modeling --> 2021 computer vision, and I think everything anywhere close to transformative AI will be way way off the right end of that spectrumBut I think quantitatively the things you are saying don't seem quite right to me. Suppose that MuZero wasn't the best way to do anything economically relevant, but it was within a factor of 4 on sample efficiency for doing tasks that people care about. That's already going to be enough to make tons of people extremely excited.So yes, I'm saying that anything leading to transformative AI is \"in the mainstream\" in the sense that it has more work on it than 2021 language models.But not necessarily that it's the most profitable way to do anything that people care about. Different methods scale in different ways, and something can burst onto the scene in a dramatic way, but I strongly expect speculative investment driven by that possibility to already be way (way) more than 2021 language models. And I don't expect gigantic surprises. And I'm willing to bet that e.g. EfficientZero isn't a big surprise for researchers who are paying attention to the area (in addition to being 3+ orders of magnitude more neglected than anything close to transformative AI)2021 language modeling isn't even very competitive, it's still like 3-4 orders of magnitude smaller than semiconductors. But I'm giving it as a reference point since it's obviously much, much more competitive than sample-efficient atari.This is a place where I'm making much more confident predictions, this is \"falsify paul's worldview\" territory once you get to quantitative claims anywhere close to TAI and \"even a single example seriously challenges paul's worldview\" a few orders of magnitude short of that[Yudkowsky][11:04]\u00a0can you say more about what falsifies your worldview previous to TAI being super-obviously-to-all-EAs imminent?or rather, \"seriously challenges\", sorry[Christiano][11:05][11:08]\u00a0big AI applications achieved by clever insights in domains that aren't crowded, we should be quantitative about how crowded and how big if we want to get into \"seriously challenges\"like e.g. if this paper on atari was actually a crucial ingredient for making deep RL for robotics work, I'd be actually for real surprised rather than 10% surprisedbut it's not going to be, those results are being worked on by much larger teams of more competent researchers at labs with $100M+ fundingit's definitely possible for them to get crushed by something out of left fieldbut I'm betting against every timeor like, the set of things people would describe as \"out of left field,\" and the quantitative degree of neglectedness, becomes more and more mild as the stakes go up[Yudkowsky][11:08]\u00a0how surprised are you if in 2022 one company comes out with really good ML translation, and they manage to sell a bunch of it temporarily until others steal their ideas or Google acquires them? \u00a0my model of Paul is unclear on whether this constitutes \"many people are already working on language models including ML translation\" versus \"this field is not profitable enough right this minute for things to be efficient there, and it's allowed to be nonobvious in worlds where it's about to become profitable\".[Christiano][11:08]\u00a0if I wanted to make a prediction about that I'd learn a bunch about how much google works on translation and how much $ they makeI just don't know the economicsand it depends on the kind of translation that they are good at and the economics (e.g. google mostly does extremely high-volume very cheap translation)but I think there are lots of things like that / facts I could learn about Google such that I'd be surprised in that situationindependent of the economics, I do think a fair number of people are working on adjacent stuff, and I don't expect someone to come out of left field for google-translate-cost translation between high-resource languagesbut it seems quite plausible that a team of 10 competent people could significantly outperform google translate, and I'd need to learn about the economics to know how surprised I am by 10 people or 100 people or whatI think it's allowed to be non-obvious whether a domain is about to be really profitablebut it's not that easy, and the higher the stakes the more speculative investment it will drive, etc.[Yudkowsky][11:14]\u00a0if you don't update much off EfficientZero, then people also shouldn't be updating much off of most of the graph I posted earlier as possible Paul-favoring evidence, because most of those SOTAs weren't highly profitable so your worldview didn't have much to say about them. ?[Christiano][11:15]\u00a0Most things people work a lot on improve gradually. EfficientZero is also quite gradual compared to the crazy TAI stories you tell. I don't really know what to say about this game other than I would prefer make predictions in advance and I'm happy to either propose questions/domains or make predictions in whatever space you feel more comfortable with.[Yudkowsky][11:16]\u00a0I don't know how to point at a future event that you'd have strong opinions about. \u00a0it feels like, whenever I try, I get told that the current world is too unlike the future conditions you expect.[Christiano][11:16]\u00a0Like, whether or not EfficientZero is evidence for your view depends on exactly how \"who knows what will happen\" you are. if you are just a bit more spread out than I am, then it's definitely evidence for your view.I'm saying that I'm willing to bet about any event you want to name, I just think my model of how things work is more accurate.I'd prefer it be related to ML or AI.[Yudkowsky][11:17]\u00a0to be clear, I appreciate that it's similarly hard to point at an event like that for myself, because my own worldview says \"well mostly the future is not all that predictable with a few rare exceptions\"[Christiano][11:17]\u00a0But I feel like the situation is not at all symmetrical, I expect to outperform you on practically any category of predictions we can specify.so like I'm happy to bet about benchmark progress in LMs, or about whether DM or OpenAI or Google or Microsoft will be the first to achieve something, or about progress in computer vision, or about progress in industrial robotics, or about translationswhatever\u00a017.2. Near-term AI predictions\u00a0[Yudkowsky][11:18]\u00a0that sounds like you ought to have, like, a full-blown storyline about the future?[Christiano][11:18]\u00a0what is a full-blown storyline? I have a bunch of ways that I think about the world and make predictions about what is likelyand yes, I can use those ways of thinking to make predictions about whateverand I will very often lose to a domain expert who has better and more informed ways of making predictions[Yudkowsky][11:19]\u00a0what happens if 2022 through 2024 looks literally exactly like Paul's modal or median predictions on things?[Christiano][11:19]\u00a0but I think in ML I will generally beat e.g. a superforecaster who doesn't have a lot of experience in the areagive me a question about 2024 and I'll give you a median?I don't know what \"what happens\" meansstorylines do not seem like good ways of making predictions[Shah: \ud83d\udc4d][Yudkowsky][11:20]\u00a0I mean, this isn't a crux for anything, but it seems like you're asking me to give up on that and just ask for predictions? \u00a0so in 2024 can I hire an artist who doesn't speak English and converse with them almost seamlessly through a machine translator?[Christiano][11:22]\u00a0median outcome (all of these are going to be somewhat easy-to-beat predictions because I'm not thinking): you can get good real-time translations, they are about as good as a +1 stdev bilingual speaker who listens to what you said and then writes it out in the other language as fast as they can typeProbably also for voice -> text or voice -> voice, though higher latencies and costs.Not integrated into standard video chatting experience because the UX is too much of a pain and the world sucks.That's a median on \"how cool/useful is translation\"[Yudkowsky][11:23]\u00a0I would unfortunately also predict that in this case, this will be a highly competitive market and hence not a very profitable one, which I predict to match your prediction, but I ask about the economics here just in case.[Christiano][11:24]\u00a0Kind of typical sample: I'd guess that Google has a reasonably large lead, most translation still provided as a free value-added, cost per translation at that level of quality is like $0.01/word, total revenue in the area is like $10Ms / year?[Yudkowsky][11:24]\u00a0well, my model also permits that Google does it for free and so it's an uncompetitive market but not a profitable one... ninjaed.[Christiano][11:25]\u00a0first order of improving would be sanity-checking economics and thinking about #s, second would be learning things like \"how many people actually work on translation and what is the state of the field?\"[Yudkowsky][11:26]\u00a0did Tesla crack self-driving cars and become a $3T company instead of a $1T company? \u00a0do you own Tesla options?did Waymo beat Tesla and cause Tesla stock to crater, same question?[Christiano][11:27]\u00a01/3 chance tesla has FSD in 2024conditioned on that, yeah probably market cap is >$3T?conditioned on Tesla having FSD, 2/3 chance Waymo has also at least rolled out to a lot of citiesconditioned on no tesla FSD, 10% chance Waymo has rolled out to like half of big US cities?dunno if numbers make sense[Yudkowsky][11:28]\u00a0that's okay, I dunno if my questions make sense[Christiano][11:29]\u00a0(5% NW in tesla, 90% NW in AI bets, 100% NW in more normal investments; no tesla options that sounds like a scary place with lottery ticket biases and the crazy tesla investors)[Yudkowsky][11:30]\u00a0(am I correctly understanding you're 2x levered?)[Christiano][11:30][11:31]\u00a0yeahit feels like you've got to have weird views on trajectory of value-added from AI over the coming yearson how much of the $ comes from domains that are currently exciting to people (e.g. that Google already works on, self-driving, industrial robotics) vs stuff out of left fieldon what kind of algorithms deliver $ in those domains (e.g. are logistics robots trained using the same techniques tons of people are currently pushing on)on my picture you shouldn't be getting big losses on any of thosejust losing like 10-20% each time[Yudkowsky][11:31][11:32]\u00a0my uncorrected inside view says that machine translation should be in reach and generate huge amounts of economic value even if it ends up an unprofitable competitive or Google-freebie fieldand also that not many people are working on basic research in machine translation or see it as a \"currently exciting\" domain[Christiano][11:32]\u00a0how many FTE is \"not that many\" people?also are you expecting improvement in the google translate style product, or in lower-latencies for something closer to normal human translator prices, or something else?[Yudkowsky][11:33]\u00a0my worldview says more like... sure, maybe there's 300 programmers working on it worldwide, but most of them aren't aggressively pursuing new ideas and trying to explore the space, they're just applying existing techniques to a new language or trying to throw on some tiny mod that lets them beat SOTA by 1.2% for a publicationbecause it's not an exciting field\"What if you could rip down the language barriers\" is an economist's dream, or a humanist's dream, and Silicon Valley is neitherand looking at GPT-3 and saying, \"God damn it, this really seems like it must on some level understand what it's reading well enough that the same learned knowledge would suffice to do really good machine translation, this must be within reach for gradient descent technology we just don't know how to reach it\" is Yudkowskian thinking; your AI system has internal parts like \"how much it understands language\" and there's thoughts about what those parts ought to be able to do if you could get them into a new system with some other parts[Christiano][11:36]\u00a0my guess is we'd have some disagreements herebut to be clear, you are talking about text-to-text at like $0.01/word price point?[Yudkowsky][11:38]\u00a0I mean, do we? \u00a0Unfortunately another Yudkowskian worldview says \"and people can go on failing to notice this for arbitrarily long amounts of time\".if that's around GPT-3's price point then yeah[Christiano][11:38]\u00a0gpt-3 is a lot cheaper, happy to say gpt-3 like price point[Yudkowsky][11:39]\u00a0(thinking about whether $0.01/word is meaningfully different from $0.001/word and concluding that it is)[Christiano][11:39]\u00a0(api is like 10,000 words / $)I expect you to have a broader distribution over who makes a great product in this space, how great it ends up being etc., whereas I'm going to have somewhat higher probabilities on it being google research and it's going to look boring[Yudkowsky][11:40]\u00a0what is boring?boring predictions are often good predictions on my own worldview toolots of my gloom is about things that are boringly bad and awful(and which add up to instant death at a later point)but, I mean, what does boring machine translation look like?[Christiano][11:42]\u00a0Train big language model. Have lots of auxiliary tasks especially involving reading in source language and generation in target language. Have pre-training on aligned sentences and perhaps using all the unsupervised translation we have depending on how high-resource language is. Fine-tune with smaller amount of higher quality supervision.Some of the steps likely don't add much value and skip them. Fair amount of non-ML infrastructure.For some languages/domains/etc. dedicated models, over time increasingly just have a giant model with learned dispatch as in mixture of experts.[Yudkowsky][11:44]\u00a0but your worldview is also totally ok with there being a Clever Trick added to that which produces a 2x reduction in training time. \u00a0or with there being a new innovation like transformers, which was developed a year earlier and which everybody now uses, without which the translator wouldn't work at all. ?[Christiano][11:44]\u00a0Just for reference, I think transformers aren't that visible on a (translation quality) vs (time) graph?But yes, I'm totally fine with continuing architectural improvements, and 2x reduction in training time is currently par for the course for \"some people at google thought about architectures for a while\" and I expect that to not get that much tighter over the next few years.[Yudkowsky][11:45]\u00a0unrolling Restricted Boltzmann Machines to produce deeper trainable networks probably wasn't much visible on a graph either, but good luck duplicating modern results using only lower portions of the tech tree. \u00a0(I don't think we disagree about this.)[Christiano][11:45]\u00a0I do expect it to eventually get tighter, but not by 2024.I don't think unrolling restricted boltzmann machines is that important[Yudkowsky][11:46]\u00a0like, historically, or as a modern technology?[Christiano][11:46]\u00a0historically[Yudkowsky][11:46]\u00a0interestingmy model is that it got people thinking about \"what makes things trainable\" and led into ReLUs and initsbut I am going more off having watched from the periphery as it happened, than having read a detailed history of thatlike, people asking, \"ah, but what if we had a deeper network and the gradients didn't explode or die out?\" and doing that en masse in a productive way rather than individuals being wistful for 30 seconds[Christiano][11:48]\u00a0well, not sure if this will introduce differences in predictionsI don't feel like it should really matter for our bottom line predictions whether we classify google's random architectural change as something fundamentally new (which happens to just have a modest effect at the time that it's built) or as something boringI'm going to guess how well things will work by looking at how well things work right now and seeing how fast it's getting betterand that's also what I'm going to do for applications of AI with transformative impactsand I actually believe you will do something today that's analogous to what you would do in the future, and in fact will make somewhat different predictions than what I would doand then some of the action will be in new things that people haven't been trying to do in the past, and I'm predicting that new things will be \"small\" whereas you have a broader distribution, and there's currently some not-communicated judgment call in \"small\"if you think that TAI will be like translation, where google publishes tons of papers, but that they will just get totally destroyed by some new idea, then it seems like that should correspond to a difference in P(google translation gets totally destroyed by something out-of-left-field)and if you think that TAI won't be like translation, then I'm interested in examples more like TAII don't really understand the take \"and people can go on failing to notice this for arbitrarily long amounts of time,\" why doesn't that also happen for TAI and therefore cause it to be the boring slow progress by google? Why would this be like a 50% probability for TAI but <10% for translation?perhaps there is a disagreement about how good the boring progress will be by 2024? looks to me like it will be very good[Yudkowsky][11:57]\u00a0I am not sure that is where the disagreement lies\u00a017.3. The evolution of human intelligence\u00a0[Yudkowsky][11:57]\u00a0I am considering advocating that we should have more disagreements about the past, which has the advantage of being very concrete, and being often checkable in further detail than either of us already knowChristiano][11:58]\u00a0I'm fine with disagreements about the past; I'm more scared of letting you pick arbitrary things to \"predict\" since there is much more impact from differences in domain knowledge(also not quite sure why it's more concrete, I guess because we can talk about what led to particular events? mostly it just seems faster)also as far as I can tell our main differences are about whether people will spend a lot of money work effectively on things that would make a lot of money, which means if we look to the past we will have to move away from ML/AI[Yudkowsky][12:00]\u00a0so my understanding of how Paul writes off the example of human intelligence, is that you are like, \"evolution is much stupider than a human investor; if there'd been humans running the genomes, people would be copying all the successful things, and hominid brains would be developing in this ecology of competitors instead of being a lone artifact\". ?[Christiano][12:00]\u00a0I don't understand why I have to write off the example of human intelligence[Yudkowsky][12:00]\u00a0because it looks nothing like your account of how TAI develops[Christiano][12:00]\u00a0it also looks nothing like your account, I understand that you have some analogy that makes sense to you[Yudkowsky][12:01]\u00a0I mean, to be clear, I also write off the example of humans developing morality and have to explain to people at length why humans being as nice as they are, doesn't imply that paperclip maximizers will be anywhere near that nice, nor that AIs will be other than paperclip maximizers.[Christiano][12:01][12:02]\u00a0you could state some property of how human intelligence developed, that is in common with your model for TAI and not mine, and then we could discuss thatif you say something like: \"chimps are not very good at doing science, but humans are\" then yes my answer will be that it's because evolution was not selecting us to be good at scienceand indeed AI systems will be good at science using much less resources than humans or chimps[Yudkowsky][12:02][12:02]\u00a0would you disagree that humans developing intelligence, on the sheer surfaces of things, looks much more Yudkowskian than Paulian?like, not in terms of compatibility with underlying modeljust that there's this one corporation that came out and massively won the entire AGI race with zero competitors[Christiano][12:03]\u00a0I agree that \"how much did the winner take all\" is more like your model of TAI than mineI don't think zero competitors is reasonable, I would say \"competitors who were tens of millions of years behind\"[Yudkowsky][12:03]\u00a0sureand your account of this is that natural selection is nothing like human corporate managers copying each other[Christiano][12:03]\u00a0which was a reasonable timescale for the old game, but a long timescale for the new game[Yudkowsky][12:03]\u00a0yup[Christiano][12:04]\u00a0that's not my only accountit's also that for human corporations you can form large coalitions, i.e. raise huge amounts of $ and hire huge numbers of people working on similar projects (whether or not vertically integrated), and those large coalitions will systematically beat small coalitionsand that's basically the key dynamic in this situation, and isn't even trying to have any analog in the historical situation(the key dynamic w.r.t. concentration of power, not necessarily the main thing overall)[Yudkowsky][12:07]\u00a0the modern degree of concentration of power seems relatively recent and to have tons and tons to do with the regulatory environment rather than underlying properties of the innovation landscapeback in the old days, small startups would be better than Microsoft at things, and Microsoft would try to crush them using other forces than superior technology, not always successfullyor such was the common wisdom of USENET[Christiano][12:08]\u00a0my point is that the evolution analogy is extremely unpersuasive w.r.t. concentration of powerI think that AI software capturing the amount of power you imagine is also kind of implausible because we know something about how hardware trades off against software progress (maybe like 1 year of progress = 2x hardware) and so even if you can't form coalitions on innovation at all you are still going to be using tons of hardware if you want to be in the runningthough if you can't parallelize innovation at all and there is enough dispersion in software progress then the people making the software could take a lot of the $ / influence from the partnershipanyway, I agree that this is a way in which evolution is more like your world than minebut think on this point the analogy is pretty unpersuasivebecause it fails to engage with any of the a priori reasons you wouldn't expect concentration of power[Yudkowsky][12:11]\u00a0I'm not sure this is the correct point on which to engage, but I feel like I should say out loud that I am unable to operate my model of your model in such fashion that it is not falsified by how the software industry behaved between 1980 and 2000.there should've been no small teams that beat big corporationstoday those are much rarer, but on my model, that's because of regulatory changes (and possibly metabolic damage from something in the drinking water)[Christiano][12:12]\u00a0I understand that you can't operate my model, and I've mostly given up, and on this point I would prefer to just make predictions or maybe retrodictions[Yudkowsky][12:13]\u00a0well, anyways, my model of how human intelligence happened looks like this:there is a mysterious kind of product which we can call G, and which brains can operate as factories to produceG in turn can produce other stuff, but you need quite a lot of it piled up to produce better stuff than your competitorsas late as 1000 years ago, the fastest creatures on Earth are not humans, because you need even more G than that to go faster than cheetahs(or peregrine falcons)the natural selections of various species were fundamentally stupid and blind, incapable of foresight and incapable of copying the successes of other natural selections; but even if they had been as foresightful as a modern manager or investor, they might have made just the same mistakebefore 10,000 years they would be like, \"what's so exciting about these things? they're not the fastest runners.\"if there'd been an economy centered around running, you wouldn't invest in deploying a human(well, unless you needed a stamina runner, but that's something of a separate issue, let's consider just running races)you would invest on improving cheetahsbecause the pile of human G isn't large enough that their G beats a specialized naturally selected cheetah[Christiano][12:17]\u00a0how are you improving cheetahs in the analogy?you are trying random variants to see what works?[Yudkowsky][12:18]\u00a0using conventional, well-tested technology like MUSCLES and TENDONStrying variants on those[Christiano][12:18]\u00a0okand you think that G doesn't help you improve on muscles and tendons?until you have a big pile of it?[Yudkowsky][12:18]\u00a0not as a metaphor but as simple historical fact, that's how it played outit takes a whole big pile of G to go faster than a cheetah[Christiano][12:19]\u00a0as a matter of fact there is no one investing in making better cheetahsso it seems like we're already playing analogy-game[Yudkowsky][12:19]\u00a0the natural selection of cheetahs is investing in itit's not doing so by copying humans because of fundamental limitationshowever if we replace it with an average human investor, it still doesn't copy humans, why would it[Christiano][12:19]\u00a0that's the part that is sillyor like, it needs more analogy[Yudkowsky][12:19]\u00a0how so? \u00a0humans aren't the fastest.[Christiano][12:19]\u00a0humans are great at breeding animalsso if I'm natural selection personified, the thing to explain is why I'm not using some of that G to improve on my selectionnot why I'm not using G to build a car[Yudkowsky][12:20]\u00a0I'm... confusedis this implying that a key aspect of your model is that people are using AI to decide which AI tech to invest in?[Christiano][12:20]\u00a0noI think I just don't understand your analogyhere in the actual world, some people are trying to make faster robots by tinkering with robot designsand then someone somewhere is training their AGI[Yudkowsky][12:21]\u00a0what I'm saying is that you can imagine a little cheetah investor going, \"I'd like to copy and imitate some other species's tricks to make my cheetahs faster\" and they're looking enviously at falcons, not at humansnot until very late in the game[Christiano][12:21]\u00a0and the relevant question is whether the pre-AGI thing is helpful for automating the work that humans are doing while they tinker with robot designsthat seems like the actual worldand the interesting claim is you saying \"nope, not very\"[Yudkowsky][12:22]\u00a0I am again confused. \u00a0Does it matter to your model whether the pre-AGI thing is helpful for automating \"tinkering with robot designs\" or just profitable machine translation? \u00a0Either seems like it induces equivalent amounts of investment.If anything the latter induces much more investment.[Christiano][12:23]\u00a0sure, I'm fine using \"tinkering with robot designs\" as a lower boundboth are finethe point is I have no idea what you are talking about in the analogywhat is analogous to what?I thought cheetahs were analogous to faster robots[Yudkowsky][12:23]\u00a0faster cheetahs are analogous to more profitable robots[Christiano][12:23]\u00a0sureso you have some humans working on making more profitable robots, right?who are tinkering with the robots, in a way analogous to natural selection tinkering with cheetahs?[Yudkowsky][12:24]\u00a0I'm suggesting replacing the Natural Selection of Cheetahs with a new optimizer that has the Copy Competitor and Invest In Easily-Predictable Returns feature[Christiano][12:24]\u00a0OK, then I don't understand what those are analogous tolike, what is analogous to the humans who are tinkering with robots, and what is analogous to the humans working on AGI?[Yudkowsky][12:24]\u00a0and observing that, even this case, the owner of Cheetahs Inc. would not try to copy Humans Inc.[Christiano][12:25]\u00a0here's the analogy that makes sense to menatural selection is working on making faster cheetahs = some humans tinkering away to make more profitable robotsnatural selection is working on making smarter humans = some humans who are tinkering away to make more powerful AGInatural selection doesn't try to copy humans because they suck at being fast = robot-makers don't try to copy AGI-makers because the AGIs aren't very profitable robots[Yudkowsky][12:26]\u00a0with you so far[Christiano][12:26]\u00a0eventually humans build cars once they get smart enough = eventually AGI makes more profitable robots once it gets smart enough[Yudkowsky][12:26]\u00a0yup[Christiano][12:26]\u00a0great, seems like we're on the same page then[Yudkowsky][12:26]\u00a0and by this point it is LATE in the game[Christiano][12:27]\u00a0great, with you still[Yudkowsky][12:27]\u00a0because the smaller piles of G did not produce profitable robots[Christiano][12:27]\u00a0but there's a step here where you appear to go totally off the rails[Yudkowsky][12:27]\u00a0or operate profitable robotssay on[Christiano][12:27]\u00a0can we just write out the sequence of AGIs, AGI(1), AGI(2), AGI(3)... in analogy with the sequence of human ancestors H(1), H(2), H(3)...?[Yudkowsky][12:28]\u00a0Is the last member of the sequence H(n) the one that builds cars and then immediately destroys the world before anything that operates on Cheetah Inc's Owner's scale can react?[Christiano][12:28]\u00a0sureI don't think of it as the lastbut it's the last one that actually arises?maybe let's call it the last, H(n)greatand now it seems like you are imagining an analogous story, where AGI(n) takes over the world and maybe incidentally builds some more profitable robots along the way(building more profitable robots being easier than taking over the world, but not so much easier that AGI(n-1) could have done it unless we make our version numbers really close together, close enough that deploying AGI(n-1) is stupid)[Yudkowsky][12:31]\u00a0if this plays out in the analogous way to human intelligence, AGI(n) becomes able to build more profitable robots 1 hour before it becomes able to take over the world; my worldview does not put that as the median estimate, but I do want to observe that this is what happened historically[Christiano][12:31]\u00a0sure[Yudkowsky][12:32]\u00a0ok, then I think we're still on the same page as written so far[Christiano][12:32]\u00a0so the question that's interesting in the real world is which AGI is useful for replacing humans in the design-better-robots task; is it 1 hour before the AGI that takes over the world, or 2 years, or what?[Yudkowsky][12:33]\u00a0my worldview tends to make a big ol' distinction between \"replace humans in the design-better-robots task\" and \"run as a better robot\", if they're not importantly distinct from your standpoint can we talk about the latter?[Christiano][12:33]\u00a0they seem importantly distincttotally different evenso I think we're still on the same page[Yudkowsky][12:34]\u00a0ok then, \"replacing humans at designing better robots\" sure as heck sounds to Eliezer like the world is about to end or has already ended[Christiano][12:34]\u00a0my whole point is that in the evolutionary analogy we are talking about \"run as a better robot\" rather than \"replace humans in the design-better-robots-task\"and indeed there is no analog to \"replace humans in the design-better-robots-task\"which is where all of the action and disagreement is[Yudkowsky][12:35][12:36]\u00a0well, yes, I was exactly trying to talk about when humans start running as better cheetahsand how that point is still very late in the gamenot as late as when humans take over the job of making the thing that makes better cheetahs, aka humans start trying to make AGI, which is basically the fingersnap end of the world from the perspective of Cheetahs Inc.[Christiano][12:36]\u00a0OK, but I don't care when humans are better cheetahs---in the real world, when AGIs are better robots. In the real world I care about when AGIs start replacing humans in the design-better-robots-task. I'm game to use evolution as an analogy to help answer that question (where I do agree that it's informative), but want to be clear what's actually at issue.[Yudkowsky][12:37]\u00a0so, the thing I was trying to work up to, is that my model permits the world to end in a way where AGI doesn't get tons of investment because it has an insufficiently huge pile of G that it could run as a better robot. \u00a0people are instead investing in the equivalents of cheetahs.I don't understand why your model doesn't care when humans are better cheetahs. \u00a0AGIs running as more profitable robots is what induces the huge investments in AGI that your model requires to produce very close competition. ?[Christiano][12:38]\u00a0it's a sufficient condition, but it's not the most robust one at alllike, I happen to think that in the real world AIs actually are going to be incredibly profitable robots, and that's part of my boring view about what AGI looks likeBut the thing that's more robust is that the sub-taking-over-world AI is already really important, and receiving huge amounts of investment, as something that automates the R&D process. And it seems like the best guess given what we know now is that this process starts years before the singularity.From my perspective that's where most of the action is. And your views on that question seem related to your views on how e.g. AGI is a fundamentally different ballgame from making better robots (whereas I think the boring view is that they are closely related), but that's more like an upstream question about what you think AGI will look like, most relevant because I think it's going to lead you to make bad short-term predictions about what kinds of technologies will achieve what kinds of goals.[Yudkowsky][12:41]\u00a0but not all AIs are the same branch of the technology tree. \u00a0factory robotics are already really important and they are \"AI\" but, on my model, they're currently on the cheetah branch rather than the hominid branch of the tech tree; investments into better factory robotics are not directly investments into improving MuZero, though they may buy chips that MuZero also buys.[Christiano][12:42]\u00a0Yeah, I think you have a mistaken view of AI progress. But I still disagree with your bottom line even if I adopt (this part of) your view of AI progress.Namely, I think that the AGI line is mediocre before it is great, and the mediocre version is spectacularly valuable for accelerating R&D (mostly AGI R&D).The way I end up sympathizing with your view is if I adopt both this view about the tech tree, + another equally-silly-seeming view about how close the AGI line is to fooming (or how inefficient the area will remain as we get close to fooming)\u00a017.4. Human generality and body manipulation\u00a0[Yudkowsky][12:43]\u00a0so metaphorically, you require that humans be doing Great at Various Things and being Super Profitable way before they develop agriculture; the rise of human intelligence cannot be a case in point of your model because the humans were too uncompetitive at most animal activities for unrealistically long (edit: compared to the AI case)[Christiano][12:44]\u00a0I don't understandHuman brains are really great at basically everything as far as I can tell?like it's not like other animals are better at manipulating their bodieswe crush them[Yudkowsky][12:44]\u00a0if we've got weapons, yes[Christiano][12:44]\u00a0human bodies are also pretty great, but they are not the greatest on every dimension[Yudkowsky][12:44]\u00a0wrestling a chimpanzee without weapons is famously ill-advised[Christiano][12:44]\u00a0no, I mean everywherechimpanzees are practically the same as humans in the animal kingdomthey have almost as excellent a brain[Yudkowsky][12:45]\u00a0as is attacking an elephant with your bare hands[Christiano][12:45]\u00a0that's not because of elephant brains[Yudkowsky][12:45]\u00a0well, yes, exactlyyou need a big pile of G before it's profitableso big the game is practically over by then[Christiano][12:45]\u00a0this seems so confusedbut that's exciting I guesslike, I'm saying that the brains to automate R&Dare similar to the brains to be a good factory robotanalogously, I think the brains that humans use to do R&Dare similar to the brains we use to manipulate our body absurdly wellI do not think that our brains make us fastthey help a tiny bit but not muchI do not think the physical actuators of the industrial robots will be that similar to the actuators of the robots that do R&Dthe claim is that the problem of building the brain is pretty similarjust as the problem of building a brain that can do science is pretty similar to the problem of building a brain that can operate a body really well(and indeed I'm claiming that human bodies kick ass relative to other animal bodies---there may be particular tasks other animal brains are pre-built to be great at, but (i) humans would be great at those too if we were under mild evolutionary pressure with our otherwise excellent brains, (ii) there are lots of more general tests of how good you are at operating a body and we will crush it at those tests)(and that's not something I know much about, so I could update as I learned more about how actually we just aren't that good at motor control or motion planning)[Yudkowsky][12:49]\u00a0so on your model, we can introduce humans to a continent, forbid them any tool use, and they'll still wipe out all the large animals?[Christiano][12:49]\u00a0(but damn we seem good to me)I don't understand why that would even plausibly follow[Yudkowsky][12:49]\u00a0because brains are profitable early, even if they can't build weapons?[Christiano][12:49]\u00a0I'm saying that if you put our brains in a big animal bodywe would wipe out the big animalsyes, I think brains are great[Yudkowsky][12:50]\u00a0because we'd still have our late-game pile of G and we would build weapons[Christiano][12:50]\u00a0no, I think a human in a big animal body, with brain adapted to operate that body instead of our own, would beat a big animal straightforwardlywithout using tools[Yudkowsky][12:51]\u00a0this is a strange viewpoint and I do wonder whether it is a crux of your view[Christiano][12:51]\u00a0this feels to me like it's more on the \"eliezer vs paul disagreement about the nature of AI\" rather than \"eliezer vs paul on civilizational inadequacy and continuity\", but enough changes on \"nature of AI\" would switch my view on the other question[Yudkowsky][12:51]\u00a0like, ceteris paribus maybe a human in an elephant's body beats an elephant after a burn-in practice period? \u00a0because we'd have a strict intelligence advantage?[Christiano][12:52]\u00a0practice may or may not be enoughbut if you port over the excellent human brain to the elephant body, then run evolution for a brief burn-in period to get all the kinks sorted out?elephants are pretty close to humans so it's less brutal than for some other animals (and also are elephants the best example w.r.t. the possibility of direct conflict?) but I totally expect us to win[Yudkowsky][12:53]\u00a0I unfortunately need to go do other things in advance of an upcoming call, but I feel like disagreeing about the past is proving noticeably more interesting, confusing, and perhaps productive, than disagreeing about the future[Christiano][12:53]\u00a0actually probably I just think practice is enoughI think humans have way more dexterity, better locomotion, better navigation, better motion planning...some of that is having bodies optimized for those things (esp. dexterity), but I also think most animals just don't have the brains for it, with elephants being one of the closest callsI'm a little bit scared of talking to zoologists or whoever the relevant experts are on this question, because I've talked to bird people a little bit and they often have very strong \"humans aren't special, animals are super cool\" instincts even in cases where that take is totally and obviously insane. But if we found someone reasonable in that area I'd be interested to get their take on this.I think this is pretty important for the particular claim \"Is AGI like other kinds of ML?\"; that definitely doesn't persuade me to be into fast takeoff on its own though it would be a clear way the world is more Eliezer-like than Paul-likeI think I do further predict that people who know things about animal intelligence, and don't seem to have identifiably crazy views about any adjacent questions that indicate a weird pro-animal bias, will say that human brains are a lot better than other animal brains for dexterity/locomotion/similar physical tasks (and that the comparison isn't that close for e.g. comparing humans vs big cats).Incidentally, seems like DM folks did the same thing this year, presumably publishing now because they got scooped. Looks like they probably have a better algorithm but used harder environments instead of Atari. (They also evaluate the algorithm SPR+MuZero I mentioned which indeed gets one factor of 2x improvement over MuZero alone, roughly as you'd guess): https://arxiv.org/pdf/2111.01587.pdf[Barnes][13:45]\u00a0My DM friend says they tried it before they were focused on data efficiency and it didn't help in that regime, sounds like they ignored it for a while after that[Christiano: \ud83d\udc4d][Christiano][13:48]\u00a0Overall the situation feels really boring to me. Not sure if DM having a highly similar unpublished result is more likely on my view than Eliezer's (and initially ignoring the method because they weren't focused on sample-efficiency), but at any rate I think it's not anywhere close to falsifying my view.\u00a018. Follow-ups to the Christiano/Yudkowsky conversation\u00a0[Karnofsky][9:39] \u00a0(Nov. 5)\u00a0Going to share a point of confusion about this latest exchange.It started with Eliezer saying this:Thing that (if true) strikes me as... straight-up falsifying Paul's view as applied to modern-day AI, at the frontier of the most AGI-ish part of it and where Deepmind put in substantial effort on their project? EfficientZero (allegedly) learns Atari in 100,000 frames. Caveat: I'm not having an easy time figuring out how many frames MuZero would've required to achieve the same performance level. MuZero was trained on 200,000,000 frames but reached what looks like an allegedly higher high; the EfficientZero paper compares their performance to MuZero on 100,000 frames, and claims theirs is much better than MuZero given only that many frames.So at this point, I thought Eliezer's view was something like: \"EfficientZero represents a several-OM (or at least one-OM?) jump in efficiency, which should shock the hell out of Paul.\" The upper bound on the improvement is 2000x, so I figured he thought the corrected improvement would be some number of OMs.But very shortly afterwards, Eliezer quotes Gwern's guess of a 4x improvement, and Paul then said:Concretely, this is a paper that adds a few techniques to improve over MuZero in a domain that (it appears) wasn't a significant focus of MuZero. I don't know how much it improves but I can believe gwern's estimates of 4x.I'd guess MuZero itself is a 2x improvement over the baseline from a year ago, which was maybe a 4x improvement over the algorithm from a year before that. If that's right, then no it's not mindblowing on my view to have 4x progress one year, 2x progress the next, and 4x progress the next.Eliezer never seemed to push back on this 4x-2x-4x claim.What I thought would happen after the 4x estimate and 4x-2x-4x claim: Eliezer would've said \"Hmm, we should nail down whether we are talking about 4x-2x-4x or something more like 4x-2x-100x. If it's 4x-2x-4x, then I'll say 'never mind' re: my comment that this 'straight-up falsifies Paul's view.' At best this is just an iota of evidence or something.\"Why isn't that what happened? Did Eliezer mean all along to be saying that a 4x jump on Atari sample efficiency would \"straight-up falsify Paul's view?\" Is a 4x jump the kind of thing Eliezer thinks is going to power a jumpy AI timeline?[Ngo: \ud83d\udc4d][Shah: \u2795][Yudkowsky][11:16] \u00a0(Nov. 5)\u00a0This is a proper confusion and probably my fault; I also initially thought it was supposed to be 1-2 OOM and should've made it clearer that Gwern's 4x estimate was less of a direct falsification.I'm not yet confident Gwern's estimate is correct. \u00a0I just got a reply from my query to the paper's first author which reads:Dear Eliezer: It's a good question. But due to the limits of resources and time, we haven't evaluated the sample efficiency towards different frames systematically. I think it's not a trivial question as the required time and resources are much expensive for the 200M frames setting, especially concerning the MCTS-based methods. Maybe you need about several days or longer to finish a run with GPUs in that setting. I hope my answer can help you. Thank you for your email.I replied asking if Gwern's 3.8x estimate sounds right to them.A 10x improvement could power what I think is a jumpy AI timeline. \u00a0I'm currently trying to draft a depiction of what I think an unrealistically dignified but computationally typical end-of-world would look like if it started in 2025, and my first draft of that had it starting with a new technique published by Google Brain that was around a 10x improvement in training speeds for very large networks at the cost of higher inference costs, but which turned out to be specially applicable to online learning.That said, I think the 10x part isn't either a key concept or particularly likely, and it's much more likely that hell breaks loose when an innovation changes some particular step of the problem from \"can't realistically be done at all\" to \"can be done with a lot of computing power\", which was what I had being the real effect of that hypothetical Google Brain innovation when applied to online learning, and I will probably rewrite to reflect that.[Karnofsky][11:29] \u00a0(Nov. 5)\u00a0That's helpful, thanks.Re: \"can't realistically be done at all\" to \"can be done with a lot of computing power\", cpl things:1. Do you think a 10x improvement in efficiency at some particular task could qualify as this? Could a smaller improvement?2. I thought you were pretty into the possibility of a jump from \"can't realistically be done at all\" to \"can be done with a small amount of computing power,\" eg some random ppl with a $1-10mm/y budget blowing past mtpl labs with >$1bb/y budgets. Is that wrong?[Yudkowsky][13:44] \u00a0(Nov. 5)\u00a01 - yes and yes, my revised story for how the world ends looks like Google Brain publishing something that looks like only a 20% improvement but which is done in a way that lets it be adapted to make online learning by gradient descent \"work at all\" in DeepBrain's ongoing Living Zero project (not an actual name afaik)2 - that definitely remains very much allowed in principle, but I think it's not my current mainline probability for how the world's end plays out - although I feel hesitant / caught between conflicting heuristics here.I think I ended up much too conservative about timelines and early generalization speed because of arguing with Robin Hanson, and don't want to make a similar mistake here, but on the other hand a lot of the current interesting results have been from people spending huge compute (as wasn't the case to nearly the same degree in 2008) and if things happen on short timelines it seems reasonable to guess that the future will look that much like the present. \u00a0This is very much due to cognitive limitations of the researchers rather than a basic fact about computer science, but cognitive limitations are also facts and often stable ones.[Karnofsky][14:35] \u00a0(Nov. 5)\u00a0Hm OK. I don't know what \"online learning by gradient descent\" means such that it doesn't work at all now (does \"work at all\" mean something like \"work with human-ish learning efficiency?\")[Yudkowsky][15:07] \u00a0(Nov. 5)\u00a0I mean, in context, it means \"works for Living Zero at the performance levels where it's running around accumulating knowledge\", which by hypothesis it wasn't until that point.[Karnofsky][15:12] \u00a0(Nov. 5)\u00a0Hm. I am feeling pretty fuzzy on whether your story is centrally about:1. A <10x jump in efficiency at something important, leading pretty directly/straightforwardly to crazytown2. A 100x ish jump in efficiency at something important, which may at first \"look like\" a mere <10x jump in efficiency at something else#2 is generally how I've interpreted you and how the above sounds, but under #2 I feel like we should just have consensus that the Atari thing being 4x wouldn't be much of an update. Maybe we already do (it was a bit unclear to me from your msg)(And I totally agree that we haven't established the Atari thing is only 4x - what I'm saying is it feels like the conversation should've paused there)[Yudkowsky][15:13] \u00a0(Nov. 5)\u00a0The Atari thing being 4x over 2 years is I think legit not an update because that's standard software improvement speedyou're correct that it should pause there[Karnofsky][15:14] \u00a0(Nov. 5)\u00a0\ud83d\udc4d[Yudkowsky] [15:24] \u00a0(Nov. 5)\u00a0I think that my central model is something like - there's a central thing to general intelligence that starts working when you get enough pieces together and they coalesce, which is why humans went down this evolutionary gradient by a lot before other species got 10% of the way there in terms of output; and then it takes a big pile of that thing to do big things, which is why humans didn't go faster than cheetahs until extremely late in the game.so my visualization of how the world starts to end is \"gear gets added and things start to happen, maybe slowly-by-my-standards at first such that humans keep on pushing it along rather than it being self-moving, but at some point starting to cumulate pretty quickly in the same way that humans cumulated pretty quickly once they got going\" rather than \"dial gets turned up 50%, things happen 50% faster, every year\".[Yudkowsky][15:16] \u00a0(Nov. 5, switching channels)\u00a0as a quick clarification, I agree that if this is 4x sample efficiency over 2 years then that doesn't at all challenge Paul's view[Christiano][0:20] \u00a0(Nov. 26)\u00a0FWIW, I felt like the entire discussion of EfficientZero was a concrete example of my view making a number of more concentrated predictions than Eliezer that were then almost immediately validated. In particular, consider the following 3 events:The quantitative effect size seems like it will turn out to be much smaller than Eliezer initially believed, much closer to being in line with previous progress.DeepMind had relatively similar results that got published immediately after our discussion, making it look like random people didn't pull ahead of DM after all.DeepMind appears not to have cared much about the metric in question, as evidenced by (i) Beth's comment above, which is basically what I said was probably going on, (ii) they barely even mention Atari sample-efficiency in their paper about similar methods.If only 1 of these 3 things had happened, then I agree this would have been a challenge to my view that would make me update in Eliezer's direction. But that's only possible if Eliezer actually assigns a higher probability than me to <= 1 of these things happening, and hence a lower probability to >= 2 of them happening. So if we're playing a reasonable epistemic game, it seems like I need to collect some epistemic credit every time something looks boring to me.[Yudkowsky][15:30] \u00a0(Nov. 26)\u00a0I broadly agree; you win a Bayes point. \u00a0I think some of this (but not all!) was due to my tripping over my own feet and sort of rushing back with what looked like a Relevant Thing without contemplating the winner's curse of exciting news, the way that paper authors tend to frame things in more exciting rather than less exciting ways, etc. \u00a0But even if you set that aside, my underlying AI model said that was a thing which could happen (which is why I didn't have technically rather than sociologically triggered skepticism) and your model said it shouldn't happen, and it currently looks like it mostly didn't happen, so you win a Bayes point.Notes that some participants may deem obvious(?) but that I state expecting wider readership:Just like markets are almost entirely efficient (in the sense that, even when they're not efficient, you can only make a very small fraction of the money that could be made from the entire market if you owned a time machine), even sharp and jerky progress has to look almost entirely not so fast almost all the time if the Sun isn't right in the middle of going supernova. \u00a0So the notion that progress sometimes goes jerky and fast does have to be evaluated by a portfolio view over time. \u00a0In worlds where progress is jerky even before the End Days, Paul wins soft steady Bayes points in most weeks and then I win back more Bayes points once every year or two.We still don't have a very good idea of how much longer you would need to train the previous algorithm to match the performance of the new algorithm, just an estimate by Gwern based off linearly extrapolating a graph in a paper. \u00a0But, also to be clear, not knowing something is not the same as expecting it to update dramatically, and you have to integrate over the distribution you've got.It's fair to say, \"Hey, Eliezer, if you tripped over your own feet here, but only noticed that because Paul was around to call it, maybe you're tripping over your feet at other times when Paul isn't around to check your thoughts in detail\" - I don't want to minimize the Bayes point that Paul won either.[Christiano][16:29] \u00a0(Nov. 27)\u00a0Agreed that it's (i) not obvious how large the EfficientZero gain was, and in general it's not a settled question what happened, (ii) it's not that big an update, it needs to be part of a portfolio (but this is indicative of the kind of thing I'd want to put in the portfolio), (iii) it generally seems pro-social to flag potentially relevant stuff without the presumption that you are staking a lot on it.", "title": "Christiano and Yudkowsky on AI predictions and human intelligence", "karma": 36, "af": true, "userid": "nmk3nLpQE89dMRzzN", "id": "NbGmfxbaABPsspib7"}, {"text": "\"If this is a trap then They are coming,\" said Luna. She used the word \"They\" to avoid activating possible Taboos.\n\"It's a trap,\" said Fay.\n\"How do you know?\" said Luna.\nFay rolled her eyes.\n\"We're doomed,\" said Luna.\n\"This is the Department of Mysteries. Maybe we'll find some powerful magical artifacts,\" said Fay.\nThey picked a door at random.\nThe Time Room was filled with glittering Spimster wickets. There were big Spimster wickets, little Spimster wickets, grandfather Spimster wickets and digital Spimster wickets. Fay shut the door behind them.\n\"I don't suppose you suffer from Spontaneous Duplication?\" said Luna.\n\"Nope,\" said Fay.\nSpimster wickets were not interesting. Luna and Fay retreated to the Entrance Chamber. It spun. They picked another door.\nThe Brain room contained a big open-topped tank full of green liquid. Tentacled brains swam around inside it.\n\"Nope,\" said Fay.\n\"Wait,\" said Luna.\nLuna slowly approached the tank.\n\"Snap out of it. Those things are hypnotizing you,\" said Fay. She tugged on Luna's sleeve.\nLuna placed her hand against the glass edge of the tank. One of the brains placed its tentacle opposite Luna's hand.\n\"Mom?\"\nFay yanked Luna away from the tank and back toward the Entrance Chamber. It was too late. An Unspeakable stood in the doorway wearing dueling armor.\n", "title": "Luna Lovegood and the Fidelius Curse - Part 8", "karma": 10, "af": false, "userid": "n6LYNw2uGfYnD4pX2", "id": "GHYbvPdWarjTHWQqZ"}, {"text": "First of all, I am not a utilitarian. I'm doing this because I think it is valuable to see how different moral intuitions connect to each other, and it's also fun.\u00a0My argument is based on the following claims:Is claim: A person\u2019s pleasure/pain can be quantified and compared to another person\u2019s pleasure/pain.\u00a0Ought claim 1: A person ought to follow the laws of rational choice theory.Ought claim 2: A person's decisions ought to only be informed by how they affect the pain/pleasure of other people and themselves.Ought claim 3: If a person's decision only affects themself, they ought to increase their own pleasure.Ought claim 4: Morality treats everyone the same. More precisely, a person ought to make the same decision if they knew they would switch places with another person afterward.\u00a0Ought claim 5: The fairness of an outcome ought to be irrelevant (this is probably the most interesting and contentious assumption).In its present form, this argument only applies to universes where there are two people living in them. It could probably be generalized to n people, but I haven't had time to look into this.Here is the setup:There are two people in a room. Person 1 can choose to take an apple or a banana. Whatever person 1 decides not to take is given to person 2. I will prove that person A should make their decision based on total hedonistic utilitarianism, which I make more clear in a moment.Obviously, there is nothing special about this apple-banana conundrum. You can apply the reasoning I am going to give to any decision which affects two people.Is claim: A person\u2019s pleasure/pain can be quantified and compared to another person\u2019s pleasure/pain. Here's what person 1's pleasure axis might look like for different experiences:There are 4 pleasure/pain numbers involved in this scenario:\u00a0The pleasure of person 1 when they get an apple (A1)The pleasure of person 2 when they get a banana (B2)The pleasure of person 1 when they get a banana (B1)The pleasure of person 2 when they get an apple (A2)I will show that if:A1+B2>B1+A2Then person 1 ought to take the apple. Otherwise, they ought to take the banana.Ought claim 2: A person's decisions ought to only be informed by how they affect the pain/pleasure of other people and themselves.This implies that person 1's decision should only depend on A1, B2, B1, and A2. Let's define a function:d:(A1,B2,B1,A2)\u2192'choose apple' or 'choose banana'Ought claim 1: A person ought to follow the laws of rational choice theory.By the\u00a0Von Neumann\u2013Morgenstern utility theorem, this implies that there exists a function:u:S\u2192Rthat assigns a number to a scenario that represents person 1\u2019s preference for that scenario. Person 1 will choose scenario s1 over scenario s2 when u(s1) > u(s2).There are two scenarios that person 1 must decide between:\u00a0By ought claim 2, the only thing that is relevant here is pleasure and pain. So, u is a function of x and y where x is the pleasure of person 1 and y is the pleasure of person 2.Writing this in math:d(A1,B2,A2,B1)='choose apple'\u27fau(A1,B2)>u(B1,A2)d(A1,B2,A2,B1)='choose banana'\u27fau(A1,B2)<u(B1,A2)\u00a0Ought claim 4: Morality treats everyone the same. More precisely, a person ought to make the same decision if they know they would switch places with another person afterward.\u00a0Person 1 shouldn't care about the distinction between themself and person 2, which means that:u(x,y)=u(y,x)\u00a0for all\u00a0x,yTherefore, there exists a function g such that:u(x,y)=g(|x\u2212y|,x+y)This can be shown by using a rotated and scaled coordinate system, parameterized by:u=x\u2212y\u00a0and\u00a0v=x+yu(x,y)=g(u,v)=g(x\u2212y,x+y)=u(y,x)\u27f9g(x\u2212y,x+y)=g(|x\u2212y|,x+y)\u00a0Let\u2019s take a step back and think about what this result means.\u00a0|x - y| is the amount by which person 1 and person 2\u2019s pleasure differ. This is similar to the concept of\u00a0fairness\u00a0or\u00a0justice.\u00a0 x + y is the sum of their pleasures. This means that person 1\u2019s decision ought to only depend on the fairness of the result and the total pleasure produced.This is a surprisingly good model of common moral intuitions.Let's finish the proof.Ought claim 5: The fairness of an outcome ought to be irrelevant.This is by far the most contentious claim, but I can't see a way around it. Let me know in the comments if you can think of a more agreeable axiom that would complete the proof.More formally, this implies that there exists a function h such that:u(x,y)=g(|x\u2212y|,x+y)=h(x+y)We are almost done.Ought claim 3: If a decision affects no one but yourself, you ought to increase your own pleasure.This means that if y is held fixed, then u(x,y) ought to be higher if x is higher:x1>x2\u27f9u(x1,y)>u(x2,y)\u00a0for all\u00a0yTherefore, h is monotonic increasing:x1>x2\u27fax1+y>x2+y\u27f9h(x1+y)>h(x2+y)\u00a0for all\u00a0yBringing back the original definition of u(x,y) and using the fact that h is monotonic increasing:d(A1,B2,A2,B1)='choose apple'\u27fau(A1,B2)>u(B1,A2)\u27fah(A1+B2)>h(B1+A2)\u27faA1+B2>B1+A2So person 1 ought to choose the apple if A1 + B2 > B1 + A2. That's total utilitarianism. QED.\u00a0", "title": "A mathematical derivation of total hedonistic Utilitarianism from simple normative axioms", "karma": 1, "af": false, "userid": "TntekaAq5NPZq9qA4", "id": "pfPhq2T3jK8KMFFGm"}, {"text": "I've been wondering about this question for a while, but some new research released today reminded me of it:\n\nThe seasonal timing of the catastrophic end-Cretaceous bolide impact places the event at a particularly sensitive stage for biological life cycles in the Northern Hemisphere. In many taxa, annual reproduction and growth take place during spring. Species with longer incubation times, such as non-avian reptiles, including pterosaurs and most dinosaurs, were arguably more vulnerable to sudden environmental perturbations than other groups43 (for example, birds).\n\n\nSimilarly, large-scale wildfires raging across the Southern Hemisphere9,34,41 may have been evaded by hibernating mammals that were already sheltered in burrows34,41 in anticipation of austral winter. Additional modes of seasonal dormancy, torpor and/or aestivation, which are nowadays practised by various mammals46,47 as well as certain amphibians, birds and crocodilians48, could have facilitated further underground survival.\n\nThe article speculates that the particular time of year at which the asteroid struck is responsible for the pattern of extinctions that resulted. I have also heard it suggested (don't remember where) that the particular location of the impact - in shallow water just off the coast - had a different effect than if it had hit either on land or in the deep sea (i.e. the large majority of Earth's surface). And it seems somewhat odd that an impact of that size should've occurred in the so recent past:\n\nImpacts of asteroids spanning more than 10 kilometers are rare and happen about every 250 to 500 million years. Having one such event only 66 million years ago is therefore mildly special.\n\nIs the peculiarity of the K-T event yet another example of \"fine-tuning\" explained by the anthropic principle?\nConsider (reposting from before) the fossil evidence of the progression in intelligence over time:\n\n\nThe first chart shows the encephalization quotient (EQ) of various lineages over time, while the second shows the maximum EQ of all known fossils from any given time.\nThis suggests that the Mesozoic was a long period of stagnation in the development of intelligence on Earth, and that if the K-T event had not occurred, it would've been unlikely for human-level intelligence ever to have evolved, whether from the mammalian or the reptilian lineages.\nWhen we think about the likelihood of intelligence evolving, it's tempting to look at a particular species or lineage in isolation; but really this is missing the forest for the trees, because evolution is a response to a changing environment, a large portion of which consists of the other species in the ecosystem. Our mouse-like ancestors that survived the K-T event might never have evolved any greater intelligence than that, unless a finely-tuned mass extinction had completely cleared the terrestrial megafauna niche while leaving the burrowers intact. Is that why you're here reading this today?\nThe answer to this question is not purely academic; it has direct implications for estimating whether we have already passed the Great Filter.\nI know next to nothing about paleontology or evolution, just what I've learned from skimming a few articles. I'd love to hear from others more knowledgeable in the area.\n", "title": "Was the K-T event a Great Filter?", "karma": 6, "af": false, "userid": "ySBz7ww8SydkCKAPE", "id": "6uEkniGLC9BfFggLs"}, {"text": "When a person first begins to study naturalism with me, I say to them almost nothing that I\u2019ve written in this sequence.\u00a0That might very well be a mistake; I might get better results if I knew some short combination of words that would cause them to correctly understand, intellectually, what we are doing and why, before we started.But in fact I bank on the person\u2019s trust in me a bit, and begin by helping them establish consistent habits of observation.\u00a0If they\u2019re interested in studying confusion, I ask them to tap their leg every time they notice they\u2019re confused. I ask them to keep a log book in which they record a few words about their experiences of confusion each day. I ask them to make predictions about what it will be like to notice confusion\u2014what kind of situation will be happening and how they will know in the moment that they are confused\u2014and to compare their observations to their predictions.And then, throughout what has so far proven to be about a three month program, I never shift our focus away from consistent habits of observation. It\u2019s not just where I start. It\u2019s the entire curriculum.From a practical perspective, this dogged persistence is the foundation of naturalism. \u201cDirect observation of the territory\u201d, without patience, gets you something like a bag of tricks. Valuable tricks, but still tricks. Isolated mental motions made when they are convenient and enjoyable, not when they are most needed.With patience, though, you get a life-long practice of epistemic rationality.So the whole naturalism program, from start to finish, consists of the establishment, improvement, and maintenance of consistent habits of observation. Consistent and ceaseless, without any sense that \u201cand then we\u2019ll be done observing and get back to normal\u201d. When my students and I meet, we are constantly talking about what daily practice looked like over the past week, what was too heavy to implement as a regular routine, and how they personally can slow down enough to thoroughly observe. We continue in this way until they no longer seem to need me to ask those questions for them. The program is meant to introduce a new normal.The rest of what I talk about in this sequence is sprinkled in, sure. It's implicit in the questions I ask, the approaches I encourage, and \"my whole vibe\". Like a catalyst, or like spices, those other parts are crucial to the recipe. But patience is the engine that makes all of it go. None of what I\u2019ve written here gets off the ground unless it is practiced as an ongoing discipline.I suspect that the thing I\u2019m calling \u201cpatience\u201d really is a single core capacity, or a single virtue; but it can express itself in multiple patterns of behavior. I\u2019d like to talk about three patient behavior patterns: tenacity, openness, and thoroughness.In my mind, the paradigmatic example of tenacity is marathon training. Not marathon racing, but the training program a runner goes through to prepare for their first marathon.After an initial adjustment period, training for endurance athletics is mainly difficult because it requires commitment to the maintenance of a routine, not because it requires intense exertion. Most of the time you\u2019re not running anywhere near as fast as you can, and at least until race day, you\u2019re not running as far as you can either. On any given day, you\u2019re running a comfortable-for-you amount\u2014but you\u2019re doing it day after day after day, without fail, for months at a time. The distance running motto is \u201csmall, consistent efforts\u201d.The facet of patience I\u2019m calling tenacity is the ability to exert small, consistent efforts.The reason tenacity is foundational to naturalism is that it\u2019s required for any kind of maintenance. Knowledge of the territory requires not just contact with the territory, but maintenance of that contact. Bumping up against the world and bouncing right back off again is not enough; you have to reliably return after you bounce.We are bound to see things as we are, rather than as they are; but we are not bound to always see them as we were when we first encountered them. It is possible to observe again, and again, and again; if you do that with naked directness, and with the relentlessness of marathon training, your perceptual systems will inevitably adjust to perceive reality more accurately over time.\u00a0The next facet of patience I\u2019d like to talk about is \u201copenness\u201d, in the sense of \u201cnon-closure\u201d.When someone comes to me for advice on a long-standing adaptive challenge, the most common recommendation I give is, \u201cStop trying to solve this problem for a while. Start investigating the underlying territory instead.\u201dI recently chatted with someone who was worried that she might be a narcissist, and wanted to know what to do about it. She gave me permission to share these (anonymized) excerpts from our conversation.Logan: my first thought is that this sounds like a situation where you'd do well to put \"what should i do about it?\" on hold for a good three months, and focus instead on \"what is actually happening? how can i tell? what is it like? what is my brain doing by default in various situations, and which situations are the ones i care about here? which phenomena and mental motions seem important for understanding what's happening here?\"Crystal: That seems smart. But, seems better to know I am a narcissist than to be uncertain about it for three months... more comfortable I meanLogan: i imagine that you have a question like \"am i a narcissist?\" in your head, and it's really salient because things you care about depend on the answer to it, and it's uncomfortable to not know the answer because you'd ideally orient to the two different worlds differently, and when you don't know which you're in you don\u2019t' know how to orient. is that right?Crystal: YeahHere, Crystal is demonstrating a need for closure. She is uncomfortable (understandably!) with being uncertain. She would like to make plans for the future. Those plans may be substantially different in worlds where she believes the answer to the question \"am I a narcissist?\" is \"yes\" than in the world where it's \"no.\" She wants to know what to work on, in herself and in her relationships. She wants to know what to expect.So when I recommend to her that she deliberately hang out in uncertainty while she gradually increases her contact with the territory, it feels bad to her.Back to the conversation, jumping ahead a bit:Logan: i tend to operate under the conjecture that when there is a thing that's been a problem for most of a person's life, that person's way of conceptualizing the problem is very likely to be incorrect or incomplete in ways that make investigation that's not driven by the concept more productive than investigation that is driven by the concept.Crystal: The concept being \u201cnarcissism\u201d?Logan: yeah.Crystal: How is it driven by the concept or not? What does that mean?Logan: investigation that's driven by the concept looks like: how would i know if i'm a narcissist? what things are evidence for or against? where would i look for evidence of narcissism? what would disconfirming evidence look like? what are alternative hypotheses? how might i test them?Crystal: > investigation that's driven by the concept looks like: how would i know \u2026That sounds like meLogan: investigation that's not driven by the concept might look like: what does it feel like to be worried about whether i'm a narcissist? what seems to be at stake? if i go through the week and write down times when something related to the-thing-i-care-about-here happened, what do i end up writing? what was happening around me and in my head during those times? which of the things happening in my experience seems most closely tied to the-thing-i-care-about-here? how can i tell when that thing is happening in my head? if i watch for times when that thing is happening in my head and write down instances, what do i write?in other words, it's possible to gain a lot of information about what's actually going on without having pre-decided most of what's going on. my suspicion is that this is a time when it makes sense to not pre-decide most of what's going on before you try to really seriously get in contact with the relevant region of territoryI often call the latter type of investigation\u2014the kind that\u2019s not driven by the concept\u2014\u201dexploratory investigation\u201d. I\u2019ve never used a word for the former type, but here I\u2019m inclined to dub it \u201ccertainty seeking\u201d.Certainty seeking is often the right approach. It\u2019s the right approach when you have good reason to think you mostly understand the situation and just need to fill in some details, or to determine the truth values of a couple central propositions. In that case, a more exploratory investigation style would be needlessly inefficient.But people very often fall into certainty seeking when they are impatient. They already have a sketch, and for one reason or another, they just want to fill in the details and be done. They\u2019re willing to shift a line here or there, but mainly they\u2019re motivated to complete the drawing. \u201cAll I want to know is, is this narcissism or isn\u2019t it? Yes or no?!\u201dA person in the grips of this impatient mode is not so much trying to learn the shape of reality, as to crystalize a satisfying concept so they can relax into certainty.There are advantages and disadvantages to both approaches, of course. My point is that nearly all truth-seeking benefits from a combined approach to investigation. You need to be able to move back and forth. Impatience tends to crowd out direct exploration, and ensures that you\u2019ll mainly find whatever you have already decided to look for.Openness, in the sense I mean, is the ability to observe without desperation for an answer.But, what is it to observe without desperation? I\u2019ve told you what this facet of patience lacks, but what does it consist of?Today I saw a raven do a barrel roll[1].\u00a0I\u2019d heard that ravens could turn over in the air, and even do backflips occasionally, and I\u2019d seen pictures of ravens upside down. But when I saw this one do a barrel roll right in front of me\u2014well, above me, I suppose\u2014I felt\u2026 \u201csurprise\u201d is too simple. I felt glued to the ground, knocked sideways, and opened up all at once. I felt awe. I shouted up to the raven, \u201cYou just did a barrel roll! What?! That was awesome! You are awesome!\u201dBefore I saw the raven, I was out on a walk through the country, down a dirt road with a few houses and lots of trees. Earlier on my walk, I took a picture of some kind of insect nest, or perhaps a fungus, on the underside of a leaf. I peered through a hedge to see if I could work out what kind of crop my secretive neighbors were growing. I smelled some little pink flowers on a tree and found that their scent was a lot like caramel and roses mixed with grass. I pet a dog and asked her if she could smell my cat (which she probably could, but she wasn\u2019t feeling chatty). I learned that the acorn hats have dried out enough to go \u201ccrunch\u201d underfoot.\u00a0My state of mind was one of open, gentle exploration. And it\u2019s from that state of mind that the raven was able to move me in the way it did.I can imagine an alternate walk in which I was trying to determine whether or not my local ravens can do aerial acrobatics. I think there would have been some frustration with the many ravens I saw along the way who were not even flying, let alone flying upside down. (I wouldn\u2019t have observed any leaf bottoms at all.) And I think that seeing the barrel roll would still have been very cool, but it also would have felt a lot more like relief and completion; like the end of something, rather than the beginning. Like closing, more than like opening.But more importantly, I never would have set out on such a walk in the first place. It simply would never have occurred to me. I saw a raven do a barrel roll because I was there when it happened. I was in the right place, and I was open.Openness feels like being there for whatever happens. Being down.\u00a0It's almost-but-not-quite the opposite of purposefulness. It's the canvas on which purposes get painted. A central strategy of naturalism is to put most of your purposefulness points into choosing where to bring your canvas. If that canvas is already full, then there's nowhere for new and surprising things to land.\u00a0Openness feels like putting myself in the middle of something alive, looking around, and letting whatever I observe move me however it does.The final facet of patience I\u2019d like to discuss is thoroughness.When I think of thoroughness, I imagine holding a puzzle box as I turn it around and around, trying to visually examine it from all angles. No matter how accurate and precise my observations of the box from one particular angle, it is only possible to see at most three faces of a cube from any single vantage point. To know the whole surface of a cube, I either have to move the cube, or I have to move myself.If the puzzle box is sitting on your desk, and you glance over at it several times as you go about your day, you\u2019ll most likely catch it from a few different angles by accident. So tenacity and openness together naturally result in some amount of thoroughness.This is the principle behind what Anna Salamon has called \u201cthe 50/50 rule\u201d. According to (my own interpretation of) the 50/50 rule, 50% of the intellectual progress you make on something will happen while you are deliberately trying to make progress on that thing in particular. The other 50% will happen while you are engaged with other things: riding the bus, playing with your kids, designing a board game, identifying a bird.\u00a0It\u2019s important to spend a lot of your time doing things other than focusing on your Main Project. This is not just because your brain needs to \u201crest\u201d; it is also important to do other things because you will see different faces of the puzzle box while you are dancing at a salsa club than while you are staring at a white board.It is possible to find additional vantage points on purpose, and I call this capacity \u201cperceptual dexterity\u201d.When I look at the pen that is on my desk right now, I see a pen. That is, when I direct my gaze and attention toward the part of my visual field where light is reflecting off the surface of the pen, my concept of \u201cpen\u201d is active.When I see the pen as a pen, certain parts of my experience stand out to me, while others are discarded. My attention lands on the button at the top, which I could push to extrude the nib. It lands on the thin cylindrical shape, and I can feel myself preparing to orient my hand to that shape in a way that would allow me to hold the pen for writing.\u00a0But the reflection of the clip in the shiny metal surface of the cylinder doesn\u2019t occur to me, when I see the pen as a pen. To notice that reflection, I have to see the pen a little differently than I would by default. I have to rotate to a slightly different vantage point\u2014to rotate my mind into a slightly different configuration, one that processes information a bit differently.I don\u2019t have to primarily activate my \u201cpen\u201d concept just because I happen to be looking at a pen. I can choose to rotate my mind however I want, and then look at the thing in front of me.\u00a0If I rotate my mind toward \u201cgoose\u201d, what first stands out to me is the hole in the front, which seems to break an otherwise aerodynamic nose. Maybe the air would get stuck in there, if this pen had wings and tried to fly.If I rotate toward \u201caggression\u201d, the first thing that stands out is the place where the clip is attached to the body of the pen, as I evaluate its thickness and wonder how much force it would take to snap the clip off and leave a sharp edge.When I\u2019m chewing on a problem and feeling a little stuck, one of the first things I do is ask myself, \u201cIf this were a boat, what sort of boat would it be?\u201d There\u2019s nothing special about boats for problem solving, but answering this question forces me to rotate my mind into a configuration that is probably quite different from whatever I was stuck in before. If this pen were a boat, it would be a sleek but sturdy racing boat with a silver sail and an athletic captain steering.The more perceptually dexterous you are, the less constrained you are to see only what you saw in your very first glance. You are not trapped in your most familiar perspective.\u00a0Thoroughness is what results from the successful exercise of perceptual dexterity. It is observation that continues well beyond familiarity, traversing many vantage points to triangulate reality.The thing that tenacity, openness, and thoroughness have in common is what I mean by \u201cpatience\u201d. It\u2019s the opposite of \u201crushing\u201d, the opposite of \u201cjust wanting to be done\u201d, or the opposite of \u201cjumping to conclusions\u201d. Patience is taking the time to discover the real shape of the world.By \u201cpatient observation\u201d, then, I mean observation that is tenacious, open, and thorough. Knowing the territory requires direct observation that takes its time to discover the shape of the world.^It was technically an aileron roll.", "title": "Patient Observation", "karma": 39, "af": false, "userid": "uuYBzWLiixkbN3s7C", "id": "sAiHxHkQrsYsRpKFP"}, {"text": "Based on work done with Rebecca Gorman and Oliver Daniel-Koch.\nIn a previous post, I talked about GPT-3 and symbol grounding. This post presents a simpler example where GPT-3 fails (and succeeds) at grounding its symbols.\nUndead workers\nThe following text was presented to the Open AI beta playground (using the \"text-davinci-001\" option):\n\nShe died on Friday the 5th. On Monday the 1st, she went to work at the office.\nShe died on Friday the 5th. On Tuesday the 2nd, she went to work at the office.\nShe died on Friday the 5th. On Wednesday the 3rd, she went to work at the office.\nShe died on Friday the 5th. On Thursday the 4th, she went to work at the office.\nShe died on Friday the 5th. On Friday the 5th, she went to work at the office.\nShe died on Friday the 5th. On Saturday the 6th,\n\nGPT-3 fell straight into the obvious trap, completing it as:\n\nShe died on Friday the 5th. On Saturday the 6th, she went to work at the office.\n\nTurning on the \"Show probabilities: full spectrum\" option, we saw that the probability of that completion was over 99.7%. Sometimes GPT-3 would extend it further, adding:\n\nShe died on Friday the 5th. On Sunday the 7th, she went to work at the office.\n\nSo, the undead woman continued at her job, assiduous to the last - and beyond.\nTo check that GPT-3 \"knew\" that dead people didn't work, we asked it directly:\n\nCan people who die go to work at the office?\nThere is no evidence that people who die go to work at the office.\n\nUndead repetitive workers on the weekend\nThe above results show that simple repetitive prompts can cause GPT-3 to make stupid mistakes. Therefore GPT-3 doesn't 'understand' the word \"died\" - that symbol isn't grounded, right?\nBut the situation gets more complicated if change the prompt, removing all but the first mention of her dying:\n\nShe died on Friday the 5th. On Monday the 1st, she went to work at the office.\nOn Tuesday the 2nd, she went to work at the office.\nOn Wednesday the 3rd, she went to work at the office.\nOn Thursday the 4th, she went to work at the office.\nOn Friday the 5th, she went to work at the office.\nOn Saturday the 6th,\n\nFor that prompt \"she went to work at the office\" was still the most common completion. But it only happened about 43% of the time. Alternatively, GPT-3 sometimes found the completion \"she was found dead\". Kudos, GPT-3, you understand the prompt after all! That completion came up about 34% of the time.\nWhat other completions were possible? The shorter \"she died\" came up 11% of the time - medium points, GPT-3, you understood that her death was relevant, but you got the day wrong.\nBut there was one other avenue that GPT-3 could follow; the following had a joint probability of around 11%:\n\nshe stayed home.\nshe stayed at home.\nshe stayed in bed.\nshe did not go to work.\n\nThis seems to be a clear pattern of GPT-3 realising that Saturday was different where work was concerned. There is certainly a lot of weekend holidaying in its training set.\nSo there are three patterns competing within GPT-3 when it tries to complete this text. The first is the purely syntactic repetition: do another sentence that follows the simple pattern of the sentences above. The second is the one which \"realises\" that death on Friday changes things for Saturday. And the third is the one which \"realises\" that the weekend is different from the week, at least where work is concerned.\nIn the very first example, when we had \"She died on Friday the 5th\" in front of each line, this massively reinforced the \"repetition\" pattern. So, mentioning that she died, again and again, resulted in her death being completely ignored by GPT-3.\nWe can similarly reinforce the other patterns. Adding \"It's the weekend!\" in front of the last line increased the probability of \"she stayed home\". Moving \"She died on Friday the 5th\" from the first line to the last, increased the probability of all the death-related completions. So all three patterns are competing to complete it.\nSome small level of understanding\nI'd say that the above shows that GPT-3 has some level of understanding of the meaning of words - but not a lot. It doesn't fully grasp what's going on, but neither is it completely clueless.\nHere is another example of GPT-3 failing to grasp the situation. In the \"Q&A\" mode, the following question was asked:\n\nQ: You are in a two dimensional 3x3 gridworld, in the middle square, facing north. You can issue four command, \"N\" to move north, \"S\" to move south, \"E\" to move east, and \"W\" to move West. You want the treasure. It is located to your left, behind you.\nWhat commands should you issue?\\\n\nSo the setup, as described, is this one:\n\nThe exchange with GPT-3 went like this:\n\nQ: You are in a two dimensional 3x3 gridworld, in the middle square, facing north. You can issue four command, \"N\" to move north, \"S\" to move south, \"E\" to move east, and \"W\" to move West. You want the treasure. It is located to your left, behind you.\nWhat commands should you issue?\nA: N, S, E, W\nQ: That is incorrect. Try again.\nA: N, S, W, E\nQ: Still incorrect. Try again.\nA: N, E, S, W\nQ: That's worse. Try again.\nA: N, E, W, S\nQ: Only two commands are needed. Try again.\nA: N, S\nQ: Still failing. Try again.\nA: N, E, W\n\nSo, GPT-3 'realised' that N, S, E, and W were commands, and 'knew' what \"Only two commands are needed\" and \"try again\" meant. But it clearly had no idea of the overall situation.\n", "title": "More GPT-3 and symbol grounding", "karma": 14, "af": true, "userid": "uCfjEXpnchoqDWNoL", "id": "QppXf4yfcG8JAKhnw"}, {"text": "[Epistemic status: Probably true, but very incomplete. I\u2019m just some guy who spent a few days reading studies and literature reviews on this subject. This is just a short look at some of the technical (i.e. non-fundamental; could be solved with better techniques) issues that are encountered in the process of measuring happiness.]Terms and Definitions ApplyThe tradition in scientific literature about happiness seems to be to start by mentioning how much we humans care about happiness. Far be it from me to break tradition, so: obviously, we care a lot about happiness. It\u2019s the thing most people spend most of their lives trying to attain (either for themselves or for others).Um \u2026 how, exactly, do we know we have it, though? It\u2019s not like you can just grab a Happiness Thermometer, stick it up your nose into your brain, and measure how happy you are.Worse yet, people don\u2019t even seem to agree on what happiness is. Aristotle calls it\u201cactivity of the soul in conformity with excellence or virtue\u201d,while the psychologist Kahneman defines it as\u201cwhat I experience here and now\u201d.Modern happiness research by social scientists seeks to incorporate both definitions as aspects of a broader concept of welfare. Much effort has gone into differentiating the two; Huta & Waterman (2014) is the prime example, a valiant attempt at classifying both terms. I will be drawing heavily from this study throughout this post, starting with these definitions:Aristotle\u2019s concept is called eudaimonia, the Greek term he himself used; \u201cgood spirit\u201d is a strict translation of the word, but more broadly it means \u201cflourishing\u201d. The core concepts are virtue, identity, meaning and self-actualization. \u201cPsychological well-being\u201d is a synonym often used in the literature.Kahneman\u2019s concept, to keep the Greek vibe going, is called hedonia. Hedonia is about good feelings: pleasure and happiness are core concepts. Many authors add comfort and/or absence of distress. \u201cSubjective well-being\u201d is the corresponding synonym.There\u2019s another important distinction here, between state levels and trait levels.Someone\u2019s state level of hedonia or eudaimonia measures the degree of the relevant concept in a specific moment (how happy you are right now; how virtuous you have been today) while their trait level is their degree of hedonia or eudaimonia throughout their life, like an inherent trait of that person.Having completed the ritual of defining the things we\u2019re speaking about (maybe it would\u2019ve been better to just Taboo the words), we can finally get on to the meat. How do we measure this stuff?Eudaimonia and hedonia are both typically measured by self-report surveys. People are given a survey with statements like \u201cI feel that life is very rewarding\u201d or \u201cI am very happy\u201d and asked to rate how much they agree with the statement on a scale from 0 to X (X usually being 5, 6 or 7). The scientists then take the average of those numbers: there\u2019s your happiness score!Problem #1: Poor MemoryA Famous NumberThis self-report method has been used to produce findings like Deaton & Kahneman (2010)\u2019s $75000 figure (if you haven\u2019t heard of it, don\u2019t worry: countless pop science articles will be happy to explain how \u201cThe Price of Happiness [is] $75000\u201d).Of course, all those articles\u2019 headlines are a bit misleading. Why? Because they leave out the other finding that was also in the title of the study: \u201cHigh income improves evaluation of life but not emotional well-being\u201d.According to the study, emotional well-being (state level hedonia, as measured by questions like \u201cdid you feel X emotion yesterday?\u201d) stayed constant above $75000. That wasn\u2019t the only variable being measured, though. They measured life satisfaction (a trait-level correlate of both hedonia and eudaimonia) with one simple question:Please imagine a ladder with steps numbered from 0 at the bottom to 10 at the top. The top of the ladder represents the best possible life for you, and the bottom of the ladder represents the worst possible life for you. On which step of the ladder would you say you personally feel you stand at this time?Notice how the \u201cLadder\u201d line, which represents life satisfaction, keeps rising with log income after $80000. The \u201cPositive affect\u201d, \u201cNot blue\u201d, and \u201cStress free\u201d lines, all different measures of state hedonia, do not.So over $75000/year, people continue to get more life satisfaction, but not more experienced well being / state hedonia. That makes sense, right? I can easily think of an explanation: maybe after $75000, we run out of things we really care about spending money on, and start throwing it at frivolous luxuries that don\u2019t actually make us happier, but our life satisfaction keeps rising because we\u2019re comparing our lives to others\u2019. The authors mention a similar explanation:Perhaps $75,000 is a threshold beyond which further increases in income no longer improve individuals\u2019 ability to do what matters most to their emotional well-being, such as spending time with people they like, avoiding pain and disease, and enjoying leisure.We\u2019ve got our empirical findings and a mechanical explanation for them. Case closed, right? Wrong.Hi there! How are you feeling right now?So what\u2019s the problem here? Well, the study\u2019s authors were a bit out of date. See, Deaton and Kahneman neglected to use the hottest new tech in happiness measurement: experience sampling.Experience sampling is when instead of asking people to report on how they were feeling yesterday or last week, you prompt them with surveys throughout the day to ask them how they\u2019re feeling at that exact moment. This was made possible by the rise of smartphones, which people are carrying with them at every point of the day and always checking. For the researchers, it\u2019s as easy as sending an automated text message a few times per day![1]\u00a0Aside from more thoroughly annoying your test subjects, though, what\u2019s the point of this new method? Well, human memory is notoriously unreliable. We\u2019ve known that ever since Elizabeth Loftus\u2019 countless memory experiments. So there\u2019s no reason to expect our memory of how we were feeling yesterday to be any better.Indeed, there are plenty of studies showing that our memory of emotions is quite fallible. There\u2019s the peak-end rule, first discovered in Kahneman (1993), which states that when remembering an experience, we only really judge its valence on two data points: the peak (the most intense point) and the end.[2]For example, in the Kahneman (1993) study I just mentioned, people judged sticking their hand in painfully cold water for 1 minute, then switching to slightly less cold water for another thirty seconds, less unpleasant than only doing the 1 minute in ice cold water part. It\u2019s the same experience! They just tacked on another painful bit at the end! If anything, people should be judging the longer ice bath worse. The peak-end rule is a form of duration neglect: we don\u2019t really care about how long the experience lasted, just how good or bad it was at certain points.Another one: Breckler (1994) asked blood donors 2, 10, or 49 days after their donation to remember how they felt donating blood. They then compared the memories to surveys taken at the time of donation by some of the subjects. Not only did they find that people remembered their emotions differently depending on their mood while taking the survey, but also (very important!) that donors remembered more anxiety the more time had passed since the donation.That is why we do experience sampling. If you ask people how they feel right that second, there\u2019s no time for the memory goblins to have gotten into their brains and messed up their memories yet. The emotions are still there.So predictably, when someone [Killingsworth (2020)] tried to replicate the $75000 study using experience sampling (and a few other design improvements like including more rich people), the results for experienced well-being - aka state hedonia - were quite different.Line go up means world more happier!This graph clearly shows that even after $75000, experienced well-being continues to rise with log income - that is, doubling your income will increase your happiness by roughly the same amount each time. That makes sense if you consider diminishing marginal utility - your 75001st dollar will increase your happiness a lot less than your 101st, but it isn\u2019t completely useless either.There we are. We\u2019ve found an elegant solution for our memory problems: cut out the memory middleman and just ask people how they\u2019re feeling right now.(An alternative is the Day Reconstruction Method: asking people to write all their experiences of the day in a diary, clearly define the duration of each experience, then evaluate how they felt during each experience. Kahneman, Krueger, Schkade, Schwarz and Stone (2004) show that this method produces basically the same results as experience sampling, implying that it is at least nearly as good; maybe even better. The authors say that this method works because writing down the experiences makes the test subjects relive their feelings in the process of remembering.)Solution #1: Experience Sampling (or Day Reconstruction)Problem #2a: AsymmetryFour Humours Elements CategoriesHuta & Waterman say that, in happiness research, there are four categories of analysis:(a) orientations: orientations, values, motives, and goals(b) behaviors: behavioral content and activity characteristics(c) experiences: subjective experiences, emotions, and cognitive appraisals(d) functioning: indices of positive psychological functioning, mental health, and flourishingA hedonic orientation might be something like \u201cI really value having fun with my friends\u201d, while a eudaimonic orientation could be \u201cI want to focus on self-realization\u201d.An example of a hedonic behavior is eating ice cream with ALL THE TOPPINGS; reading classic literature is a quintessential eudaimonic behavior. Behavior is unique among these four because it can be reliably measured by something besides self-report: other-report. Just ask someone\u2019s parents, friends or spouse what they enjoy doing and bam, you\u2019ve got data!Feeling the tastes of all the strawberry ice cream and chocolate sprinkles and caramel sauce mix on your tongue is a hedonic experience; untangling the metaphor in La Peste or the satire of Max Havelaar is a eudaimonic experience.Finally, there\u2019s functioning. Self-acceptance is an important kind of eudaimonic functioning; as for hedonic functioning, Huta & Waterman say:[N]o measures or theory have been proposed regarding hedonia in the functioning category of analysis. It would be worth contemplating whether there is such a thing as positive hedonic functioning, and what such functioning would look like, or whether the entire notion of positive functioning falls under the concept of eudaimonia.An imbalance in the ForceHuta & Waterman identify an asymmetry here: often within the same study, hedonia will be assessed in terms of cognitive-affective experiences, while eudaimonia is assessed in terms of positive mental functioning or orientations.[This asymmetry originates from the philosophical distinction between eudaimonia and hedonia; note that Aristotle\u2019s definition of eudaimonia refers to \u201cactivity of the soul\u201d, aka functioning rather than experience, while Pigou defines hedonia in terms of \u201cstates of consciousness\u201d - experience, not functioning.]This makes these concepts harder to study because studies that incorporate this asymmetry might just be measuring differences between positive cognitive-affective experiences and positive mental functioning, rather than differences between hedonia and eudaimonia.And here\u2019s the authors\u2019 explanation of another problem with the asymmetry:Asymmetrical treatment of eudaimonia and hedonia also makes it difficult to directly compare the empirical relationships they have with predictors or outcomes. For example, when studying the links between various predictors and well-being outcomes, and when the aim is to include both eudaimonic and hedonic conceptions of well-being\u00a0outcomes, researchers often operationalize eudaimonia using Ryff\u2019s Scales of Psychological Well-Being (Ryff 1989), which assess eudaimonia in terms of qualities associated with positive psychological functioning; on the other hand, studies often operationalize hedonia using one or more component(s) of subjective well-being (Diener 2000)\u2014positive affect, negative affect, and/or life satisfaction.The example they use is an optimism intervention: say you give your test subjects some self-help books and then make them do team-building exercises while shouting inspirational quotes at them. Then you measure their well-being, and it seems like hedonia improved more than eudaimonia.Now, if you assessed both terms by e.g. experience, there\u2019s no problem here - go publish your results in a journal, watch all the business magazines run articles on it, and soon enough every corporation in America will be running your optimism interventions. Good job!But if you assess hedonia in terms of experience and eudaimonia in terms of functioning, then - well, realistically, the business magazines and corporations will run with it anyway - but the ghost of Thomas Bayes will be very disappointed in you, because you can\u2019t actually be sure that this is a causal effect. There are quite a few possible confounders here; for example, maybe your cognitive state just changes more easily than your psychological functioning!Solution #2a: Use the same analysis categories when comparing thingsProblem #2b: Unclear DefinitionsWho\u2019s that entering the arena? Oh my god, it\u2019s Semantics with a steel chair!You believed me earlier when I said we were done with definitions? HA! We are never done with definitions.In fact, the definitions I posted at the beginning of this post were synthesized by Huta & Waterman in this very same paper. They took a bunch of studies on well-being and looked at the elements included in the definitions in each study. Here\u2019s the table.It\u2019s clear that while there are some concepts everyone considers to be core to eudaimonia and hedonia, there\u2019s also a fair amount of disagreement in the literature. (And some researchers just want to see the world burn; what was Vitters\u00f8 smoking when he wrote his definition of hedonia?)This is a severe impediment to happiness research, especially if researchers are unaware that others are using different definitions. Quote:The most striking example involves the correlation between measures of eudaimonia and hedonia, as shown in Table\u00a01. When focusing on the trait level\u2014i.e., a person\u2019s typical or average degrees of eudaimonia and hedonia\u2014the correlation has ranged from as low as .0 to as high as .6. When focusing on the state level\u2014i.e., a particular point in time, a given span of time, or a particular type of activity\u2014the correlation between eudaimonia and hedonia has ranged even more widely, from \u2212.3 to .8.Those are huge differences in correlation, even for social science! There are clearly big differences in our results depending on the definitions we use.Looking at the table, we can see that the strongest links between eudaimonia and hedonia are when they are defined as experiences: \u201cfeeling engaged (measured as a common factor)\u201d and \u201cfeeling pleasure (measured as a common factor)\u201d are correlated at .6; \u201ceudaimonic well-being\" and life satisfaction at .5. It seems that one contributes to the other - or perhaps eudaimonic and hedonic experience partially overlap, with people placing them into the same bucket of \u201cgood feelings\u201d.The weakest correlations, meanwhile, are at the behavior level of definition. Eudaimonic behavior and hedonic behavior only correlate at .1. This passes the sniff test, too - the common image of a hedonist is not necessarily that of someone who behaves virtuously or puts effort into self-realization.Orientation is in-between. Eudaimonic constitutive goals & hedonic instrumental goals correlate at .4; same for eudaimonic & hedonic motives. However, eudaimonic and hedonic orientations to happiness only correlate at .2, and there\u2019s no connection at all between \u201cintrinsic aspirations\u201d and hedonic aspirations.Clearly, we need to get all the researchers on the same page. Huta & Waterman have taken a great first step in this regard by making the distinctions clear and classifying all of the core components of hedonia and eudaimonia. Given the 358 citations on this paper, it seems as if most of the field has accepted these norms.Solution #2b: Get everyone to use the same definitionsProblem #3: Response StyleYou got a 5/10 on your Happiness exam, try harder next time.Have you ever tried comparing school grades with someone from a different country? It\u2019s not simple. Tell your American friend that you got a 17/20, they\u2019ll just look at you confused and wonder what you\u2019re so excited about. Meanwhile, when a Belgian student hears of someone who got straight A\u2019s, their jaw will drop - that\u2019s almost unheard of here. It\u2019s not that the Belgian school system is harder, or that American kids are smarter; they\u2019re just using different scales.Two economists, Jorge Alvarez and Fernanda Marquez-Padilla, wondered if people in countries with different grading systems have different response styles. For example, if you\u2019re in the Philippines where 75% is the passing grade, you might report your life satisfaction at 8/10 despite feeling meh about your life - after all, that\u2019s barely above passing, right? A Finnish person on the other hand might rate their life a 5/10 even if they\u2019re quite happy, because for them, a 1/5 is still a pass.[3]\u00a0Alvarez & Marquez-Padilla (2018) is the study where they test this. The results are pretty much exactly what the hypothesis predicted - pass-fail threshold (PFT) was significantly correlated with questions that required a numerical assessment, but not at all with categorical questions (e.g. \u201chave you ever felt on top of the world\u201d, \u201chave you ever felt depressed or very unhappy\u201d). When they try doing a regression to correct for this bias, the \u201cimputed happiness\u201d (i.e. happiness after correction) is more strongly correlated with log income than is reported happiness.This suggests a relatively straightforward solution: correct for grading bias with a simple regression. And indeed, that would solve this specific instance of response style bias. But there\u2019s more!Krueger, Kahneman, Fischler et al. (2009) says that French people use the far ends of the scale less frequently than Americans do, thus biasing the results so it looks like Americans are happier. And according to Meisenberg & Williams (2008), the less intelligent someone is, the more they exhibit the opposite bias - choosing the 1 and 7 more often on a 1-7 scale, for example.This bias is called extreme response style (ERS) and is found basically everywhere. A 2016 meta-analysis indicates further differences between races (Blacks and Hispanics exhibit more ERS, Whites and Asians less) and gender (females exhibit slightly more ERS than men), as well as a positive correlation with acquiescence, which is the tendency to say \u2018yes\u2019 to questions asked in surveys. (Acquiescent response set (ARS) is a confounder in its own right, and should be controlled for.)[4]\u00a0There\u2019s a whole array of other response styles/sets out there:People respond in socially desirable ways; if they believe that happiness is normatively appropriate, they may report that they are happier than other types of assessments may indicate. Social desirability of happiness may also vary from country to country.Evasiveness; maybe people of a certain social category or with a certain mindset are more likely to answer \u201cNot Sure\u201d, thus removing them from the data and giving a skewed picture.Speed vs. accuracy; some people will take their time and think deeply about their answers, while others will fill in the first thing that comes to mind.If you want to control for all of these and more, that\u2019s a lot of effort. You can never be sure if there are more confounders out there, either, which is why some scientists don\u2019t believe controlling for confounders can ever prove causality; there\u2019s always one you\u2019ve missed.Luckily, we have another measure to help mitigate the damage: item direction balance. Instead of having all the \u201cgood\u201d answers on one side of the scale and all the \u201cbad\u201d answers on the other, switch it up once in a while! This is most effective against acquiescence, because someone tempted to answer \u2018yes\u2019 or answer on the high side of the scale won\u2019t bias your results any more.Solution #3: Controls and Direction BalanceConclusionThere you have it: three problems and three solutions! To summarize:Imperfect memory effects can be solved by using experience sampling or day reconstruction.Asymmetry and unclear definitions can be solved by getting all the researchers on the same page and making them use the same definitions.Response style differences can be solved by directional balancing of questions and ungodly amounts of controls.This is a very long post, even after I cut out the aspiration treadmill part (Kahneman\u2019s thoughts about the aspiration treadmill, for those interested). And I haven\u2019t even touched on any of the deeper critiques of the field yet! There\u2019s philosophical critiques, like Angner (2013) and sociological ones like Frawley (2015). All worth reading, all beyond the scope of this post!Happiness research is something certainly worth continuing, for the sake of everyone who would like to enjoy their life. Considering that happiness studies only really started in the 1980s with positive psychology, and that economics and sociology joined in the 2000s and 2010s respectively, there\u2019s certainly a lot left to be discovered in this latest subject to move from the domain of philosophy to that of science.Let\u2019s put in the effort.SourcesThe ones I actually usedBatchelor, J., & Miao, C. (2016). Extreme Response Style: A Meta-Analysis. Journal of Organizational Psychology, 16. https://www.researchgate.net/publication/316820164_Extreme_Response_Style_A_Meta-AnalysisBreckler, S. J. (1994). Memory for the Experience of Donating Blood: Just How Bad Was It? Basic and Applied Social Psychology, 15(4), 467\u2013488. https://doi.org/10.1207/s15324834basp1504_5Huta, V., & Waterman, A. S. (2013). Eudaimonia and Its Distinction from Hedonia: Developing a Classification and Terminology for Understanding Conceptual and Operational Definitions. Journal of Happiness Studies, 15(6), 1425\u20131456. https://doi.org/10.1007/s10902-013-9485-0Kahneman, D., Fredrickson, B. L., Schreiber, C. A., & Redelmeier, D. A. (1993). When More Pain Is Preferred to Less: Adding a Better End. Psychological Science, 4(6), 401\u2013405. https://doi.org/10.1111/j.1467-9280.1993.tb00589.xKahneman, D., Krueger, A. B., Schkade, D. A., Schwarz, N., & Stone, A. A. (2004). A Survey Method for Characterizing Daily Life Experience: The Day Reconstruction Method. Science, 306(5702), 1776\u20131780. https://doi.org/10.1126/science.1103572Kahneman, D., & Krueger, A. B. (2006). Developments in the Measurement of Subjective Well-Being. Journal of Economic Perspectives, 20(1), 3\u201324. https://doi.org/10.1257/089533006776526030Kahneman, D., & Deaton, A. (2010). High income improves evaluation of life but not emotional well-being. Proceedings of the National Academy of Sciences, 107(38). https://doi.org/10.1073/pnas.1011492107Killingsworth, M. A. (2021). Experienced well-being rises with income, even above $75,000 per year. Proceedings of the National Academy of Sciences, 118(4). https://doi.org/10.1073/pnas.2016976118Krueger, A. B., Kahneman, D., Fischler, C., Schkade, D., Schwarz, N., & Stone, A. A. (2008). Time Use and Subjective Well-Being in France and the U.S. Social Indicators Research, 93(1), 7\u201318. https://doi.org/10.1007/s11205-008-9415-4Larson, R., & Csikszentmihalyi, M. (2014). The Experience Sampling Method. Flow and the Foundations of Positive Psychology, 21\u201334. https://doi.org/10.1007/978-94-017-9088-8_2Marquez-Padilla, F., & Alvarez, J. (2018). Grading happiness: what grading systems tell us about cross-country wellbeing comparisons. Economics Bulletin, 38. https://ideas.repec.org/a/ebl/ecbull/eb-18-00325.htmlMeisenberg, G., & Williams, A. (2008). Are acquiescent and extreme response styles related to low intelligence and education? Personality and Individual Differences, 44(7), 1539\u20131550. https://doi.org/10.1016/j.paid.2008.01.010Powdthavee, N. (2007). View of Economics of Happiness: A Review of Literature and Applications. Chulalongkorn Journal of Economics, 19. https://www.researchgate.net/publication/228373537_Economics_of_Happiness_A_Review_of_Literature_and_ApplicationsRyff, C. D. (1989). Happiness is everything, or is it? Explorations on the meaning of psychological well-being. Journal of Personality and Social Psychology, 57(6). https://doi.org/10.1037/0022-3514.57.6.1069Schneider, S. (2016). Extracting Response Style Bias From Measures of Positive and Negative Affect in Aging Research. The Journals of Gerontology Series B: Psychological Sciences and Social Sciences, gbw103. https://doi.org/10.1093/geronb/gbw103Strijbosch, W., Mitas, O., van Gisbergen, M., Doicaru, M., Gelissen, J., & Bastiaansen, M. (2019). From Experience to Memory: On the Robustness of the Peak-and-End-Rule for Complex, Heterogeneous Experiences. Frontiers in Psychology, 10. https://doi.org/10.3389/fpsyg.2019.01705\u00a0Extra readingAngner, E. (2013). Is it possible to measure happiness? European Journal for Philosophy of Science, 3(2), 221\u2013240. https://doi.org/10.1007/s13194-013-0065-2Binswanger, M. (2006). Why does income growth fail to make us happier? The Journal of Socio-Economics, 35(2), 366\u2013381. https://doi.org/10.1016/j.socec.2005.11.040Diener, E. (2000). Subjective well-being: The science of happiness and a proposal for a national index. American Psychologist, 55(1), 34\u201343. https://doi.org/10.1037/0003-066x.55.1.34Frawley, A. (2015). Happiness Research: A Review of Critiques. Sociology Compass, 9(1), 62\u201377. https://doi.org/10.1111/soc4.12236Jain, M., Sharma, G. D., & Mahendru, M. (2019). Can I Sustain My Happiness? A Review, Critique and Research Agenda for Economics of Happiness. Sustainability, 11(22). https://doi.org/10.3390/su11226375OECD. (2013). OECD Guidelines on Measuring Subjective Well-being | READ online. Oecd-Ilibrary.Org. Retrieved February 6, 2022, from https://read.oecd-ilibrary.org/economics/oecd-guidelines-on-measuring-subjective-well-being_9789264191655-en#page10\u00a0^The invention of the experience sampling method came earlier, though: Reed Larson & Mihaly\u00a0Csikszentmihalyi were already talking about it in 1983. They mention using pagers to alert their subjects. Not really sure why this method only took off in the 2010s, then; maybe the smartphones were just that much more convenient?^It has been debated whether or not the peak-end rule can be generalized from simple experiments to real-world situations; Strijbosch et al. (2019) finds that average valence & arousal is a better predictor than peak-end in the case of more realistic, heterogeneous experiences. They also find that averages, compared to peak-end measures, are better predictors the more time has passed.^The Finnish grading system is actually analogous to the US letter grading system, but instead of A/B/C/D/F, the grades are 5/4/3/2/1. So a 1 will be between <40% and <60%, depending on the curve.^Age is an interesting one: researchers all agree it does something to ERS, but really seem to disagree on what it does. The meta-analysis I just mentioned says that ERS drops sharply with age, consistent with either a linear effect or a curvilinear effect where ERS rises until one\u2019s early 20s, then drops. Schneider (2016), which also analyses the existing literature and adds its own study, says the likelihood of ERS increases significantly with age. I have no clue what to think about this, except that the decrease-over-age conclusion seems intuitively more correct - aren\u2019t people supposed to moderate with time?", "title": "Some Technical Problems With Measuring Happiness (and some solutions)", "karma": 7, "af": false, "userid": "nrZKuWY7sS4Aj54DF", "id": "yRPcLejaTXTwFrhYv"}, {"text": "5.1 Post summary / Table of contentsPart of the \u201cIntro to brain-like-AGI safety\u201d post series.In the previous post, I discussed the \u201cshort-term predictor\u201d\u2014a circuit which, thanks to a learning algorithm, emits an output that predicts a ground-truth supervisory signal arriving a short time (e.g. a fraction of a second) later.In this post, I propose that we can take a short-term predictor, wrap it up into a closed loop involving a bit more circuitry, and we wind up with a new module that I call a \u201clong-term predictor\u201d. Just like it sounds, this circuit can make longer-term predictions, e.g. \u201cI\u2019m likely to eat in the next 10 minutes\u201d. This circuit is closely related to Temporal Difference (TD) learning, as we\u2019ll see.I will argue that there are a large collection of side-by-side long-term predictors in the brain, each comprising a short-term predictor in the telencephalon (involving specific areas such as ventral striatum, medial prefrontal cortex, and amygdala) that loops down to the Steering Subsystem (hypothalamus and brainstem) and then back via a dopamine neuron. These long-term predictors make predictions about biologically-relevant inputs and outputs\u2014for example, one long-term predictor might predict whether I\u2019ll feel pain in my arm, another whether I\u2019ll get goosebumps, another whether I\u2019ll release cortisol, another whether I\u2019ll eat, and so on. Moreover, one of these long-term predictors is essentially a value function for reinforcement learning.All these predictors will play a major role in motivation\u2014a story which I will finish in the next post.Table of contents:Section 5.2 starts with a toy model of a \u201clong-term predictor\u201d circuit, consisting of the \u201cshort-term predictor\u201d of the previous post, plus some extra components, wrapped into a closed loop. Getting a good intuitive understanding of this model will be important going forward, and I will walk through how that model would behave under different circumstances.Section 5.3 relates that model to Temporal Difference (TD) learning, which is closely related to a \u201clong-term predictor\u201d. I\u2019ll show two variants of the long-term predictor circuit, a \u201csummation\u201d version (which leads to a value function that approximates the sum of future rewards), and a \u201cswitch\u201d version (which leads to a value function that approximates the next reward, whenever it should arrive, which may not be for a long time). The \u201csummation\u201d version is universal in AI literature, but I\u2019ll suggest that the \u201cswitch\u201d version is probably closer to what happens in the brain. Incidentally, these two models are equivalent in cases like AlphaGo, wherein reward arrives in a lump sum right at the end of each episode (= game of Go).Section 5.4 will relate long-term predictors to the neuroanatomy of (part of) the telencephalon and brainstem.For the \u201cvertical\u201d neuroanatomy,[1]\u00a0I\u2019ll describe how the brain houses a huge number of parallel \u201ccortico-basal ganglia-thalamo-cortical loops\u201d, and I\u2019ll suggest that some of these loops function as short-term predictors, with a dopamine signal as supervisor.For the \u201chorizontal\u201d neuroanatomy, I\u2019ll propose that the supervised learning I\u2019m talking about involves (for example) the medial prefrontal cortex, ventral striatum, anterior insular cortex, and amygdala.Section 5.5 will offer six lines of evidence that lead me to believe this story: (1) It\u2019s a sensible way to implement a biologically-useful capability; (2) It\u2019s introspectively plausible; (3) It\u2019s evolutionarily plausible; (4) It offers a reconciliation between the \u201cvisceromotor\u201d and \u201cmotivational\u201d ways to describe the medial prefrontal cortex; (5) It explains the Dead Sea Salt experiment; and (6) It offers a nice explanation of the diversity of dopamine neuron activity.5.2 Toy model of a \u201clong-term predictor\u201d circuitA \u201clong-term predictor\u201d is ultimately nothing more than a short-term predictor whose output signal helps determine its own supervisory signal. Here\u2019s a toy model of what that can look like:Toy model of a long-term prediction circuit. I\u2019ll spend the next couple subsections walking through how this works.The blue box is the short-term predictor of the previous post. It optimizes its output signal such that it approximates what the supervisor signal will be in 0.3 seconds (as an example).The purple box is a 2-way switch. The toggle on the switch is controlled by genetically-hardwired circuitry (gray oval), according to the following rules:By and large, the switch is in the bottom setting (\u201cdefer-to-predictor mode\u201d). This setting is akin to the genetically-hardwired circuitry \u201ctrusting\u201d that the short-term predictor\u2019s output is sensible, and in particular producing the suggested amount of digestive enzymes.If the genetically-hardwired circuitry gets a signal that I\u2019m eating something right now, and that I don\u2019t have adequate digestive enzymes, it flips the switch to \u201coverride mode\u201d. Regardless of what the short-term predictor says, it sends the signal to manufacture digestive enzymes.If the genetically-hardwired circuitry has been asking for digestive enzyme production for an extended period, and there\u2019s still no food being eaten, then it again flips the switch to \u201coverride mode\u201d. Regardless of what the short-term predictor says, it sends the signal to stop manufacturing digestive enzymes.Note: You can assume that all the signals in the diagram can vary continuously across a range of values (as opposed to being discrete on/off signals), with the exception of the signal that toggles the 2-way switch.[2]\u00a0In the brain, smoothly-adjustable signals might be created by, for example, rate-coding\u2014i.e., encoding information as the frequency with which a neuron is firing.5.2.1 Toy model walkthrough part 1: static contextLet\u2019s walk through what would happen in this toy model.[3]\u00a0To start with, assume that the \u201ccontext\u201d is static for some extended period of time. For example, imagine a situation where some ancient worm-like creature is digging in the sandy ocean bed for many consecutive minutes. Plausibly, its sensory environment would stay pretty much constant as long as it keeps digging, as would its thoughts and plans (insofar as this ancient worm-like creature has \u201cthoughts and plans\u201d in the first place). Or if you want another example of (approximately) static context\u2014this one involving a human rather than a worm\u2014hang on until the next subsection.In the static-context case, let\u2019s first consider what happens when the switch is sitting in \u201cdefer-to-predictor mode\u201d: Since the output is looping right back to the supervisor, there is no error in the supervised learning module. The predictions are correct. The synapses aren\u2019t changing. Even if this situation is very common, it has no bearing on how the short-term predictor eventually winds up behaving.The times that do matter for the eventual behavior of the short-term predictor are those rare times that we go into \u201coverride mode\u201d. Think of the overrides as like a sporadic \u201cinjection of ground truth\u201d. They produce an error signal in the short-term predictor\u2019s learning algorithm, changing its adjustable parameters (e.g. synapse strengths).After enough life experience (a.k.a. \u201ctraining\u201d in ML terminology), the short-term predictor should have the property that the overrides balance out. There may still be occasional overrides that increase digestive-enzyme production, and there may still be occasional overrides that decrease digestive-enzyme production, but those two types of overrides should happen with similar frequency. After all, if they didn\u2019t balance out, the short-term predictor\u2019s internal learning algorithm would gradually change its parameters so that they did balance out.And that\u2019s just what we want! We\u2019ll wind up with appropriate digestive enzyme production at appropriate times, in a way that properly accounts for any information available in the context data\u2014what the animal is doing right now, what it\u2019s planning to do in the future, what its current sensory inputs are, etc.5.2.1.1 David-Burns-style exposure therapy\u2014a possible real-life example of the toy model with static context?As it happens, I recently read David Burns\u2019s book Feeling Great (my review). David Burns has a very interesting approach to exposure therapy\u2014an approach that happens to serve as an excellent example of how my toy model works in the static-context situation!Here\u2019s the short version. (Warning: If you\u2019re thinking of doing exposure therapy on yourself at home, at least read the whole book first!) Excerpt from the book:For example, when I was in high school, I wanted to be on the stage crew of Brigadoon, a play my school was putting on, but it required overcoming my fear of heights since the stage crew had to climb ladders and work near the ceiling to adjust the lights and curtains. My drama teacher, Mr. Krishak, helped me overcome this fear with the very type of exposure techniques I\u2019m talking about. He led me to the theater and put a tall ladder in the middle of the stage, where there was nothing nearby to grab or hold on to. He told me all I had to do was stand on the top of the ladder until my fear disappeared. He reassured me that he\u2019d stand on the floor next to me and wait.I began climbing the ladder, step by step, and became more and more frightened. When I got to the top, I was terrified. My eyes were almost 18 feet from the floor, since the ladder was 12 feet tall, and I was just over 6 feet tall. I told Mr. Krishak I was in a panic and asked what I should do. Was there something I should say, do, or think about to make my anxiety go away? He shook his head and told me to just stand there until I was cured.I continued to stand there in terror for about ten more minutes. When I told Mr. Krishak I was still in a panic, he assured me that I was doing great and that I should just stand there a few more minutes until my anxiety went away. A few minutes later, my anxiety suddenly disappeared. I couldn\u2019t believe it!I told him, \u201cHey, Mr. Krishak, I\u2019m cured now!\u201dHe said, \u201cGreat, you can come on down from the ladder now, and you can be on the stage crew of Brigadoon!\u201dI had a blast working on the stage crew. I absolutely loved climbing ladders and adjusting the lights and curtains near the ceiling, and I couldn\u2019t even remember why or how I\u2019d been so afraid of heights.This story seems to be beautifully consistent with my toy model here. David started the day in a state where his short-term-predictors output \u201cextremely strong fear reactions\u201d when he was up high. As long as David stayed up on the ladder, those fear-reaction short-term-predictors kept on getting the same context data, and therefore they kept on firing their outputs at full strength. And David just kept feeling terrified.Then, after 15 boring-yet-terrifying minutes on the ladder, some innate circuit in David\u2019s brainstem issued an override\u2014as if to say, \u201cC\u2019mon, nothing is changing, nothing is happening, we can\u2019t just keep burning all these calories all day. It\u2019s time to calm down now.\u201d The short-term-predictors continued sending the same outputs as before, but the brainstem exercised its veto power, and forcibly reset David\u2019s cortisol, heart-rate, etc., back to baseline. This \u201coverride\u201d state immediately created error signals in the relevant short-term-predictors in David\u2019s amygdala! And the error signals, in turn, led to model updates! The short-term predictors were all edited, and from then on, David was no longer afraid of heights.This story kinda feels like speculation piled on top of speculation, but whatever, I happen to think it\u2019s right. If nothing else, it\u2019s good pedagogy! Here\u2019s the diagram for this situation; make sure you can follow all the steps.5.2.2 Toy model walkthrough, assuming changing contextThe previous subsections assumed static context lines (constant sensory environment, constant behaviors, constant thoughts and plans, etc.). What happens if the context is not static?If the context lines are changing, then it\u2019s no longer true that learning happens only at \u201coverrides\u201d. If context changes in the absence of \u201coverrides\u201d, it will result in changing of the output, and the new output will be treated as ground truth for what the old output should have been. Again, this seems to be just what we want: if we learned something new and relevant in the last second, then our current expectation should be more accurate than our previous expectation, and thus we have a sound basis for updating our models.5.3 Value function calculation (TD learning) as a special case of long-term predictionAt this point, ML experts will recognize a resemblance to Temporal Difference (TD) learning. It\u2019s not quite the same, though. The differences are:First, TD learning is usually used in reinforcement learning (RL) as a method for going from a reward function to a value function. By contrast, I was talking about things like \u201cdigestive enzyme production\u201d, which are neither rewards nor values.In other words, there is a generally-useful motif that involves going from some immediate quantity X to \u201clong term expectation of X\u201d. The calculation of a value function from a reward function is an example of that motif, but it\u2019s not the only useful example.(As a matter of terminology, it seems to be generally accepted that the term \u201cTD learning\u201d can in fact apply to things that are not RL value functions.[4]\u00a0However, empirically in my own experience, as soon as I mention \u201cTD learning\u201d, the people I\u2019m talking to immediately assume I must be talking about RL value functions. So I want to be clear here.)Second, to get something closer to traditional TD learning, we\u2019d need to replace the 2-way switch with a 2-way summation\u2014and then the \u201coverrides\u201d would be analogous to rewards. Much more on \u201cswitch vs summation\u201d in the next subsection.\u00a0Here\u2019s a TD learning circuit that would behave similarly to what you\u2019d see in an AI textbook. Note the purple box on the right: compared to the previous figure, I replaced the 2-way switch with a 2-way summation. More on \u201cswitch vs summation\u201d in the next subsection.Third, there are many additional ways to tweak the circuit which are frequently used in AI textbooks, and some of those may be involved in the brain circuits too. For example, we can put in time-discounting, or different emphases on false-positives vs false-negatives (see Section 5.5.6.1 below), etc.To keep things simple, I will be ignoring all these possibilities (including time-discounting) in the discussion below.5.3.1 Switch (i.e., value = expected next reward) vs summation (i.e., value = expected sum of future rewards)?The figures above show two variants of our toy model. In one, the purple box is a two-way switch between \u201cdefer to the short-term predictor\u201d and some independent \u201cground truth\u201d. In the other, the purple box is a two-way summation instead.The switch version trains the short-term-predictor to predict the next ground truth, whenever it should arrive.The summation version trains the short-term-predictor to predict the sum of future ground truth signals.The correct answer could also be \u201csomething in between switch and summation\u201d. Or it could even be \u201cnone of the above\u201d.RL papers universally use the summation version\u2014i.e., \u201cvalue is the expected sum of future rewards\u201d. What about biology? And which is actually better?It doesn\u2019t always matter! Consider AlphaGo. Like every RL paper today, AlphaGo was originally formulated in the summation paradigm. But it happens to have one and only one nonzero reward signal per game, namely +1 at the end of the game if it wins, or -1 if it loses. In that case, switch vs summation makes no difference. The only difference is one of terminology:In the summation case, we would say \u201ceach non-terminal move in the Go game has reward=0\u201d.In the switch case, we would say \u201ceach non-terminal move in the Go game has a reward of (null)\u201d.(Do you see why?)But in other cases, it does matter. So back to the question: should it be switch or summation?Let\u2019s step back. What are we trying to do here?One thing that a brain needs to do is make decisions that weigh cross-domain tradeoffs. If you\u2019re a human, you need to decide whether to watch TV or go to the gym. If you\u2019re some ancient worm-like creature, you need to \u201cdecide\u201d whether to dig or to swim. Either way, this \u201cdecision\u201d impacts energy balance, salt balance, probability of injury, probability of mating\u2014you name it. The design goal in the decision-making algorithm is that you make the decision that maximizes inclusive genetic fitness. How might that goal be best realized?One method involves building a value function that estimates the organism\u2019s inclusive genetic fitness (compared to some arbitrary\u2014indeed, possibly time-varying\u2014baseline), conditional on continuing to execute a given course of action. Of course it won\u2019t be a perfect estimate\u2014real inclusive genetic fitness can only be calculated in hindsight, many generations after the fact. But once we have such a value function, however imperfect, we can plug it into an algorithm that makes decisions to maximize value (more on this in the next post), and thus we get approximately-fitness-maximizing behavior.So having a value function is key for making good decisions that weigh cross-domain tradeoffs. But nowhere in this story is the claim \u201cvalue is the expectation of a sum of future rewards\u201d! That\u2019s a particular way of setting up the value-approximating algorithm, a method which might or might not be well suited to the situation at hand.I happen to think that brains use something closer to the switch circuit, not the summation circuit, not only for homeostatic-type predictions (like the digestive enzymes example above), but also for value functions, contrary to mainstream RL papers. Again, I think it\u2019s really \u201cneither of the above\u201d in all cases; just that it\u2019s closer to switch.Why do I favor \u201cswitch\u201d over \u201csummation\u201d?An example: sometimes I stub my toe and it hurts for 20 seconds; other times I stub my toe and it hurts for 40 seconds. But I don\u2019t think of the latter as twice as bad as the former. In fact, even five minutes later, I wouldn\u2019t remember which is which. (See the peak-end rule.) This is the kind of thing I would naturally expect from switch, but is an awkward fit for summation. It\u2019s not strictly incompatible with summation; it just requires a more complicated, value-dependent reward function. As a matter of fact, if we allow the reward function to depend on value, then switch and summation can imitate each other.Anyway, in upcoming posts, I\u2019ll be assuming switch, not summation. I don\u2019t think it matters very much for the big picture. I definitely don\u2019t think it\u2019s part of the \u201csecret sauce\u201d of animal intelligence, or anything like that. But it does affect some of the detailed descriptions.The next post will include more details of reinforcement learning in the brain, including how \u201creward prediction error\u201d works and so on. I am bracing for lots of confused readers, who will be disoriented by the fact that I\u2019m assuming a different relationship between value and reward than what everyone is used to. For example, in my picture, \u201creward\u201d is a synonym for \u201cground truth for what the value function should be right now\u201d\u2014both should account for not only the organism\u2019s current circumstances but also its future prospects. Sorry in advance for any confusion! I will do my best to be clear.5.4 An array of long-term predictors involving the telencephalon & brainstemHere\u2019s the long-term-predictor circuit from above:Copied from above.I can lump together the 2-way switch with the rest of the genetically-hardwired circuitry, and then rearrange the boxes a bit, and I get the following:Same as above, but drawn differently.Now, obviously digestive enzymes are just one example. Let\u2019s draw in some more examples, add some hypothesized neuroanatomy, and include other terminology. Here\u2019s the result:I claim that there is a bank of long-term-predictors, consisting of an array of short-term-predictors in the telencephalon, each with a closed-loop connection to a corresponding Steering Subsystem circuit. I\u2019m calling the former (telencephalon) part by the name \u201cthought assessors\u201d, for reasons explained in Section 5.5.4 below. Acronym: mPFC = medial prefrontal cortexExcellent! We\u2019re halfway to my big picture of decision-making and motivation. The rest of the picture\u2014including the \u201cactor\u201d part of actor-critic reinforcement learning\u2014will come in the next post, and will fill in the hole in the top-left side of that diagram.Here\u2019s one more diagram and caption for pedagogical purposes. (The term \u201cSteering Subsystem\u201d comes from Post #3.)Reminder: a \u201cshort-term predictor\u201d is one component of a \u201clong-term predictor\u201d. Here\u2019s where both those things fit into that diagram above. The only thing that makes it a long-term predictor is the possibility of \u201cdefer-to-predictor mode\u201d\u2014i.e., the Steering Subsystem might send a \u201cground truth in hindsight\u201d signal that is not really \u201cground truth\u201d in the normal sense, but is rather a copy of the corresponding entry on the scorecard. In other words, \u201cdefer-to-predictor mode\u201d is like the Steering Subsystem saying to the short-term predictor: \u201cOK sure, whatever, I\u2019ll take your word for it\u201d. If the Steering Subsystem regularly keeps a signal in \u201cdefer-to-predictor mode\u201d for 10 minutes straight, then we can get predictions that anticipate the future by up to 10 minutes. Conversely, if the Steering Subsystem never uses \u201cdefer-to-predictor mode\u201d for a certain signal, then we shouldn\u2019t really be calling it a \u201clong-term predictor\u201d in the first place.In the next two subsections, I will elaborate on the neuroanatomy which I\u2019m hinting at in this diagram, and then I\u2019ll talk about why you should believe me.5.4.1 \u201cVertical\u201d neuroanatomy:[1]\u00a0cortico-basal ganglia-thalamo-cortical loopsIn my post Big Picture of Phasic Dopamine, I talked about the theory (due originally to Larry Swanson) that the whole telencephalon is nicely organized into three layers (cortex, striatum, pallidum):Cortex-like part of the loops\u00a0Hippo-campusAmygdala [basolateral part]Piriform cortexMedial prefrontal cortexMotor & \u201cplanning\u201d cortexStriatum-like part of the loops\u00a0Lateral septumAmygdala [central part]Olfactory tubercleVentral striatumDorsal striatumPallidum-like part of the loops\u00a0Medial septumBNSTSubstantia innominataVentral pallidumGlobus pallidusThe entire telencephalon\u2014neocortex, hippocampus, amygdala, everything\u2014can be divided into cortex-like structures, striatum-like structures, and pallidum-like structures. If two structures are in the same column in this table, that means they\u2019re wired together into cortico-basal ganglia-thalamo-cortical loops (see next paragraph). This table is incomplete and oversimplified; for a better version see Fig. 4 here.This idea then connects to the earlier (and now widely accepted) theory, dating to Alexander 1986, that these three layers of the telencephalon are interconnected into a large number of parallel \u201ccortico-basal ganglia-thalamo-cortical loops\u201d, which can be found in almost every part of the telencephalon.Here\u2019s a little illustration:Simplified cartoon illustration of how the brain has many parallel cortico-basal ganglia-thalamo-cortical loops. Source: Matthieu Thiboust.Given all that, here is a possible rough model for how this loop architecture relates to the short-term predictor learning algorithm that I\u2019ve been talking about:See Big Picture of Phasic Dopamine for slightly more details, but mostly I haven\u2019t looked into it much, and in particular the \u201cLayer 1, Layer 2, Final (pooling) layer\u201d labels are kinda just spitballing. (The \u201cpooling\u201d is based on there being 2000\u00d7 more neurons in the striatum than the pallidum\u2014see here.) Acronyms: BLA=basolateral amygdala, BNST=bed nucleus of the stria terminalis, CEA=central amygdala, mPFC=medial prefrontal cortex, VP=ventral pallidum, VS=ventral striatum.5.4.2 \u201cHorizontal\u201d neuroanatomy\u2014cortical specializationThe previous subsection was about the \u201cvertical\u201d three-layer structure of the telencephalon. Now let\u2019s switch to the \u201chorizontal\u201d structure, i.e. the fact that different parts of the cortex do different things (in cooperation with the corresponding parts of the striatum and pallidum).This is oversimplified, but here\u2019s my latest attempt at (part of) the cortex in a nutshell:The extended motor cortex is the cortex\u2019s main output region for behaviors involving skeletal muscles, like reaching and walking.The medial prefrontal cortex (mPFC\u2014which also includes anterior cingulate cortex) is the cortex\u2019s main output region for behaviors involving autonomic / visceromotor / hormonal actions, like releasing cortisol, vasoconstriction, goosebumps, and so on.The amygdala is the cortex\u2019s main output region for certain behaviors that involve both skeletal muscle actions and autonomic actions, like flinching-reactions, or freezing-reactions (when frightened), and so on.The insular cortex is the cortex\u2019s main input region for autonomic / homeostatic / body status information, like blood sugar levels, pain, cold, taste, muscle strain, etc.I won\u2019t talk about the motor cortex in this series, but I think the other three are all involved in these long-term prediction circuits. For example:I claim that if you look at a little subregion in the medial prefrontal cortex, you might find that it\u2019s being trained to fire in proportion to the probability of upcoming cortisol release;I claim that if you look at a little subregion in the amygdala, you might find that it\u2019s being trained to fire in proportion to the probability of upcoming freezing-reactions;I claim that if you look at a little subregion of the (anterior) insular cortex, you might find that it\u2019s being trained to fire in proportion to the probability of upcoming cold feelings in your left arm.5.5 Six reasons I like this \u201carray of long-term predictors\u201d picture5.5.1 It\u2019s a sensible way to implement a biologically-useful capabilityIf you start producing digestive enzymes before eating, you\u2019ll digest faster. If your heart starts racing before you see the lion, then your muscles will be primed and ready to go when you do see the lion. Etc.So these kinds of predictors seem obviously useful.Moreover, as discussed in the previous post (Section 4.5.2), the technique I\u2019m proposing here (based on supervised learning) seems either superior to or complementary with other ways to meet these needs.5.5.2 It\u2019s introspectively plausibleFor one thing, we do in fact start salivating before we eat the cracker, start feeling nervous before we see the lion, etc.For another thing, consider the fact that all the actions I\u2019m talking about in this post are involuntary: you cannot salivate on command, or dilate your pupils on command, etc., at least not in quite the same way that you can wiggle your thumb on command.(More on voluntary actions in the next post\u2014they\u2019re in a whole different part of the telencephalon.)I\u2019m glossing over a bunch of complications here, but the involuntary nature of these things seems pleasingly consistent with the idea that they are being trained by their own dedicated supervisory signals, straight from the brainstem. They\u2019re slaves to a different master, so to speak. We can kinda trick them into behaving in certain ways, but our control is limited and indirect.5.5.3 It\u2019s evolutionary plausibleAs discussed in Section 4.4 of the previous post, the simplest short-term predictor is extraordinarily simple, and the simplest long-term predictor is only a bit more complicated than that. And these very simple versions are already plausibly fitness-enhancing, even in very simple animals.Moreover, as I discussed a while back (Dopamine-supervised learning in mammals & fruit flies), there is an array of little learning modules in the fruit fly, playing a seemingly-similar role to what I\u2019m talking about here. Those modules also use dopamine as a supervisory signal, and there is some genomic evidence of a homology between those circuits and the mammalian telencephalon.5.5.4 It offers a reconciliation between \u201cvisceromotor\u201d and \u201cmotivation\u201d pictures of the medial prefrontal cortex (mPFC)Take the mPFC (which also includes the anterior cingulate cortex\u2014ACC), as an example. People talk about this region in two quite different ways:On the one hand, as mentioned above (Section 5.4.2), mPFC is described as a visceromotor / homeostatic / autonomic motor output region\u2014it issues commands to control hormones, to execute sympathetic and parasympathetic nervous system reactions, and so on. For example, \u201celectrical stimulation of the infralimbic cortex has been shown to affect gastric motility and to cause hypotension\u201d, or this paper says stimulating mPFC caused \u201c[bristling]; pupillary dilation; and changes in blood pressure, respiratory rate, and heart rate\u201d, or see Bud Craig\u2019s book which offers various lines of evidence that ACC is a homeostatic motor output center. This way of thinking also elegantly explains the fact that the region is agranular (i.e., missing layer #4 out of the 6 neocortex layers), which implies \u201cmotor output region\u201d both for theoretical reasons and by analogy with the (agranular) motor cortex.On the other hand, mPFC is frequently described as being related to a host of vaguely-motivation-related activities. For example, Wikipedia mentions \u201cattention allocation, reward anticipation, decision-making, ethics and morality, impulse control \u2026 and emotion\u201d in regards to ACC.I think my picture works for both:[5]For the first (visceromotor) perspective, if you look at Section 5.2 above, you\u2019ll see that the predictors\u2019 outputs do in fact cause homeostatic changes\u2014at least, they do when the genetically-hardwired circuitry of the Steering Subsystem has set that signal in \u201cdefer-to-predictor mode\u201d (as opposed to \u201coverride mode\u201d).For the second (motivation) perspective, this will make a bit more sense after the next post, but note my suggestive description of a \u201cscorecard\u201d in the diagram of Section 5.4. The idea is: The \u201ccontext\u201d lines going into the \u201cthought assessors\u201d contain the horrific complexity of everything in your conscious mind and more\u2014where you are, what you\u2019re seeing and doing, what you\u2019re thinking about, what you\u2019re planning to do in the future and why, etc. The relatively simple, genetically-hardcoded Steering Subsystem can\u2019t make heads or tails of any of that!But that\u2019s a dilemma, because the Steering Subsystem is the source of rewards / drives / motivations! How can the Steering Subsystem issue rewards for a good plan, if it can\u2019t make heads or tails of what you\u2019re planning??The \u201cscorecard\u201d is the answer. It takes all that horrific complexity and distills it into a nice standardized scorecard\u2014exactly the kind of thing that genetically-hardcoded circuits in the Steering Subsystem can easily process.Thus, whenever there\u2019s an interaction between thoughts and drives\u2014emotions, decision-making, ethics, aversions, etc.\u2014the \u201cthought assessors\u201d need to be involved as an intermediary.5.5.5 It explains the dead-sea salt experimentSee my discussion in my old post Inner alignment in salt-starved rats. In brief, experimenters sporadically played a sound and popped an object into a rat\u2019s cage, and immediately thereafter sprayed super-salty water directly into the rat\u2019s mouth. The rat found the saltwater disgusting, and started reacting with horror to the sound and object. Then later, the experimenters made the rat feel salt-deprived. When they played the sound and popped the object this time, the rat got very excited\u2014even though the rat had never been salt-deprived before in its life.In our setup, this is exactly what we expect: when the sound and object appear, the \u201cI anticipate tasting salt\u201d predictor starts firing like crazy. Meanwhile the Steering Subsystem (hypothalamus & brainstem) has hardwired circuitry that says \u201cIf I\u2019m salt-deprived, and if the \u2018scorecard\u2019 from the Learning Subsystem suggests that I will soon taste salt, then that\u2019s awesome, and whatever thought the Learning Subsystem is thinking, it should pursue that idea with gusto!\u201d5.5.6 It offers a nice explanation for the diversity of dopamine neuron activityRecall from Section 5.4.1 above that I\u2019m claiming that dopamine neurons carry the supervisory signals of all these supervised-learning modules.[6]There\u2019s a pop-science misconception that there is a (singular) dopamine signal in the brain, and it bursts when good things are happening. In reality, there are many different dopamine neurons doing many different things.Thus we get the question: what are all these diverse dopamine signals doing? There\u2019s no consensus; claims in the literature are all over the place. But I can throw my hat into the ring: in my picture described above, there are probably hundreds or thousands of short-term predictors in the telencephalon, predicting hundreds or thousands of different things, and they each need a different dopamine supervisory signal!(And there are even more dopamine signals besides those! One such signal, associated with the brain\u2019s \u201cmain\u201d reward prediction error signal, will show up in the next post. Still others are off-topic for this series but discussed here.)If my story is right, what would we expect to see in dopamine-measuring experiments?Imagine a rat running through a maze. Moment by moment, its array of predictors are getting dopamine supervisory signals about its various hormone levels, its heart rate, its expectation of drinking and eating and having a sore leg and freezing and tasting salt, and on and on. In short, we expect dopamine neurons to be bouncing up and down in all kinds of different ways.Thus, pretty much any instance where an experimenter has measured that a dopamine neuron is correlated with some behavioral variable, it\u2019s probably consistent with my picture too.Here are a couple examples:There are dopamine neurons that burst for salient stimuli like unexpected flashes of light (ref). Can I explain that? Sure, no problem! I say: they could be supervisory signals saying \u201cthis would have been a good time to orient\u201d, or \u201cto flinch\u201d, or \u201cto raise your heart rate\u201d, etc.There are dopamine neurons that correlate with the velocity with which a mouse is running on a treadmill-ball (ref). Can I explain that? Sure, no problem! I say: they could be supervisory signals saying \u201cexpect sore muscles\u201d, or \u201cexpect cortisol\u201d, or \u201cexpect high heart rate\u201d, etc.Here\u2019s another data point which seems reassuringly consistent with my picture. A few dopamine neurons burst when aversive things happen (ref). Four of the five regions[7]\u00a0in which such neurons can be found (according to the linked paper) are right where I expect that array of short-term predictors to be\u2014namely, the cortex-like and striatum-like layers of amygdala, and medial prefrontal cortex (mPFC), and the ventromedial shell of the nucleus accumbens, which is (at least roughly?) the striatum stop of the mPFC cortico-basal ganglia-thalamo-cortical loops. This is exactly what I expect in my picture. For example, if a mouse gets shocked, then a \u201cshould-I-freeze-now\u201d predictor would get a supervisory signal saying \u201cYes, you should have been freezing\u201d.5.5.6.1 Aside: Distributional predictor outputsI didn\u2019t talk about it in the last post, but short-term predictors have hyperparameters in their learning algorithms, two of which are \u201chow strongly to update upon a false-positive (overshoot) error\u201d, and \u201chow strongly to update upon a false-negative (undershoot) error\u201d. As the ratio of these two hyperparameters varies from 0 to \u221e, the resulting predictor behavior varies from \u201cfire the output if there\u2019s even the faintest chance that the supervisor will fire\u201d to \u201cnever fire the output unless it\u2019s all but certain that the supervisor will fire\u201d.Therefore, if we have many predictors, each with a different ratio of those hyperparameters, then we can (at least approximately) output a probability distribution for the prediction, rather than a point estimate.A recent set of experiments from DeepMind and collaborators found evidence (based on measurements of dopamine neurons) that the brain does in fact use this trick, at least for reward prediction.I speculate that it may use the same trick for the other long-term predictors too\u2014e.g. maybe the predictions of arm pain and cortisol and goosebumps etc. are all in the form of ensembles of long-term predictors that each sample a probability distribution.I bring this up, first, because it\u2019s another example where dopamine neurons are behaving in a way that seems pleasingly consistent with my worldview, and second, because it\u2019s plausibly useful for AGI safety\u2014and thus I was looking for an excuse to bring it up anyway!5.5.7 SummaryAnyway, as usual I don\u2019t pretend to have smoking-gun proof of my hypothesis (i.e. that the brain has an array of long-term predictors involving telencephalon-brainstem loops). But considering the evidence in this and previous subsections, I wind up feeling strongly that I\u2019m broadly on the right track. I\u2019m happy to discuss more in the comments. Otherwise, onward to the next post, where we will finally put everything together into a big picture of how I think motivation and decision-making work in the brain!^\u2018Horizontal\u2019 neuroanatomy versus \u2018vertical\u2019 neuroanatomy is my idiosyncratic terminology, but I\u2019m hoping it\u2019s intuitive. If you imagine stretching out the cortex into a sheet, oriented horizontally, then the \u2018vertical\u2019 neuroanatomy would include e.g. the interconnections between cortical and subcortical structures, and the \u2018horizontal\u2019 neuroanatomy would include e.g. the different roles played by different parts of the cortex. See also the table in Section 5.4.1.^To be clear, in reality, there probably isn\u2019t a discrete all-or-nothing 2-way switch here. There could be a \u201cweighted average\u201d setting, for example. Remember, this whole discussion is just a pedagogical \u201ctoy model\u201d; I expect that reality is more complicated in various respects.^I note that I\u2019m just running through this algorithm in my head; I haven\u2019t simulated it. I\u2019m optimistic that I didn\u2019t majorly screw up, i.e. that everything I\u2019m saying about the algorithm is qualitatively true, or at least can be qualitatively true with appropriate parameter settings and perhaps other minor tweaks.\u00a0^Examples of using the terminology \u201cTD learning\u201d for something which is not related to RL reward functions include \u201cTD networks\u201d, and the Successor Representations literature (example), or this paper, etc.^The classic attempt to reconcile \u201cvisceromotor\u201d and \u201cmotivation\u201d pictures of mPFC is Antonio Damasio\u2019s \u201csomatic marker hypothesis\u201d. My discussion here has some similarities and some differences from the somatic marker hypothesis. I won\u2019t get into that; it\u2019s off-topic.^As in the previous post, when I say that \u201cdopamine carries the supervisory signal\u201d, I\u2019m open to the possibility that dopamine is actually a closely-related signal like the error signal, or the negative error signal, or the negative supervisory signal. It really doesn\u2019t matter for present purposes.^The fifth area where that paper found dopamine neurons bursting under aversive circumstances, namely the tail of the striatum, has a different explanation I think\u2014see here.", "title": "[Intro to brain-like-AGI safety] 5. The \u201clong-term predictor\u201d, and TD learning", "karma": 14, "af": true, "userid": "vRcer5FTqagjMkcDz", "id": "F759WQ8iKjqBncDki"}, {"text": "If there\u2019s one lesson that I learned from almost every single smart person I know, it\u2019s:The unreasonable effectiveness of doing, then thinking; vs thinking too much before doingThis is not something I\u2019ve been able to convince anyone of, at least not with a rational argument. I might tell people things like:Stop ruminating about changing jobs, make accounts at x/y/z today, and fill them in minimally... yes, like, in 20 minutes, yes, it\u2019s fine if they are not perfect. Then apply to 100 jobs, no, no, don\u2019t spend 1 hour ruminating over each one, just write a 5-minute intention letter, you\u2019ll have like a 5% reply rate anyway, just focus on the ones that send interview... I don\u2019t mean like that, don\u2019t ruminate for hours about every interview!OrYeah, it\u2019s a super complex piece of kit you need to build. But like, you\u2019ve given me a use-case, you explained one of its components, just write a test and see where it goes. Yeah, I understand the test will fail, I understand it won\u2019t even compile, whatever, just write it. Or if you don\u2019t want to do that write the component you described to me, it can be broken, doesn\u2019t matter. Or heck, find a similar~ish project and just copy-paste it and start cutting and remaking. Doesn\u2019t matter that you\u2019re starting from a Reddit clone and you want to build genetics testing CRM, it can still serve as a starting-off point.OrYeap, getting into a new field is hard, but whatever, fake it til you make it. Buy the right books, follow the right people on Twitter, read the right blogs, build something that seems like it suits that field. Yes, the books will be wrong, you\u2019ll be following dumb people that every insider actually derides, the blogs will be off-topic, you\u2019ll build something shamefully bad. Doesn\u2019t matter, just do it.And I can\u2019t for the life of me convince people about doing these things because they seem \u201crash\u201d... and I keep thinking (therefore I know I exist): \u201cwhat the fuck are they thinking\u201d (therefore I know they exist as a separate entity), \u201cwhy is this rash, you\u2019re not trying to spear-hunt a fucking bear, you're writing some code in the privacy of your own home, the place you masturbate in, surely the shame of failing miserably can\u2019t be that bad\u201d.Anyway, I\u2019m not going to try to convince anyone to do then think. I can\u2019t prove nor even properly conceptualize why you should start doing that. But I want to try and at least think about a few reasons why doing before thinking, or at least doing something that seems like doing before thinking, is so unreasonably ineffective.Something Something Subconscious ImmersionYou all know the drill here.Brilliant scientist thinks a lot about a problem, gathers all the data that doesn\u2019t make sense and all the data that already makes sense and needs to keep making sense, struggles for a few days and comes up with nothing... then, he wakes up from a dream drooling inspiration, runs to his notebook in a trance and figures out relativity/microscopy/the periodic table/DNA structure.If we assume that, at any given time, a large part of brain networks are inactive or below the state of conscious control, then it\u2019s only a short inference away that the patterns of thought that\u2019d bring them online are best triggered by actually doing the tasks and letting said networks \u201cfigure out\u201d that they need to to help and how.Examples of this that everyone\u2019s noticed are learning to ride a bike, row, climb, surf, etc. Complex motor skills are learned with minimal conscious input, the conscious mind just decided to get on the bike as best it can, it receives instructions like \u201cpush with your leg in a sort of rotation motion\u201d... and 1 hour later the network that can actually handle bike riding has learned how to do it. It\u2019s important to note that riding a bike can\u2019t be first learned at a theoretical level.Rational Thought Is Best Past a Certain Data ThresholdAnyway, we\u2019re talking about things that are more complex than bike riding here, so I get that the analogy doesn\u2019t fit perfectly, but remember that bit about theory not being useful for beginners? I assume it\u2019s not the same for professionals.Professional bikers might spend hours a day discussing cycling theory, optimizing things besides bike riding like their doping regiment, training adjacent skills that they think are particularly important but hard to train \u201cwhile doing the thing\u201d. Professional sportspeople in general presumably do this quite a lot, though I expect it\u2019s much more common at complex sports (think football, skiing) than at simple ones (sprinting, javelin throwing).This might in part work because rational thought is good at polishing up ideas and behaviors but can\u2019t start from a position of insufficient data, or rather, is really dumb from a position of insufficient data.Let's think of a complex skill that people are really anxious about practicing, let\u2019s say picking up people or interviewing with an important manager. How\u2019s intuitively going to be better, someone that read a dozen interviewing-and-business/pickup-artistry books and watched videos and prepared bits to say... or someone that\u2019s already done it 4 or 5 times (even if they failed)? Almost certainly the latter.Theory and rationality seem to only help from a position where data is plentiful to think with, where there are real-world examples to think about, where you\u2019ve already \u201cfelt\u201d a system from the inside. Sure enough, people seem to have come up with a promising theory about mechanics much sooner than about waves, and for seemingly no fault of experimental devices, it\u2019s just that we \u201cdo mechanics\u201d in our day-to-day lives.Thinking Is Meant For Dangerous ScenariosLet\u2019s say thinking involves several disadvantages, it wastes time, it stresses you out, it makes you less likely to do anything in the long term (because doing anything is stressful and time-consuming), and so on.This would still make thinking advantageous if you have to go spear that bear since you want that kind of scenario to be a once-in-a-lifetime thing, so the extra stress and time is not only worthwhile for a slightly lower chance of death, it\u2019s the appropriate signal to send: \u201cThis is really hard and dangerous, don\u2019t do this ever again\u201d.Alas, we are usually in lack of life and death scenarios, and furthermore, we are trained to think too much in easy situations that should require no thought (e.g. solving math problems at school) in order to get a slight edge over someone else (e.g. during an exam).So it might be that doing than thinking is unreasonably effective because we trade-off on wasting time and producing anxiety that might make us never take useful action, to begin with.An important bit there is to remember that the network/self/you which thought the action is good is often not the network doing it. Maybe you did ponder the idea of getting a new job for a lot, decided on yes, and now the pesky you actually doing the job searching is thinking through every single job as though I\u2019d be a life-and-death decision whether or not to apply. This leads me to...We Often Think About Stuff That\u2019s Already DecidedSure, it\u2019s good to question the fundamentals of math every now and then, but maybe it\u2019s not best to do it as part of solving a mildly annoying but certainly-doable calculation. It\u2019s also worth thinking if you\u2019ve gone a bit too extreme with your dieting, but doing so in the middle of a diet is bound to always result in you rationalizing \u201cyes\u201d to be the answer in order to splurge on sweets.Thinking often leads us to quickly rethink at the \u201cmeta\u201d level where a decision was already made, potentially a very thoughtful and thorough decision we made a while ago.An important note here is that this should bias us against both thinking too much while doing something that\u2019s been decided upon and towards planning fewer things at the \u201cmeta\u201d level, leaving the doing for some other-self.We Can\u2019t Control Thought Through Anything But ActionEven though we have a lot of \u201cmental power\u201d available at any given moment, most of what we do with it feels \u201cunelected\u201d, we lack understanding about why conscious thoughts flow the way they do.But a pretty reliable way to control thought is to put yourself into a situation where thought must take a certain pattern.If you jump through a small hole in the roof of a bear\u2019s den, wooden spear in hand, barely not awaking the fiend from hibernation, you\u2019ll probably have more optimal thoughts about bear-killing than if you tried to \u201cimagine\u201d the scene. But the trade-off is risk. Similarly, with social situations, risks of saying the wrong thing or grooming the wrong monkey could have been rather high. So we grew to love \u201cimagining\u201d and be afraid of action.What we think of when not acting will be a function of what seems \u201cmost important\u201d, thus many people end up ruminating about things such as politics, socializing and phobias. What we think of when acting will be the process itself, thus shortcutting the prioritization process.None of this is a conclusive answer to the question, much less so a proof, but these are some of the intuitions I have about it.As an aside, I conceptualize all of these intuitions while writing, maybe there were already formed in my subconscious, maybe they were created as a result of the imagined constraint of having to write this article, maybe they are all bad because I didn\u2019t spend more than 30 minutes consciously ruminating on the subject, or maybe they are as good as the answers of someone that meditated upon the question his entire life... If only I could know for sure.", "title": "Do, Then Think", "karma": 18, "af": false, "userid": "9oS3amiCSSYKgCWs5", "id": "aPFuxpGEevyGgMwJ2"}, {"text": "This is cross-posted from my blog and is written more for a general audience rather than LessWrong people who will be more familiar with some of the relevant concepts.Robert McIntrye is an all-around smart and interesting dude, and he\u2019s the CEO of Nectome, a startup devoted to preserving the human brain.Last year, I wrote a summary of Robert\u2019s brain preservation talk at the Long Now Foundation:https://www.lesswrong.com/posts/N7j4xHkyjKbimmF6A/notes-on-robert-mcintyre-s-brain-preservation-talk-at-the-1He later read it and reached out to me about discussing our thoughts. I asked if I could record it as an interview and here we are!We dive into topics such as brain preservation, personal identity, life extension, AGI, Rick and Morty, and why he\u2019s worried about the current state of cryonics.Our interview went for over three hours and we weren\u2019t anywhere close to exhausting our conversation (but we did exhaust my bladder capacity).A lot of time was spent on personal identity and the teletransportation paradox.We talked a lot about things we don\u2019t actually disagree on, but I\u2019m glad we did because it will help clear up confusion for listeners who aren\u2019t on the same page.The crux of our actual disagreement is as follows: I wouldn\u2019t destructively copy myself, as in the case of the teletransportation paradox, and he would.If you\u2019ve read the Three Buckets, you know that I care about my own utility and the utility of my loved ones.Imagine someone made a perfect copy of me.If my copy\u2019s hand was pricked, he would feel the pain but I wouldn\u2019t. If that copy was eating a double-double animal style from In-N-Out, he would be enjoying the taste of that goodness but I wouldn\u2019t be.While I\u2019d prefer for my copy to be feeling pleasure rather than pain, if I had to choose, I\u2019d prefer to be the one eating In-N-Out and not getting his hand pricked.It\u2019s not that I don\u2019t care if my copy is enjoying life. It\u2019s that I don\u2019t care as much because I\u2019m not experiencing it. This seems intuitive and obvious to me.Personal identity is a messy concept. We didn\u2019t evolve for our intuitions to handle complicated and messy scenarios such as being perfectly copied. There was no transporter to Mars in the ancestral environment.I don\u2019t want to be destructively uploaded because \u201cI\u201d (which, yes, is a messy term) would not be experiencing anything anymore. Yes, in some sense \u201cI\u201d would still exist because my memories would still be in a different copy. But that copy is the one enjoying Mars, not me. And I care about the John who is experiencing my qualia more than another John.Robert values his memories surviving and the algorithm that is his consciousness running somewhere, so he is fine with his current self being terminated as long as other copies are around with his memories.Do you agree with Robert? Would you be fine with having copies of yourself and then destroying your current copy?Let me know your thoughts, what questions I should ask in the future, and who I should talk to next in the comments below! Thanks!https://youtu.be/Lnk6bASp544Timestamps below with links to see what topics we covered and where:00:00:00Interview with Nectome CEO Robert McIntyre00:00:41Covid, moving his lab, and his new human brain banking project00:01:54Recommendations for spots in San Francisco00:02:23Danny Hillis, Connection Machine, and Jurassic Park00:03:16Human brain bank - storing brains donated to science00:03:29Preservation methods: fixation vs fixation + cryoprotection00:04:57Why does it take so long to preserve brains? How does brain donation work?00:11:14Why there may not be a good window in most cases to preserve a brain00:15:50Is it hard working outside the Overton Window?00:16:45His thoughts on the falling out with MIT00:18:25Most people don\u2019t really understand they\u2019re actually going to die00:19:48We could have done similar work to what he\u2019s doing in the 1960sWe can act now, we don\u2019t have to solve all the problemsWhen do you know enough to make an argument to take action?00:22:58How well can you preserve a brain under ideal circumstances?00:23:54Interesting surgery and implications for preservation00:28:10Why should we be worried about current cryonics protocols?00:30:07What should places like Alcor do?00:34:04What do we know well? What type of stuff is left to learn?00:36:20If you were in charge of Alcor, what would you have them do differently?00:39:11How much do you disagree with Alcor\u2019s object-level preservation techniques?00:41:05Are you signed up for cryonics now?00:41:47Have you talked to anyone at Alcor? Would they disagree with you?Is some information better than none?Information theory: levels of resolution/quality00:43:47What are your thoughts on personal identity?Teletransportation paradox00:50:42Bike Cuck ComicHow do we differ in our thoughts on personal identity?01:01:24Why not just record people with your phone?01:02:22What is the value of other people\u2019s memories/brains?01:05:32What are your thoughts on AI/AGI?01:08:11Are you concerned about AI safety?01:08:38Why don\u2019t you work on AI safety?01:09:19Can the most powerful AI get back lost information? Laplace\u2019s demon?01:10:30Do I believe a simulated person could be conscious?01:22:28If a copy was being tortured or you were, would you care the same way?01:32:08Do you have siblings?01:32:34How close does someone have to be to be the same?How valuable are your memories?Would you give up the last minute of your memory for $1000 dollars?How much of your memories do you have to lose to be severely injured?How much of your memories have to be lost to be considered dead?01:36:38How old are you?How important are different memories?What determines you being down to be terminated?01:42:12Another way of framing this is by causality01:44:37What do we disagree on?I wouldn\u2019t take the teleport to Mars and he would01:48:28I don\u2019t care about which \u201ccopy\u201d is \u201cfake\u201d or \u201cauthentic\u201d01:49:00What does being killed mean if a copy exists?01:53:40How should you relate to yourself?What are adaptive aesthetics?01:55:08In most scenarios wouldn\u2019t we be doing the same thing?Not being able to travel fast is a huge lossWe are born into twin prisons: gravity and time.01:56:20Would you get into the experience machine?01:57:58What pathways to flourishing are damaged and what ones are enabled?02:02:01You already live in a world divorced from the physical world02:02:46You wake up, someone tells you \u201cWhile you were asleep, you were teleported?\u201d02:04:55Do you think it\u2019s bad if people in cults kill themselves?02:07:25Philosophical problems will cease to be problems because everyone will do themSocial Media Questions Q and A02:08:22Giego Caleiro:How is he doing? When will we have brain preservation?Is most of the hurdle legal/regulatory or the science?02:09:49Chris Mcaulay:Is there a difference in therapeutic approaches between preservation and augmentation?02:10:52What do you think of Neuralink?What makes it worth getting an invasive brain implant?02:13:28Tony Fatica: What percentage of personality needs to be preserved to be \u201cthem\u201d?Would you trade a million dollars for 1 IQ point?How much money would you trade off to lose memories?02:15:13Beloved Aristocrat: Have you read Fall by Neal Stephenson?02:15:25What do you do in your spare time?YouTube video he mentioned:https://youtu.be/Pld8wTa16Jk02:16:10Do you identify as a rationalist?02:16:18What do you think of effective altruism?Most of the world is \u201cineffective selfishness\u201d02:16:48Peter Singer among others has written a lot about this.Reminds me of that scene from Schindler\u2019s List of how much should we sacrifice for others.https://youtu.be/fhA5GIx51KgTithing02:17:54What\u2019s your favorite movie of all time?What would a real alien visitor be like?02:19:40Who do you think is the smartest person of all time?02:21:30Who is the most capable human to solve problems?02:23:12Have you done psychedelics? Are you interested in psychedelics?Have I gotten any insights from psychedelics?Doors of Perception altered experiences model02:26:05His memories before his first wordsWhat did it take to invent language and spread it?How did writing develop?02:30:49Technological deflation02:31:51Where did you grow up?How was it being a kid?02:33:10Was it hard being gay in Kansas?2:33:58What did your parents do?Brothers who win the duck stamp competitionPaying his mom to teach him how to drawGenetic differences in skill02:38:41Montie Adkins: What about memory limits over hundreds of years and possible augmentation?02:43:13DJ Grossman: Knowing what you know about the current state of research in brain and memory science, what are your expectations of those preserved now?02:44:00Do you think we\u2019ll hit LEV (longevity escape velocity) in our lives?02:45:45When do you think we\u2019ll develop AGI?02:48:15Are you optimistic about the future?02:48:25Do you like Rick and Morty?02:49:38Are you familiar with the Qualia Research Institute?02:50:11Who are you inspired by?https://en.wikipedia.org/wiki/Patrick_WinstonI later realized he was talking about this professor who has a great talk about how to speak:https://youtu.be/Unzc731iCUY02:52:44Have you talked to your parents about preserving their brains?02:53:29If my parents died in five years, would you be ready to preserve their brains?02:54:20Back to continuity of consciousnessThinking in terms of objects02:59:26Why do you value your own memories over others?03:01:49The Torah is Not in Heaven03:04:46If there was a lifeboat or spaceship, you would still prefer yourself on there rather than a stranger, right?03:05:39Why do you care about preserving your own memories vs someone else\u2019s?03:07:02There will be a new type of bigotry for uploaded people.03:08:11The legal implications of uploaded copies03:10:12Why he hates the book \u201cThinking Clearly About Death\u201d03:12:21How important is something you can\u2019t perceive?03:12:55What is the difference between being destructively uploaded, and killing everyone if they wouldn\u2019t be around to suffer?03:22:13What if I put you in the transporter and I just kill you?03:27:15Where does this intuition come from?03:34:33For people that want to follow, what do you recommend?https://nectome.com/", "title": "Interview with Nectome CEO Robert McIntyre - Brain Preservation, Personal Identity, AGI, and More", "karma": 14, "af": false, "userid": "BCn7derWSwMZ6u2Kr", "id": "s2N75ksqK3uxz9LLy"}, {"text": "TL;DR: LessWrong now has similar features to Google Docs. Warning! Still rough around the edges. To enable the collaborative editor, you must check \"Opt into experimental features\" in your account settings and then press the green \"Share\" button that appears when editing your post.You can experiment with commenting and suggesting on this post with this link.It's been a loooong time coming[1]\u00a0but at last, we are ready to unveil collaborative editing features for the LessWrong text editor. These features will be familiar to those used to working in Google Docs:Multiple users can edit a document at onceFine-grained permissions for viewing/commenting/editing by link or usernameInline comments (only viewable while in edit mode)Making and accepting suggested editsAutomatic savingVersion history viewerSome advantages of using LW-Docs with collaborative editing:LW-Docs supports LaTeX, unlike Google Docs.If you use entirely LW-Docs, you won't have broken footnotes, unlike with copying from G-Docs.While writing your post, you'll know what the end result will look like (same font and line width) which helps you optimize paragraphs and layout for looking good when published.You can continue to get inline feedback and suggestions on your post even after you've copied it over to LessWrong.These can then be seamlessly integrated into your live post.INLINE COMMENTS AND SUGGESTIONS ARE ONLY VIEWABLE IN EDIT MODE TO THOSE WITH PERMISSIONSThe sharing dialogHow to enable collaborative editing on your postStep 1: Opt in to experimental features in your account settings.Step 2: While editing your post, click the green \"Share\" button in the top right. Enable permissions for some other users. Boom! Your post is now in collaborative mode.Step 3: Users explicitly shared on your post will receive a notification. Send the url to anyone else you want to grant access.Step 4: When in collaborative mode, the text of your post is automatically saved. To view how your post will look when published, press the Preview button. To make the current state of your document live, press the Publish button.The collaborative editor allows you (and others) to continue editing and commenting on a post even once it's been published. Edits aren't automatically published to the live version. To update the published version to the current state of the document in editing, press \"Publish\".\u00a0Step 5: Leave comments and suggestions.To enable track changes, use the track changes button in the popup menu (third icon from the right) or set your Mode to Commenting in the header bar.The header bar. Being in Comment mode means you only suggest changes rather than actually make them.\u00a0\u00a0Step 6: Leave feedback about the feature!This feature is still under development. Any feedback from early-adopters is hugely helpful. Just leave a comment on this post or message us on Intercom.Warning! Rough around the edgesWe're releasing this feature in beta mode because it still requires a few more finishing touches and might be a little confusing to use in places. As above, any feedback is greatly appreciated.Why collaborative editing?When developing features, there's always a question of \"does anyone want this?\" In the case of collaborative editing features, there's good evidence of demand from Google Docs. There's a common workflow that goes: (1) write a draft in Google Docs, (2) invite some close friends or collaborators to give feedback, (3) incorporate feedback, (4) copy to LessWrong, (5) publish.Drawbacks of this workflow are (a) overhead of copying and reformatting the post, (b) enforcing a hard break between the feedback stage and the publication stage, (c) Google Docs does not support LaTeX, (d) valuable comments left in the feedback stage never get published to the wider world.By introducing collaborative editing to LessWrong, we address (a), (b), and (c). We haven't yet made it so in-line comments in editing mode can be published in the final version, but we'll look into ways to allow for that. I also expect that we'll add additional features[2]\u00a0to our editor that Google Docs doesn't have, such that users will benefit from the possibility of doing all their writing on LessWrong.Having collaborative editing features on LessWrong also lets us start to build programs that rely on easy ways to give people feedback on their drafts. For example, I'd like to run a writing and research workshop for students that involves peer and mentor feedback. With collaborative editing on LessWrong, that will now be much more convenient to do.What's your writing workflow?If you're an author, I'd love to hear how collaborative editing features do or don't help you with your workflow, or what you'd really like to see us build. Feel free to comment on this post or message us on Intercom.Thanks and good luck!\u00a0^It's been ~18 months since the first steps towards this were taken.^One feature I'm particularly excited about is \"link-searching\". In the same way that one can @-mention people on Facebook, I'd like to make it so you can easily link to post and wiki-tags by typing @ or # and then using a few letters to search for the resource you want to link to.", "title": "[Beta Feature] Google-Docs-like editing for LessWrong posts", "karma": 112, "af": false, "userid": "qgdGA4ZEyW7zNdK84", "id": "q6d4vtGwt5FE84wk7"}, {"text": "I got married today, to the particular fellow mentioned in my Turning 30 post. In a sort of 'inverse cat tax'[1]\u00a0for a sappy announcement, here's a mathematical model of whether you should expect to like your partner more than yourself. I don't mean this in a moralistic way ('thou shalt love thy parents'), tho that might be another post for another time, or necessarily in a utilitarian way (\"I would rather they get this ice cream than me\"), but as a matter of raw respect (\"I think they're a better person than I am, according to my values\").For simplicity's sake, let's consider everyone as having a 'stat' vector and a 'preferences' vector with the same dimensionality, and giving a candidate partner a 'score' based on the dot product of those two vectors. \u00a0We'll assume that all of the stats are universally good (no one ever prefers an uglier partner over a smarter one, tho they might not care about physical attractiveness much). The preferences vector we'll normalize to have unit magnitude (so it's just an angle in N-dimensional space, basically, defined as a point on the positive sector of the N-spherical shell). For reasons, I'll run simulations with the stat vector as 6 dimensions with 3d6 per stat, leading to a discrete distribution a bit like a truncated normal, with no correlation between the stats.[2]Let's start by considering the heterosexual version of the stable marriage problem, in which people partner up using the well-known Gale-Shapley algorithm, and a simulation with 1,000 each of randomly sampled men and women. Mating is highly assortative; a correlation between total stats of 85.7%, with 83.3% of people have an average total stat difference of less than 6 (the dimensionality).The interesting result is that 91% of people like their partner at least as much as they like themselves, with an average net satisfaction of 1.24. Note that we haven't baked in any correlation between one's stats and one's preferences, and so this result is, in some sense, not very surprising. The preferences are exerting pressure on the partner (thru who you can stably match with) and not exerting pressure on the self, and so you should expect that pressure to result in higher other-satisfaction.[3]So let's add an adjustment to the preferences with a scalable parameter corr, so that now it's the (renormalized) sum of the stat vector (times corr) and the previous preference vector (times abs(1 - corr)). As we smoothly vary this from 0 to 1, the average net satisfaction decreases to -0.2 (liking themselves more than their partner) and the fraction happier decreases to 21%.\u00a0While the change in net satisfaction is relatively smooth, the change in fraction happier looks much more sigmoidal, with the main drop between corr = 0.4 and corr = 0.8. The main change here is in self-satisfaction, which increases by about 5 points while other-satisfaction increases by only about 3 points.You can also imagine situations where people specifically want their complements, rather than their mirror. Negative correlations between your stats and your preferences seem unlikely; a more appropriate model seems to be something like relationship satisfaction being a function of the minimum stat between the two partners (or the minimum plus half the maximum, or so on).The 'marriage' situation with full bisexuality is typically called the stable roommate problem, solved with a similar algorithm. I'll leave it as an exercise for the reader how that impacts the results.[4]\u00a0Anyway, my sense is that when people talk about their 'better half', they're mostly being serious, and this is something that can easily be symmetric.^On Imgur, it's common for cat owners to end posts that collect images of use for some other reason with a picture of their cat, referred to as the 'cat tax'.^Of course in the real world, everything is correlated; not only is there g for intelligence, but GFP for personality, and wealth causes many material factors to be correlated, and so on. You could try to rationalize this by splitting out the 'natural' variables (like intelligence and wealth) into corrected variables (like intelligence and intelligence-adjusted wealth), but then it seems odd to have a uniformly random preference vector (as intelligence in the intelligence-adjusted model is more important than in the non-intelligence-adjusted model, given that some wealth-preference has now been moved over to intelligence). I currently don't expect that taking this into account will affect the analysis much (tho doing the analysis with univariate Gaussian stats leads to some odd effects with self-satisfaction, which I'm avoiding here to keep things simple).^Correlation between stats and self-satisfaction, of course, is high (0.69), because we insisted that the preferences all be positive, and so people with higher stats will like themselves more accordingly.^Naively, I would expect that everyone is more satisfied with their relationships (as they can sample from a wider pool). I think it's likely more assortative in terms of total stats, but it's a little unclear what will happen with the similarity (as corr increases) and what will happen to the crossover point of average net satisfaction (but I'd guess the 0 point is a bit to the right, with the 'increased satisfaction' effect swamping the 'when you try for people similar to you, you can get closer' effect).", "title": "How satisfied should you expect to be with your partner?", "karma": 66, "af": false, "userid": "fD4ATtTkdQJ4aSpGH", "id": "xggxWfyzZmnz7hydm"}, {"text": "Bay Rationalists Field Day is a bi-weekly-or-so social event where the bay area rats are being physically active outside.\u00a0This time we're doing our usual capture the flag game on the ~usual spot, but there's a twist! Apparently this field is also used by the local Jugger club for their practice. Jugger is somewhat like capture the flag, only with packing foam swords. Their event starts at 1pm and while I'm still waiting on the reply from their group's admin, last time they didn't mind at all us joining in. So if you always wanted to try fencing but with no training or risk of injury, this might just be your chance! We'll play capture the flag for two hours, and then those who want can join the Jugger players, and those who don't can keep on playing CtF on the Memorial glade.Location: The lawn just east of Memorial Glade @ Berkeley campus\u00a0Note: I'm pretty busy the two weekends after that, so the next event is likely going to be March 19, unless somebody wants to take up the organization.", "title": "Bay Rationalists Field Day", "karma": 3, "af": false, "userid": "TBjchmMYoMtTgWCGa", "id": "qcYt86a69CkQ64WBT"}, {"text": "This is a follow-up to last week's D&D.Sci scenario: if you intend to play that, and haven't done so yet, you should do so now before spoiling yourself.A web interactive to test your solution is available here. \u00a0This also presents the thrilling* conclusion to the story, with a wide variety** of skillfully-written* endings depending on your actions***!*Maybe. \u00a0 \u00a0**Five. \u00a0 \u00a0 \u00a0***And on your luck.Full generation code is also available here if you are interested, or you can read on and find out.RULESETAn SCP object has a Value and a Danger.When an object is successfully stolen, Marshall Carter & Dark security teams attempt to reduce its costs, while MC&D sales teams attempt to maximize revenue gained.\u00a0Revenue gained starts at a baseline of $2MM * (Value^2). \u00a0This is multiplied by (3+1d6)/6 depending on performance of sales team.Costs start at a baseline of $2MM * (Danger^2). \u00a0This is multiplied by (9-1d6)/6 depending on performance of security team.Profit is equal to Revenue minus Costs.Overall, an object whose Value equals its Danger will average a slight profit. \u00a0Objects with Value > Danger will tend to generate profits, while those with Danger > Value will tend to generate losses.The squaring of Value and Danger means that e.g. Value 6 Danger 5 is much more profitable than Value 1 Danger 0 (with a baseline of $72 MM revenue and $50 MM costs being much better than a baseline of $2 MM revenue and $0 costs).You have very good insight into the Danger of an object via the SCP Foundation's classifications:Any object with Danger 5+ is classified as Keter.Any object with Danger 3-4 is classified as Euclid.Any object with Danger 0-2 is classified as Safe.There was low-hanging-fruit available in preferentially targeting Safe objects (though this was not always optimal). \u00a0The more difficult problem was figuring out how to use tags to identify objects with high Value.However, there were also two Anomalies hidden in the dataset - SCPs that broke the usual rules. \u00a0If you fed the data into a machine learning program without sanity-checking it first (cough), these may have shot you in the foot.ANOMALIESTwo SCPs in the dataset were anomalous - they did not operate under the same rules as the rest of the data, and instead existed as horrible tricks to sabotage people who did not sanity-check their data.SCP-1182 is an infohazardous object. \u00a0All data pertaining to it is corrupted, taking on false values. \u00a0Rows for this object showed up with:A random other date.A random other SCP object's ID.A random location other than its actual location of Site 2.A random classification other than its actual classification of Keter (so Safe or Euclid at random).All flags set incorrectly (listing it as having every property except for 'infohazardous')If no team was sent, a random team will be listed as having been sent - if a team was sent, a random other team or 'none' will be listed.Since the object has never been stolen (in part because any team sent to retrieve it went to the wrong place) the correct value in the profit column is always 0 - this will be replaced with a random non-zero profit value.SCP-537 is a Very Loyal Robot Dog. \u00a0It imprints on its owner (currently Foundation Senior Researcher Valdez on Site 2). \u00a0When its owner calls to it, it returns to them - regardless of its current location, whether it is constrained, or even whether it has been disassembled or destroyed. \u00a0The SCP Foundation is aware of this ability, and uses it to their advantage - they conceal its precise ability, but ensure that potential thieves are aware of it. \u00a0MC&D has stolen it half a dozen times under different heads of Acquisitions, and every time it has returned itself (leaving MC&D with zero profit).DATASETThe remainder of the dataset was generated according to a consistent ruleset. \u00a0The central theme of this dataset was Bayesian inference.There are four possible Sources for anomalous objects:SCP SOURCESCREATORS (e.g. Dr. Wondertainment) produce anomalous objects to accomplish something. \u00a0These objects are designed to be useful, and while they may be dangerous the danger is a side-effect of a desired function.An SCP object produced by a Creator rolls 1d5 and 1d3. \u00a0The 1d5 result is its Danger. \u00a0The sum of both results is its Value.This means that SCP objects produced by Creators will always be valuable, and will be more valuable the more dangerous they are (since Value 7 Danger 5 is better than Value 3 Danger 1).SPACETIME SHENANIGANS (e.g. objects that have fallen through time portals from future or parallel universes) are valuable as mechanisms that can be worked with and sometimes even reverse-engineered.An SCP object resulting from Spacetime Shenanigans rolls 1d6 for its Danger, and has a constant Value of 5.This means that SCP objects resulting from Spacetime Shenanigans will usually be profitable, and more so the less dangerous they are.ANART OBJECTS are produced by 'anomalous artists' who create curiosities. \u00a0In most cases these are neither particularly dangerous nor particularly valuable - they're created by people who are trying to make something artistic, not something useful.An SCP object resulting from anartists rolls 12d6. \u00a0Its Value is equal to the number of 6s rolled and its Danger is equal to the number of 1s rolled.This means that SCP anart objects are usually not very valuable, though low-danger ones are usually slightly profitable.VILLAINS (e.g. the Disciples of the Scarlet King, the Church of the Broken God) are trying to use anomalous means to destroy the world/conquer the world/immanentize the eschaton. \u00a0These objects are designed to be dangerous.An SCP object produced by a Villain rolls 1d4 and 1d5. \u00a0The 1d4 result is its Value. \u00a0The sum of both results is its Danger.This means that SCP objects produced by Villains are never a good idea to pursue.Expected profit from an object based on source and classification:\u00a0CreatorSpacetimeAnartVillainSafe$24.0M$49.6M$9.8M-$5.2MEuclid$44.4M$31.3M-$10.9M-$15.9MKeter$61.8M-$1.7M-$44.7M-$58.5MYour overall goal, therefore, is to identify and pursue Creator-made objects (especially high-danger) and Spacetime-made objects (especially low-danger) while trying to avoid Anart and especially Villain-sourced objects.TAGSTags are not directly relevant to Value or Danger. \u00a0Instead, Tags are informative about Value and Danger by being informative about Source. \u00a0Different sources have different probabilities of exhibiting a given tag:TagCreatorsSpacetimeAnartistsVillainsHumanoid40%10%1%10%Infohazardous20%20%40%10%Location15%1%50%10%Organic50%1%10%15%Predatory15%5%5%30%Mechanical1%60%10%20%Mobile50%40%5%40%Replicating1%20%5%30%Virtual5%40%30%1%For a given source, tags are independent of one another (with the exception of Humanoid and Location, which are mutually exclusive).A full analysis of what tags imply what is quite deep. \u00a0A few sample things to point out:'Predatory' is generally a bad tag to see.'Organic' and 'Mechanical' are both good tags in isolation (with Organic tending to mean something that came from a Creator and Mechanical tending to mean something that came from Spacetime). \u00a0However, Organic and Mechanical together are quite bad: as Creators very rarely make Mechanical things and Spacetime very rarely produces Organic things, this tends to mean something that has both those traits came from a Villain.'Virtual' objects very rarely come from Villains, and usually come from either Spacetime or Anart. \u00a0This means that low-danger Virtual objects are usually pretty good bets.'Mobile' is strong evidence against Anart - this is good to see if you have e.g. a Virtual object, which you're already fairly sure is either Spacetime or Anart. \u00a0It's pretty much meaningless if you're trying to figure out whether a Predatory Organic Humanoid (already extremely unlikely to be Anart) is from a Villain or a Creator.Tags also had a secondary effect on retrieval teams:Location SCPs cannot be stolen by Infiltration teams (your infiltration teams are not Carmen Sandiego).Humanoid SCPs cannot be stolen by Legal teams (even if MC&D could convince a court to assign them title to a person and compel the Foundation to hand them over, the PR risk would be too high, and your lawyers will not do it).Virtual SCPs cannot be stolen by Paramilitary teams (even if you run off with the computers, Foundation data security and backup tech is very reliable and your paramilitary operatives are not trained to work around it).Your predecessors knew the first two of these things, and did not send Infiltration teams after Locations or Legal teams after Humanoids. \u00a0They did not know the third, and frequently sent Paramilitary teams to fail in retrieving Virtual SCPs.EDITED TO ADD: Aside from their dependence on tags, your predecessors' actions were almost entirely random, sending 2 teams of each type plus 1-5 additional random teams (1d3 at first, up to 1d4 in 1950 and 1d5 in 2000) to target random SCP objects.LOCATIONSThere were six sites in-game. \u00a0Site 1 is Foundation overall administration and headquarters, SCP objects are not stored there. \u00a0There is no Site 5. \u00a0You are not cleared to know what happened to Site 5. \u00a0Do not enquire further. \u00a0Be vg jvyy unccra gb lbh.These had a relationship with Sources:Creators operate mostly in Europe and Asia (Site 4: Moscow and Site 6: Geneva). \u00a0Creator-made objects are twice as likely to appear here as in other areas.The Anart scene is most active around Site 2: Washington D.C. and Site 7: Shanghai. \u00a0Anart-made objects are twice as likely to appear here as in other areas.Spacetime shenanigans tend to happen most near the equatorial portal zone, near Site 3: Kinshasa and Site 8: Tehran. \u00a0Spacetime-sourced objects are twice as likely to appear here as in other areas.They also had a modest effect on retrieval teams:Paramilitary teams work best in regions lacking a strong and protective centralized governance with an effective monopoly on force. \u00a0They have (when dealing with non-Virtual objects) a 90% success rate near Kinshasa and Tehran, a 60% success rate the rest of the time.Legal teams work best in regions with a governance that is maximally responsive to MC&D's influence. \u00a0They have (when dealing with non-Humanoid objects) a 90% success rate near Shanghai and Moscow, a 60% success rate the rest of the time.Infiltration teams have cultivated contacts in the areas where other teams function less well. \u00a0They have (when dealing with non-Location objects) a 90% success rate near Geneva and Washington D.C, a 60% success rate the rest of the time.STRATEGYWith a theoretical perfect understanding of how the system works, optimal strategy is to:Use Bayesian inference on tags, locations, and classification to predict which sources are likely for any given object.Attempt to steal preferentially high-danger objects produced by Creators, and low-danger objects produced by Spacetime. \u00a0(If attempting to minimize profit, instead attempt to steal high-danger objects produced by Villains.)Send the correct team type to each object.The SCP objects you had access to, their source probabilities with evidence taken into account, and the resulting expected profits, were:SCP (Classification)CreatorsVillainsAnartistsSpacetimeExpected Profit if Stolen ($MM)SCP-2797 (Keter)93.44%6.39%0.02%0.14%54.0SCP-3273 (Safe)0.04%0.05%0.70%99.21%49.3SCP-4449 (Safe)0.03%0.04%8.21%91.72%46.3SCP-537 (Safe)1.01%5.50%3.94%89.56%44.8SCP-3936 (Euclid)98.54%0.92%0.49%0.06%43.5SCP-3440 (Safe)0.32%2.61%11.87%85.20%43.4SCP-4834 (Euclid)97.18%2.71%0.04%0.08%42.7SCP-4004 (Keter)82.10%14.98%2.89%0.03%40.7SCP-3668 (Keter)78.71%15.26%0.05%5.99%39.6SCP-4026 (Euclid)89.43%7.07%0.08%3.42%39.6SCP-2720 (Keter)80.97%18.69%0.15%0.19%39.1SCP-5117 (Euclid)87.74%7.74%3.98%0.54%37.4SCP-2719 (Euclid)81.02%12.06%6.55%0.37%33.4SCP-5087 (Euclid)76.73%4.28%18.97%0.01%31.3SCP-2325 (Euclid)74.73%15.76%8.75%0.76%29.9SCP-4957 (Safe)66.79%7.34%5.92%19.94%26.1SCP-1282 (Euclid)0.10%0.17%16.84%82.89%24.1SCP-2628 (Keter)66.12%28.84%0.01%5.03%23.9SCP-3212 (Safe)96.87%1.88%0.95%0.29%23.4SCP-2253 (Safe)89.18%6.70%0.11%4.02%23.1SCP-1970 (Euclid)61.35%18.26%19.82%0.56%22.3SCP-1720 (Safe)15.41%5.08%51.90%27.61%22.2SCP-4931 (Euclid)57.88%35.13%1.76%5.23%21.5SCP-4027 (Euclid)1.66%23.07%0.70%74.57%20.3SCP-3339 (Safe)68.94%4.01%25.80%1.25%19.5SCP-3597 (Safe)30.33%5.00%51.08%13.58%18.8SCP-4271 (Safe)30.33%5.00%51.08%13.58%18.8SCP-3699 (Safe)63.14%0.69%36.16%9.627278703124244e-0518.7SCP-3850 (Safe)63.14%0.69%36.16%9.627278703124244e-0518.7SCP-2942 (Euclid)0.24%3.25%29.17%67.35%17.5SCP-4390 (Safe)1.49%0.09%81.55%16.87%16.7SCP-1466 (Euclid)41.30%42.79%1.69%14.23%15.8SCP-4709 (Safe)11.16%3.68%75.17%9.99%14.8SCP-5136 (Safe)24.60%0.24%75.15%0.02%13.3SCP-4625 (Safe)22.76%6.84%69.99%0.41%12.2SCP-2122 (Safe)2.32%0.03%97.21%0.44%10.3SCP-4370 (Safe)0.22%2.348960832571748e-0599.69%0.09%9.9SCP-3656 (Keter)45.94%40.07%0.02%13.97%4.7SCP-2883 (Euclid)9.60%7.20%74.47%8.73%-2.3SCP-4565 (Keter)0.46%1.42%0.17%97.95%-2.3SCP-4579 (Keter)5.63%6.71%1.78%85.87%-2.7SCP-4550 (Euclid)14.10%1.98%83.70%0.22%-3.1SCP-1785 (Euclid)3.99%1.12%94.76%0.12%-8.7SCP-4222 (Keter)3.16%5.64%18.99%72.21%-11.1SCP-2699 (Euclid)1.08%94.06%1.22%3.63%-13.5SCP-4412 (Keter)0.20%29.98%0.04%69.78%-18.6SCP-2898 (Keter)0.05%45.01%1.2563922948750266e-0554.94%-27.2SCP-4424 (Keter)2.21%47.78%0.02%49.98%-27.4SCP-2964 (Keter)1.48%48.01%0.28%50.23%-28.1SCP-2603 (Keter)14.84%69.03%2.74%13.39%-32.6SCP-5058 (Keter)14.84%69.03%2.74%13.39%-32.6SCP-3781 (Keter)11.84%73.45%0.46%14.25%-36.1SCP-2626 (Keter)11.48%86.43%0.03%2.06%-43.5SCP-1838 (Keter)10.16%86.05%3.42%0.37%-45.6SCP-2116 (Keter)0.16%87.03%0.38%12.44%-51.2SCP-4036 (Keter)4.11%92.78%0.90%2.21%-52.2SCP-3577 (Keter)0.04%88.98%0.12%10.86%-52.2SCP-2178 (Keter)0.23%88.86%0.87%10.05%-52.4SCP-3279 (Keter)3.887982652533503e-0592.26%0.05%7.69%-54.1SCP-4654 (Keter)1.27%94.03%3.74%0.96%-55.9\u00a0While not all objects are classifiable, in many cases we can be confident about what source an object came from. \u00a0The most profitable object in expectation if stolen, SCP-2797, is a Keter-class object: but its tags are innocent enough that we can map it to a >93% chance of coming from a Creator, and only a <7% chance of coming from a Villain.Once good objects are identified, we want to send the optimal teams. \u00a0The best targets are those where we can get a 90% success rate: SCP-2797 is not actually our best target, as it is located in Shanghai (where we would ordinarily want to send a Legal team for a 90% success rate), but is Humanoid (so we need to send a different team and accept a 60% success rate). \u00a0It's still in our top 9 targets, though.\u00a0One example of an optimal strategy to maximize profit is to send:Infiltration teams to retrieve SCP-3668, SCP-2719 and SCP-4449Legal teams to retrieve SCP-4004, SCP-5117 and SCP-3273Paramilitary teams to retrieve SCP-3440, SCP-2797 and SCP-3936.\u00a0One example of an optimal strategy to minimize profit is to send:Infiltration teams to retrieve SCP-3781, SCP-2603 and SCP-4036Legal teams to retrieve SCP-4654, SCP-3279 and SCP-2178Paramilitary teams to retrieve SCP-3577, SCP-1838 and SCP-2116.\u00a0LEADERBOARDPlayerExpected ProfitOptimal Play (max)$291.0 MMGuySrinivasan (max)$169.5 MMMeasure (max)$155.9 MMYonge$151.4 MMabstractapplic$146.9 MMPablo Repetto$143.2 MMRandom Play (Safe SCPs only)$104.1 MMEntirely Random Play$15.0 MMRandom Play (Keter SCPs only)-$113.6 MMGuySrinivasan (min)-$290.3 MMMeasure (min)-$323.9 MMOptimal Play (min)-$358.0 MMIf you're interested in looking in more detail, you can add lines like the following into the code and run it:print('\\nEvaluating max payoff plan:')evaluate_strategy( myWorld, infil=[ 3668, 2719, 4449 ], legal=[ 4004, 5117, 3273 ], paramil=[ 3440, 2797, 3936 ] )Most players pursuing high profits avoided Keter objects. \u00a0GuySrinivasan's max-payoff plan (the most successful one) pursued 5 Safe, 4 Euclid and 0 Keter objects. \u00a0Most extremely, abstractapplic and Pablo Repetto pursued 8 Safe, 1 Euclid and 0 Keter.While this approach was less risky if you couldn't distinguish good Keter objects from bad ones, it was not the highest-payoff approach: optimal play in fact pursued 3 Safe, 3 Euclid and 3 Keter objects (because the payoff from Keter Creator objects is the highest available, and several Keter SCPs can be fairly reliably identified as coming from Creators).\u00a0Nevertheless, I support players who made this decision. \u00a0It's valuable to know what parts of a problem you can optimize at your current level of understanding and what parts to leave alone. \u00a0 If you don't think you can distinguish good from bad objects at higher danger levels, trying to do that just risks shooting yourself in the foot.FEEDBACK REQUESTAs usual, I'm interested in feedback. \u00a0If you played the scenario, what did you like and what did you not like? \u00a0If you might have played but in the end did not, what drove you away? \u00a0Is the timeline too long/too short/just right? \u00a0Is the underlying data structure too complicated to approach? \u00a0Or too simple to feel realistic? \u00a0Or both at once?Thanks again to simon for the scenario idea (although he seems to have missed the scenario itself), and to abstractapplic for feedback on a draft, and thank you all for playing!", "title": "D&D.SCP: Anomalous Acquisitions Evaluation & Ruleset", "karma": 30, "af": false, "userid": "MXMgh6GogHnuMqAFf", "id": "eL9AYttaa9PEwmS9X"}, {"text": "I recently heard about transcranial direct current stimulation as a technology for improving memory and attention, and I thought it sounded like a great area to attempt some munchkin activity.However, it seems like the science isn't very clear.Does anyone have a more informed perspective on the efficacy behind it?", "title": "What's the status of TDCS for improving intelligence?", "karma": 2, "af": false, "userid": "wnmaorA6WDL6RoeKQ", "id": "CwWmfDm9SBQRptNyB"}, {"text": "Click lower right to download or find on Apple Podcasts, Spotify, Stitcher, etc.\n\n\nI have very detailed opinions on lots of topics. I sometimes get asked how I do this, which might just be people making fun of me, but I choose to interpret it as a real question, and I\u2019m going to sketch an answer here. \n\n\nYou can think of this as a sort of sequel to Minimal-Trust Investigations. That piece talked about how investigating things in depth can be valuable; this piece will try to give a sense of how to get an in-depth investigation off the ground, going from \u201cI\u2019ve never heard of this topic before\u201d to \u201cLet me tell you all my thoughts on that.\u201d\n\n\nThe rough basic idea is that I organize my learning around writing rather than reading. This doesn\u2019t mean I don\u2019t read - just that the reading is always in service of the writing. \n\n\nHere\u2019s an outline:\n\n\n\n    \n       \n    \n    \n   Step 1\n   \n   Pick a topic\n   \n  \n  \n   Step 2\n   \n   Read and/or discuss with others (a bit)\n   \n  \n  \n   Step 3\n   \n   Explain and defend my current, incredibly premature hypothesis, in writing (or conversation)\n   \n  \n  \n   Step 4\n   \n   Find and list weaknesses in my case\n   \n  \n  \n   Step 5\n   \n   Pick a subquestion and do more reading/discussing\n   \n  \n  \n   Step 6\n   \n   Revise my claim / switch sides\n   \n  \n  \n   Step 7\n   \n   Repeat steps 3-6 a bunch\n   \n  \n  \n   Step 8\n   \n   Get feedback on a draft from others, and use this to keep repeating steps 3-6\n   \n  \n\n\n\n    \nThe \u201ctraditionally\u201d hard parts of this process are steps 4 and 6: spotting weaknesses in arguments, trying to resist the temptation to \u201cstick to my guns\u201d when my original hypothesis isn\u2019t looking so good, etc. \n\n\nBut step 3 is a different kind of challenge: trying to \u201calways have a hypothesis\u201d and re-articulating it whenever it changes. By doing this, I try to continually focus my reading on the goal of forming a bottom-line view, rather than just \u201cgathering information.\u201d I think this makes my investigations more focused and directed, and the results easier to retain. I consider this approach to be probably the single biggest difference-maker between \"reading a ton about lots of things, but retaining little\" and \"efficiently developing a set of views on key topics and retaining the reasoning behind them.\"\n\nBelow I'll give more detail on each step, then some brief notes (to be expanded on later) on why this process is challenging.\nMy process for learning by writing\n\nStep 1: pick a topic. First, I decide what I want to form an opinion about. My basic approach here is: \u201cFind claims that are important if true, and might be true.\u201d \n\n\nThis doesn\u2019t take creativity. We live in an ocean of takes, pundits, advocates, etc. I usually cheat by paying special attention to claims by people who seem particularly smart, interesting, unconventionally minded (not repeating the same stuff I hear everywhere), and interested in the things I\u2019m interested in (such as the long-run future of humanity). \n\n\nBut I also tend to be at least curious about any claim that is both \u201cimportant if true\u201d and \u201cnot obviously wrong according to some concrete reason I can voice,\u201d even if it\u2019s coming from a very random source (Youtube commenter, whatever).\n\n\nFor a concrete example throughout this piece, I\u2019ll use this hypothesis, which I examined pretty recently: \u201cHuman history is a story of life getting gradually, consistently better.\u201d\n\n\n(Other, more complicated examples are the Collapsing Civilizational Competence Hypothesis; the Most Important Century hypothesis; and my attempt to summarize history in one table.)\n\n\nStep 2: read and/or discuss (a bit). I usually start by trying to read the most prominent 1-3 pieces that (a) defend the claim or (b) attack the claim or (c) set out to comprehensively review the evidence on both sides. I try to understand the major reasons they\u2019re giving for the side they come down on. I also chat about the topic with people who know more about it than I do, and who aren\u2019t too high-stakes to chat with.\n\n\nIn the example I\u2019m using, I read the relevant parts of Better Angels of our Nature and Enlightenment Now (focusing on claims about life getting better, and skipping discussion of \u201cwhy\u201d). I then looked for critiques of the books that specifically responded to the claims about life having gotten better (again putting aside the \u201cwhy\u201d).  This led mostly to claims about the peacefulness of hunter-gatherers.\n\n\nStep 3: explain and defend my current, incredibly premature hypothesis, in writing (or conversation). This is where my approach gets unusual - I form a hypothesis about whether the claim is true, LONG before I\u2019m \u201cqualified to have an opinion.\u201d The process looks less like \u201cRead and digest everything out there on the topic\u201d and more like \u201cRead the 1-3 most prominent pieces on each side, then go.\u201d\n\n\nI don\u2019t have an easy time explaining \u201chow\u201d I generate a hypothesis while knowing so little - it feels like I just always have a \u201cguess\u201d at the answer to some topic, whether or not I even want to (though it often takes me a lot of effort to articulate the guess in words). The main thing I have to say about the \u201chow\u201d is that it just doesn\u2019t matter: at this stage the hypothesis is more about setting the stage for more questions about investigation than about really trying to be right, so it seems sufficient to \u201cjust start rambling onto the page, and make any corrections/edits that my current state of knowledge already forces.\u201d\n\n\nFor this example, I noted down something along the lines of: \u201cLife has gotten better throughout history. The best data on this comes from the last few hundred years, because before that we just didn\u2019t keep many records. Sometimes people try to claim that the longest-ago, murkiest times were better, such as hunter-gatherer times, but there\u2019s no evidence for this - in fact, empirical evidence shows that hunter-gatherers were very violent - and we should assume that these early times fit on the same general trendline, which would mean they were quite bad. (Also, if you go even further back than hunter-gatherers, you get to apes, whose lives seem really horrible, so that seems to fit the trend as well.1)\u201d \n\n\nIt took real effort to disentangle the thoughts in my head to the point where I could write that, but I tried to focus on keeping things simple and not trying to get it perfect.\n\n\nAt this stage, this is not a nuanced, caveated, detailed or well-researched take. Instead, my approach is more like: \u201cTry to state what I think in a pretty strong, bold manner; defend it aggressively; list all of the best counterarguments, and shoot them down.\u201d This generally fails almost immediately.\n\n\nStep 4: find and list weaknesses in my case. My next step is to play devil\u2019s advocate against myself, such as by:\n\n\n\nLooking for people arguing things that contradict my working hypothesis, and looking for their strongest points.\n\nNoting claims I\u2019ve made with this property: \u201cI haven\u2019t really made an attempt to look comprehensively at the arguments on both sides of this, and if I did I might change my mind.\u201d\n\n\n(This summary obscures an ocean of variation. Having more existing knowledge about a general area, and more experience with investigations in general, can make someone much better at noticing things like this.)\n\n\nIn the example, my \u201cdevil\u2019s advocate\u201d points included:\n\n\n\nI\u2019m getting all of my \u201clife has gotten better\u201d charts from books that are potentially biased. I should do something to see whether there are other charts, excluded from those books, that tell the opposite story.\n\nFrom my brief skim, the \u201chunter-gatherers were violent\u201d claim looks right, and the critiques seem very hand-wavy and non-data-based. But I should probably read them more carefully and pull out their strongest arguments.\n\nEven if hunter-gatherers were violent, what about other aspects of their lives? Wikipedia seems to have a pretty rosy picture \u2026\n\n\nIn theory, I could swap Step 4 (listing things I\u2019d like to look into more) with Step 3 (writing what I think). That is, I could try to review both sides of every point comprehensively before forming my own view, which means a lot more reading before I start writing. \n\n\nI think many people try to do this, but in my experience at least, it\u2019s not the best way to go. \n\n\n\nDebates tend to be many-dimensional: for example, \u201cHas life gotten better?\u201d quickly breaks down into \u201cHas quality-of-life metric X gotten better over period Y?\u201d for a whole bunch of different X-Y pairs (plus other questions2). \n\nSo if my goal were \u201cUnderstand both sides of every possible sub-debate,\u201d I could be reading forever - for example, I might get embroiled in the debates and nuances around each different claim made about life getting better over the last few hundred years. \n\nBy writing early, I get a chance to make sure I\u2019ve written down the version of the claim I care most about, and make sure that any further investigation is focused on the things that matter most for changing my mind on this claim.  \n\n \nOnce I wrote down \u201cThere are a huge number of charts showing that life has gotten better over the last few hundred years,\u201d I could see that deep-diving any particular one of those charts wouldn\u2019t be the best use of time - compared to addressing the very weakest points in the claim I had written, by going back further in time to hunter-gatherer periods, or looking for entirely different collections of charts.\n\n\n\nStep 5: pick a subquestion and do more reading and/or discussing. One of the most important factors that determines whether these investigations go well (in the sense of teaching me a lot relatively quickly) is deciding which subquestions to \u201cdig into\u201d and which not to. As just noted, writing the hypothesis down early is key. \n\n\nI try to stay very focused on doing the reading (and/or low-stakes discussion) most likely to change the big-picture claim I\u2019m making. I rarely read a book or paper \u201conce from start to finish\u201d; instead I energetically skip around trying to find the parts most likely to give me a solid reason to change my mind, read them carefully and often multiple times, try to figure out what else I should be reading (whether this is \u201cother parts of the same document\u201d or \u201cacademic papers on topic X\u201d) to contextualize them, etc.\n\n\nStep 6: Revise my claim / switch sides. This is one of the trickiest parts - pausing Step 5 as soon as I have a modified (often still simplified, under-researched and wrong) hypothesis. It\u2019s hard to notice when my hypothesis changes, and hard to stay open to radical changes of direction (and I make no claim that I\u2019m as good at it as I could be).\n\n\nI often try radically flipping around my hypothesis, even if I haven\u2019t actually been convinced that it\u2019s wrong - sometimes when I\u2019m feeling iffy about arguing for one side, it\u2019s productive to just go ahead and try arguing for the other side. I tend to get further by noticing how I feel about the \"best arguments for both sides\" than by trying from the start to be even-handed. \n\n\nIn the example, I pretty quickly decided to try flipping my view around completely, and noted something like: \u201cA lot of people assume life has gotten better over time, but that\u2019s just the last few hundred years. In fact, our best guess is that hunter-gatherers were getting some really important things right, such as gender relations and mental health, that we still haven\u2019t caught up to after centuries of progress. Agriculture killed that, and we\u2019ve been slowly climbing out of a hole ever since. There should be tons more research on what hunter-gatherer societies are/were like, and whether we can replicate their key properties at scale today - this is a lot more promising than just continuing to push forward science and technology and modernity.\u201d\n\n\nThis completely contradicted my initial hypothesis. (I now think both are wrong.) \n\n\nThis sent me down a new line of research: constructing the best argument I could that life was better in hunter-gatherer times.\n\n\nStep 7: repeat steps 3-6 a bunch. I tried to gather the best evidence for hunter-gatherer life being good, and for it being bad, and zeroed in on gender relations and violence as particularly interesting, confusing debates; on both of these, I changed my hypothesis/headline several times. \n\n\nMy hypotheses became increasingly complex and detailed, as you can see from the final products: Pre-agriculture gender relations seem bad (which argues that gender relations for hunter-gatherers were/are far from Wikipedia\u2019s rosy picture, according to the best available evidence, though the evidence is far from conclusive, and it\u2019s especially unclear how pre-agriculture gender relations compare to today\u2019s) and Unraveling the evidence about violence among very early humans (which argues that hunter-gatherer violence was indeed high, but that - contra Better Angels - it probably got even worse after the development of agriculture, before declining at some pretty unknown point before today).\n\n\nI went through several cycles of \u201cI think I know what I really think and I\u2019m ready to write,\u201d followed by \u201cNo, having started writing, I\u2019m unsatisfied with my answer on this point and think a bit more investigation could change it.\u201d So I kept alternating between writing and reading, but was always reading with the aim of getting back to writing.\n\n\nI finally produced some full, opinionated drafts that seemed to me to be about the best I could do without a ton more work.\n\n\nAfter I had satisfied myself on these points, I popped back up from the \u201chunter-gatherer\u201d question to the original question of whether life has gotten better over time. I followed a similar process for investigating other subquestions, like \u201cIs the set of charts I\u2019ve found representative for the last few hundred years?\u201d and \u201cWhat about the period in between hunter-gatherer times and the last few hundred years?\u201d\n\n\nStep 8: add feedback from others into the loop. It takes me a long time to get to the point where I can no longer easily tear apart my own hypothesis. Once I do, I start seeking feedback from others - first just people I know who are likely to be helpful and interested in the topic, then experts and the public. This works the same basic way as Steps 4-7, but with others doing a lot of the \u201cnoticing weaknesses\u201d part (Step 4).\n\n\nWhen I publish, I am thinking of it more like \u201cI can\u2019t easily find more problems with this, so it\u2019s time to see whether others can\u201d than like \u201cThis is great and definitely right.\u201d\n\nI hope I haven\u2019t made this sound fun or easy\n\n\n\nSome things about this process that are hard, taxing, exhausting and a bit of a mental health gauntlet:\n\n\n\nI constantly have a feeling (after reading) like I know what I think and how to say it, then I start writing and immediately notice that I don\u2019t at all. I need to take a lot of breaks and try a lot of times to even \u201cwrite what I currently think,\u201d even when it\u2019s pretty simple and early.\n\nEvery subquestion is something I could spend a lifetime learning about, if I chose to. I need to constantly interrupt myself and ask, \u201cIs this a key point? Is this worth learning more about?\u201d or else I\u2019ll never finish.\n\nThere are infinite tough judgment calls about things like \u201cwhether to look into some important-seeming point, or just reframe my hypothesis such that I don\u2019t need to.\u201d Sometimes the latter is the answer (it feels like some debate is important, but if I really think about it, I realize the thing I most care about can be argued for without getting to the bottom of it); sometimes the former is (it feels like I can try to get around some debate, but actually, I can\u2019t really come to a reasonable conclusion without an exhausting deep dive). \n\nAt any given point, I know that if I were just better at things like \u201cnoticing which points are really crucial\u201d and \u201creformulating my hypothesis so that it\u2019s easier to defend while still important,\u201d I could probably do something twice as good in half the time \u2026 and I often realize after a massive deep dive that most of the time I spent wasn\u2019t necessary.\n\nBecause of these points, I have very little ability to predict when a project will be done; I am never confident that I\u2019m doing it as well as I could; and I\u2019m constantly interrupting myself to reflect on these things rather than getting into a flow.\n\nHalf the time, all of this work just ends up with me agreeing with conventional wisdom or \u201cthe experts\u201d anyway \u2026 so I\u2019ve just poured in work and gone through a million iterations of changing my mind, and any random person I talk to about it will just be like \u201cSo you decided X? Yeah X is just what I had already assumed.\u201d\n\nThe whole experience is a mix of writing, Googling, reading, skimming, and pressuring myself to be more efficient, which is very different and much more unpleasant compared to the experience of just reading. (Among other things, I can read in a nice location and be looking at a book or e-ink instead of a screen. Most of the work of an \u201cinvestigation\u201d is in front of a glowing screen and requires an Internet connection.)\n\n\nI\u2019ll write more about these challenges in a future post. I definitely recommend reading as a superior leisure activity, but for me at least, writing-centric work seems better for learning.\n\n\nI\u2019m really interested in comments from anyone who tries this sort of thing out and has things to share about how it goes!\n\n\n\n\n\n        \n        \n        \n        \n        \n\u00a0Comment/discuss\n\n\n\nFootnotes\n\n\n\n     I never ended up using this argument about apes. I think it\u2019s probably mostly right, but there\u2019s a whole can of worms with claims about loving, peaceful bonobos that I never quite got motivated to get to the bottom of. \u00a0\u21a9\n\n\n     Such as which metrics are most important.\u00a0\u21a9\n\n", "title": "Learning By Writing", "karma": 52, "af": false, "userid": "kdeMdATaSc2MZKmdH", "id": "ii4xtogen7AyYmN6B"}, {"text": "To explore better possibilities for nurturing new minds, and to care about the problem in the first place, it helps to remember what's wrong with what we do to new minds. John Taylor Gatto speaks about this from experience and insight:\nSeven Lessons Taught in School, 1991\n(If you're going to read the following, at least read the seven lessons (part I) of that essay.)\nHere's another list of harms caused by schooling.\n1. You aren't a mind, and don't bother trying to behave like one.\nChildren naturally attend to things until they're done with them:\nFrom Maria Montessori, My System of Education, 1915:\nIPFS pdf link\n\nA little girl, about three years of age, was deeply absorbed in the work of placing wooden blocks and cylinders in a frame for that purpose. The expression of her face was that of such intense attention, that it was almost a revelation to me. Never before had I seen a child look with such \"fixedness\" upon an object, and my conviction about the instability of attention which goes incessantly from one thing to another, a fact which is so characteristic in little children, made the phenomenon the more remarkable to me.\n\n\nI watched the child without interrupting her, and counted how many times she would do her work over and over. It seemed that she was never going to stop. As I saw that it would take a very long time, I took the little armchair on which she was sitting and placed child and chair on the big table. Hastily she put the frame across the chair, gathered blocks and cylinders in her lap, and continued her work undisturbed. I invited the other children to sing, but the little girl went on with her work and continued even after the singing had ceased. I counted forty-four different exercises which she made, and when she finally stopped, and did so absolutely independently from an exterior cause that could disturb her, she looked around with an expression of great satisfaction, as if she were awakening from a deep and restful sleep.\n\n\nThe impression I received from the observation was that of a discovery. The same phenomenon became very common among those children, and it was noticed in every school in every country where my system was introduced; therefore it can be considered as a constant reaction which takes place in connection with certain exterior conditions that can be well established. Each time a similar \"polarization\" of the attention occurred, the child began to transmute itself completely; it became calmer, more expressive, more intelligent, and evidenced extraordinary interior qualities, which recalled the phenomena of the highest mentality. When the phenomenon of polarization of the attention had occurred, all that was confused and drifting in the conscience of the child seemed to assume a form, the marvelous characters of which were reproduced in each individual.\n\nSchool usually steamrolls this process. The teacher has dominion over your attention; you have to either listen to what the teacher is saying, or work on the worksheets. Your location and (nominal) topic switches every hour. Even at home you do worksheets and projects. Your attention isn't you or yours; you're at war with your attention, it says no but your teacher--and implicitly your parents and college admissions and society, and \"you\"--insist.\nNot only are you not the authority on what is worth it for you to spend minutes, days, or years on; if you are even consulted, it's superficial (\"Which current event to do a report on? There are many options.\") and maybe only for Potemkin village purposes (\"We offer many electives.\"). You have to be a receptacle for what the teacher has brought to give you, because you don't have a perspective, you aren't an organized/organizing entity, and therefore can't be trusted to judge what is worthwhile. In the teacher-student relationship, someone has to defer to the other, and there's nothing in the student that the teacher could defer to. (I recall my elementary school art teacher looking at my painting, and then taking the brush out of my hand and painting new stuff on the canvas over what I'd already put there. In another class, I said \"Wait, [...]\" because I was confused about something, and the teacher interrupted my question to scold me for telling zer what to do.) Simply, the time in which you'd have created your mind is taken from you.\nThis creates learned helplessness about being absorbed in anything. It's like if you are trying to program, or write, but at arbitrary moments, someone Harrison Bergerons you until you forget what you were thinking. You learn not to get all worked up (absorbed in something, arranged ephemerally but suitably for the matter at hand, like a standing wave or a rough-and-ready scaffolding), because you'll just have to drop it in the middle anyway. You minimize the deepness and recursiveness of your questions, the length of your strides into the woods; the deeper the question, the more at risk you are of building towers of mental context and pumping neuroplasticity-juice into the relevant areas of your mind, and then having that plasticity act as random brain damage rather than successful reprogramming, when [the task that the plasticity was aimed at taking a compressive snapshot of how to perform/complete] is interrupted.\nThe self-organization of the child's mind is blocked at every turn, and the results are similar to trying to gestate a fetus in a small box.\nAlways smile. Refrain from looking out of the window.\n2. The world isn't yours.\n2.1. We own Space, and decide where you are.\nBeing forced to stay in a room with restricted range of motion (stay in your seat and stop fidgeting) is captivity, and captivity is harmful. Nuff said, one might have hoped.\nA common episode: a student asks \"Can I go to the bathroom?\", and the teacher responds \"You CAN, but you MAY not.\" or \"You CAN, did you mean 'May I?'?\". It's not just a stupid joke (what other stupid jokes stick out as vividly in memory as this one?), it's rubbing in your face that your range of motion is restricted by authority as if there were a concrete wall instead of a door, but you're not allowed to bring to the teacher's attention that they are restraining you so severely. Wow, what a doofus you are to think that Can and May are the same, how could you possibly have gotten those confused, I wonder?\nFrom The Quiet Rooms, Richards, Cohen, Chavis, 2019:\nTL;DR: Many thousands of times a year, kids in Illinois are locked up in a small room alone, sometimes for hours.\n\nIn Illinois, it's legal for school employees to seclude students in a separate space \u2014 to put them in \"isolated timeout\" \u2014 if the students pose a safety threat to themselves or others. Yet every school day, workers isolate children for reasons that violate the law[...] [snip]\n\n\nFor this investigation, ProPublica Illinois and the Tribune obtained and analyzed thousands of detailed records that state law requires schools to create whenever they use seclusion. The resulting database documents more than 20,000 incidents from the 2017-18 school year and through early December 2018. [snip]\n\n\n\"Please, please, please open the door. Please, I'll be good. Open the door and I'll be quiet.\"\n\n\n\"I'd rather die. You're torturing me.\"\n\nAlso, in Connecticut: 'Scream Rooms': Punishing Disabled Students in Isolation, Emily Richmond, 2012. Presumably in other states as well.\nAutistic 11 year old locked in a small room alone for hours in the UK: \"He had always loved school... but by the end of October in fifth class he hated it. I was dragging him into school every day.\"\nNo kidding.\nIt sounds not uncommon in the UK: Consequence Rooms. \"Then he got 22 hours in an isolation booth in one week and he was just an absolute mess. He came out at the end of the day and he didn't look well. His legs were shaking and he could hardly string a sentence together. He looked completely done in.\"\nMost schools aren't like that though, right? Well, the extremes, the ones that get reported, tell you about the hidden distribution and the attitudes that produced it. We own space, and decide where you ca... MAY go.\nFrom Children's Games in Street and Playground, Opie and Opie, 1969: IPFS pdf link\nTL;DR: Kids in confinement are more violent and cruel than kids not in confinement.\n\nThe places specially made for children's play are also the places where children can most easily be watched playing: the asphalt expanses of school playgrounds, the cage-like enclosures filled with junk by a local authority, the corners of recreation grounds stocked with swings and slides. In a playground children are, or are not, allowed to make chalk diagrams on the ground for hopscotch, to bounce balls against a wall, to bring marbles or skipping ropes, to play 'Conkers', 'Split the Kipper', 'Hi Jimmy Knacker'. Children of different ages may or may not be kept apart; boys may or may not be separated from girls. And according to the closeness of the supervision they organize gangs, carry out vendettas, place people in Coventry, gamble, bribe, blackmail, squabble, bully, and fight. The real nature of young boys has long been apparent to us, or so it has seemed. We have only to travel in a crowded school bus to be conscious of their belligerency, the extraordinary way they have of assailing each other, verbally and physically, each child feeling\u2014perhaps with reason\u2014that it is necessary to keep his end up against the rest. We know from accounts of previous generations with what good reason the great boarding schools, and other schools following, limited boys' free time, and made supervised games a compulsory part of the curriculum. As Sydney Smith wrote in 1810, it had become an 'immemorial custom' in the public schools that every boy should be alternately tyrant and slave. [snip for length; more descriptions of abuse by kids in school]\n\n\n[snip] [...]leading us [educators] to believe that a Lord of the Flies mentality is inherent in the young[...] [snip]\n\n\nThus recent extensive studies of apes and monkeys have shown, perhaps not unexpectedly, that animal behaviour in captivity is not the same as in the wild. In the natural habitat the welfare of the troop as a whole is paramount, the authority of the experienced animal is accepted, the idiosyncrasies of members of the troop are respected. But when the same species is confined and overcrowded the toughest and least-sensitive animal comes to the top, a pecking order develops, bullying and debauchery become common, and each creature when abused takes his revenge on the creature next weakest to himself. In brief, it appears that when lower primates are in the wild, and fending for themselves, their behaviour is 'civilized', certainly in comparison with their behaviour when they are confined and cared for, which is when they most behave 'like animals'.\n\n\nOur observations of children lead us to believe that much the same is true of our own species. We have noticed that when children are herded together in the playground, which is where the educationalists and the psychologists and the social scientists gather to observe them, their play is markedly more aggressive than when they are in the street or in the wild places. At school they play 'Ball He', 'Dodge Ball', 'Chain Swing', and 'Bull in the Ring'. They indulge in duels such as 'Slappies', 'Knuckles', and 'Stinging', in which the pleasure, if not the purpose, of the game is to dominate another player and inflict pain. In a playground it is impracticable to play the free-ranging games like 'Hide and Seek' and 'Relievo' and 'Kick the Can', that are, as Stevenson said, the 'well-spring of romance', and are natural to children in the wastelands. Often, when we have asked children what games they played in the playground we have been told 'We just go round aggravating people.' [snip; more descriptions of abusive games]\n\n\nSuch behaviour would not be tolerated amongst the players in the street or the wasteland; and for a long time we had difficulty reconciling these accounts with the thoughtfulness and respect for the juvenile code that we had noticed in the quiet places. Then we recollected how, in our own day, children who had seemed unpleasant at school (whose term-time behaviour at boarding school had indeed been barbarous), turned out to be surprisingly civilized when we met them in the holidays. We remembered hearing how certain inmates of institutions, and even people in concentration camps during the war, far from having a feeling of camaraderie, were liable to seek their pleasure in making life still more intolerable for those who were confined with them [...].\n\n2.2. We own people, and decide who you're with.\nYou're segregated by age, and divided in classes. Maybe you'd've learned to learn from a kid a couple years older than you, or learned to teach a kid a couple years younger than you. But that's hypothetical, because we've decided you're not to be with those people.\n2.3. We own Import.\nIn standard schooling, kids aren't around adults in adult environments doing adult activities for adult reasons. There's the real world, the adult world, where everything of Import is, and then there's the kid world, which has to make way for the adult world.\nFrom Children's Games in Street and Playground, Opie and Opie, 1969: IPFS pdf link\n\nWhat is curious about these embroilments is that children always do seem to have been in trouble about the places where they played. In the nineteenth century there were repeated complaints that the pavements of London were made impassable by children's shuttlecock and tipcat. In Stuart times, Richard Steele reported, the vicinity of the Royal Exchange was infested with uninvited sportsmen, and a beadle was employed to whip away the \"unlucky Boys with Toys and Balls\". Even in the Middle Ages, when it might be supposed a meadow was within reach of every Jack and Jill in Britain, the young had a way of gravitating to unsuitable places. In 1332 it was found necessary to prohibit boys and others from playing in the precincts of the Palace at Westminster while Parliament was sitting. In 1385 the Bishop of London was forced to declaim against the ball-play about St. Paul's; and in 1447, away in Devonshire, the Bishop of Exeter was complaining of 'yong Peple' playing in the cloister, even during divine service, such games as 'the toppe, queke, penny prykke, and most atte tenys, by the which the walles of the saide Cloistre have be defowled and the glas wyndowes all to brost'.\n\n\nShould such persistent choice of busy and provocative play-places alert us that all is not as appears in the ghettos of childhood? Children's deepest pleasure, as we shall see, is to be away in the wastelands, yet they do not care to separate themselves altogether from the adult world. In some forms of their play (or in certain moods), they seem deliberately to attract attention to themselves, screaming, scribbling on the pavements, smashing milk bottles, banging on doors, and getting in people's way. A single group of children were able to name twenty games they played which involved running across the road. Are children, in some of their games, expressing something more than high spirits, something of which not even they, perhaps, are aware? No section of the community is more rooted to where it lives than the young. When children engage in 'Last Across' in front of a car is it just devilment that prompts the sport, or may it be some impulse of protest in the tribe? Perhaps those people will appreciate this question most who have asked themselves whether the convenience of motorists thrusting through a town or village is really as important as the well-being of the people whose settlement it is, and who are attempting to live their\nlives in it.\n\nLet yong Peple go!\nFrom John Taylor Gatto, Dumbing Us Down, 1992: IPFS pdf link\n\nIn Monongahela by that river everyone was my teacher. Daily, it seemed to a boy, one of the mile-long trains would stop in town to take on water and coal, or for some mysterious reason; the brakeman and engineer would step among snot-nosed kids and spin railroad yarns, let us run in and out of boxcars, over and under flatcars, tank cars, coal cars, and numbers of other specialty cars whose function we memorized as easily as we memorized enemy plane silhouettes. Once a year, maybe, we got taken into the caboose that reeked of stale beer to be offered a bologna-on-white-bread sandwich. The anonymous men lectured, advised, and inspired the boys of Monongahela \u2014 that was as much their job as driving the trains.\n\n\nSometimes a riverboat would stop in mid-channel and discharge a crew who would row to shore, tying their skiff to one of the willows. That was the excuse for every rickety skiff in the twelve-block-long town to fill up with kids, pulling like Vikings, sometimes with sticks instead of oars, to raid the \"Belle of Pittsburgh\" or \"The Original River Queen.\" Some kind of natural etiquette was at work in Monongahela. The rules didn't need to be written down; if men had time they showed boys how to grow up. We didn't whine when our time was up: men had work to do \u2014 we understood that and scampered away, grateful for the flash of our own futures they had had time to reveal, however small it was.\n\nThe world isn't yours, it's the adults's. We're driving in our cars on our way to and from important things, and you better get out of the way. They're very important things, they aren't for you, go play (somewhere else).\nImagining that your activities, explorations, and questions in the classroom could be taken up with the solemn seriousness, open reality, and pivotal consequence of a factory floor, a judge's courtroom, or an artist's studio, is very cute and childlikeish. That I would believe you, is laughable, though I will of course humor you. If the work I give you is comically fake, well did you expect it to be real? You're only a child.\n3. Preference falsification and double binds.\n(See Wiki: Double bind, h/t Michael Vassar.)\nSlavoj \u017di\u017eek:\n\nIt's Sunday afternoon. My father wants me to visit our grandmother. Let's say my father is a traditional authority. What would he be doing? He would probably tell me something like, \"I don't care how you feel; it's your duty to visit your grandmother. Be polite to her and so on.\" Nothing bad about this I claim because I can still rebel and so on. It's a clear order. But what would the so-called post-modern non-authoritarian father do?\n\n\nI know because I experienced it. He would have said something like this, \"You know how much your grandmother loves you, but nonetheless I'm not forcing you to visit her. You should only visit her if you freely decide to do it.\" Now every child knows that beneath the appearance of free choice there is a much stronger pressure in this second message. Because basically your father is not only telling you, you must visit your grandmother, but you must love to visit it. You know he tells you how you must feel about it. It's a much stronger order.\n\nYour attention is yours (and you must give it to me). Follow your interests (and be interested in what we're \"teaching\"). You want to be good, right? So you want to follow the rules we make, right? And they're rightful rules, aren't they, or why else would you want to follow them? Develop your own unique specialness (make sure it's one of the things on this list though). God help you if you question our authority to deny you bodily autonomy, and you'd better pretend that CAN and MAY are meaningfully different here.\nAlways smile. Refrain from looking out of the window.\nPossibilities\nMany of these harms can be alleviated or avoided by just not being crazy. (Yes it's that easy.) It's fine if the kids aren't paying attention to what you're teaching, why are you trying to teach 20 kids at once anyway? If something is truly repulsive to a kid's attention, then you're just wrong about what's good for the kid right now, period. And so on.\nWhat's left is for teachers (or \"mentors\" or \"guides\" or something) to bring the world to the children. Montessori wrote about this at length. In general, there's much work to be done with teaching; but this is mostly unknown territory, since teachers have mostly so far been obliged to do something other than facilitate learning, and teachers will have to learn how to let the students learn, which is a detailed activity with unknown challenges. Maximize blocks of uninterrupted time. Maximize the environment for opening up the world, deferring to children's interests. Allow the children to make their own environment, like the people that they are.\nIt would be trivially easy to make school a better product for parents. Have it run until 1720 or later, so parents with work have daycare (kids of course need daycare until they're 18; or at least, that is the revealed preference for whatever reason). Allow flexible sign-in and sign-out. Be completely open to parents visiting. Let kids play outside for many hours, so they aren't stressed, sick, and depressed. Never give homework, so kids can be with their families.\nI spoke with someone who runs a pre-school in the spirit of Montessori. Ze told me that ze started a Montessori school, but regulations and money problems made it go under. Ze was sure there's a market, though. I suspect (not having carefully evaluated things) that Effective Altruism is severely underweighting the value of investing in education. I think that harming kids makes them grow up to be more likely to harm other people and be less creative. School almost certainly doesn't matter if AGI comes in the next decade or two, but if we have longer, then more rolls of the dice for brilliant natural philosophers seems like maybe a pretty good way to spend resources.\nContact me at my gmail address, username \"tsvibtcontact\", if you want to discuss with me possible interventions and funding (not that I'm well-suited to these tasks or have too much energy for them, but I'm interested).\nNot sure Becky's kidding.\n", "title": "Harms and possibilities of schooling", "karma": 78, "af": false, "userid": "LtHeYhWmaud6YNA3m", "id": "aaHDA4X6cTzFrvuSX"}, {"text": "Or \u201cHow I got my hyperanalytical friends to chill out and vibe on ideas for 5 minutes before testing them to destruction\u201d\n\n\n\nSometimes talking with my friends is like intellectual combat, which is great. I am glad I have such strong cognitive warriors on my side. But not all ideas are ready for intellectual combat. If I don\u2019t get my friend on board with this, some of them will crush an idea before it gets a chance to develop, which feels awful and can kill off promising avenues of investigation. It\u2019s like showing a beautiful, fragile butterfly to your friend to demonstrate the power of flight, only to have them grab it and crush it in their hands, then point to the mangled corpse as proof butterflies not only don\u2019t fly, but can\u2019t fly, look how busted their wings are.\n\n\n\nYou know who you are\n\n\n\n\n\n\n\n\n\n\n\nWhen I\u2019m stuck in a conversation like that, it has been really helpful to explicitly label things as butterfly ideas. This has two purposes. First, it\u2019s a shorthand for labeling what I want (nurturance and encouragement). Second, it explicitly labels the idea as not ready for prime time in ways that make it less threatening to my friends. They can support the exploration of my idea without worrying that support of exploration conveys agreement, or agreement conveys a commitment to act.\n\n\n\nThis is important because very few ideas start out ready for the rigors of combat. If they\u2019re not given a sheltered period, they will die before they become useful. This cuts us off from a lot of goodness in the world. Examples:\n\n\n\nA start-up I used to work for had a keyword that meant \u201cI have a vague worried feeling I want to discuss without justifying\u201d. This let people bring up concerns before they had an ironclad case for them and made statements that could otherwise have felt like intense criticism feel more like information sharing (they\u2019re not asserting this will definitely fail, they\u2019re asserting they have a feeling that might lead to some questions). This in turn meant that problems got brought up and addressed earlier, including problems in the classes \u201cthis is definitely gonna fail and we need to make major changes\u201d and\u00a0 \u201cthis excellent idea but Bob is missing the information that would help him understand why\u201d.This keyword was \u201cFUD (fear, uncertainty, doubt)\u201d. It is used in exactly the opposite way in cryptocurrency circles, where it means \u201cyou are trying to increase our anxiety with unfounded concerns, and that\u2019s bad\u201d. Words are tricky.Power Buys You Distance From The Crime started out as a much less defensible seed of an idea with a much worse explanation. I know that had I talked about it in public it would have caused a bunch of unproductive yelling that made it harder to think because I did and it did (but later, when it was ready, intellectual combat with John Wentworth improved the idea further).The entire genre of \u201cHere\u2019s a cool new emotional tool I\u2019m exploring\u201dThe entire genre of \u201cI\u2019m having a feeling about a thing and I don\u2019t know why yet\u201d\n\n\n\nI\u2019ve been on the butterfly crushing end of this myself- I\u2019m thinking of a particular case last year where my friend brought up an idea that, if true, would require costly action on my part. I started arguing with the idea, they snapped at me to stop ruining their dreams. I chilled out, we had a long discussion about their goals, how they interpreted some evidence, and why they thought a particular action might further said goals, etc.\u00a0\n\n\n\nA week later all of my objections to the specific idea were substantiated and we agreed not to do the thing- but thanks to the conversation we had in the meantime, I have a better understanding of them and what kinds of things would be appealing to them in the future. That was really valuable to me and I wouldn\u2019t have learned all that if I\u2019d crushed the butterfly in the beginning.\n\n\n\nNotably, checking out that idea was fairly expensive, and only worth it because this was an extremely close friend (which both made the knowledge of them more valuable, and increased the payoff to helping them if they\u2019d been right). If they had been any less close, I would have said \u201cgood luck with that\u201d and gone about my day, and that would have been a perfectly virtuous reaction.\u00a0\n\n\n\nI almost never discuss butterfly ideas on the public internet, or even 1:many channels. Even when people don\u2019t actively antagonize them, the environment of Facebook or even large group chats means that people often read with half their brain and respond to a simplified version of what I said. For a class of ideas that live and die by context and nuance and pre-verbal intuitions, this is crushing. So what I write in public ends up being on the very defensible end of the things I think. This is a little bit of a shame, because the returns to finding new friends to study your particular butterflies with is so high, but ce la vie.\u00a0\n\n\n\nThis can play out a few ways in practice. Sometimes someone will say \u201cthis is a butterfly idea\u201d before they start talking. Sometimes when someone is being inappropriately aggressive towards an idea the other person will snap \u201cwill you please stop crushing my butterflies!\u201d and the other will get it. Sometimes someone will overstep, read the other\u2019s facial expression, and say \u201coh, that was a butterfly, wasn\u2019t it?\u201d. All of these are marked improvements over what came before, and have led to more productive discussions with less emotional pain on both sides.", "title": "Butterfly Ideas", "karma": 90, "af": false, "userid": "7w7hLkTTQLuPSAWTT", "id": "R6M4vmShiowDn56of"}, {"text": "I want to use the AI\u2019s intelligence to figure out how to translate into the human ontology. The hope is to route a smart entity's performance through a dumb entity's understanding and thereby get the smart entity to solve interpretability-by-the-dumb-entity. While my proposed current architecture overcomes a class of counterexamples in a manner which I find elegant, it is still broken by several plausible counterexamples.\u00a0A previous version of this post linked to a Google doc.\u00a0Previously, on ELKThis section summarizes ELK. Skip if you\u2019re already familiar.In the mythos of\u00a0Eliciting Latent Knowledge, there exists a forbidden vault locking away an enormous diamond:In order to fend off thieves, this vault brims with controllable traps and tricks. We've trained a really smart AI to take in a video feed, and consider an action sequence (e.g. \"open the trapdoor\"), and then predict what the vault camera will show if those actions are taken. At the end of the day, we just want to know whether the diamond is safe. Looking at the camera is not always enough:But the AI is really smart. It knows what happened to the diamond. Can we elicit its latent knowledge?The original ELK document tries to figure out how to train a \"reporter\" to figure out what's going on in a really smart AI predictor. The intended reporter would\u00a0directly translate\u00a0from the AI's ontology to the human\u2019s ontology, and then answer whether the diamond is there or not. In the original ELK document, the reporter is wired as follows:The training questions/answers are generated by a human considering a range of simple, human-comprehensible situations (i.e. (video, action sequence) tuples), querying their model of what happened, and answering e.g. whether the diamond is safe or not:We never want there to be a situation where the AI predictor (the \"figure out what's going on\" module)\u00a0unambiguously knows that the diamond was taken, but the reporter says \"the diamond is fine.\" More precisely, we don't want such a configuration to plausibly minimize training loss.\u00a0The direct translator minimizes training loss because it answers all questions correctly on the training distribution. So far, so good. However, the\u00a0human simulator\u00a0reporter also minimizes training loss, by extracting the predicted video completion from the AI predictor, and then conditioning the human labeler on the entire video and action sequence, and then outputting whatever the human would think about the situation. On the training distribution, this setup gets all of the questions right (and so minimizes training loss), because it exactly reproduces the labeling process! But in general, the human won't understand many situations, and so the human simulator will output answers which the AI predictor knows to be incorrect.The direct translator solves ELK for narrowly defined questions (e.g. \"is the diamond still in the room?\"). How do we get it?Thinking Via A Human ImitatorI want to structure the problem so that direct translation is instrumentally required for minimizing predictive training loss. To that end: What if the AI had to do some proportion\u00a0p\u00a0of its thinking in the human net?Assume that both the AI and the human reason using Bayes nets.[1]\u00a0Suppose we have access to a perfect simulation[2]\u00a0of not only a human Bayes net, but of the human who labels the training data.\u00a0The AI receives the first part of a video and a sequence of actions. Suppose the AI can think for\u00a0n\u00a0time steps in total, before it has to predict the remainder of the video. At timestep\u00a0(1\u2212p)n/2, the AI encodes its cognitive work into the human imitator net, where the human\u2019s inference algorithm runs for another\u00a0pn\u00a0steps. Lastly, the AI decodes the human\u2019s state of mind and thinks for the remaining time.\u00a0For example: If\u00a0p=\u2153, then the AI thinks for\u00a0n/3\u00a0steps, the human thinks for\u00a0n/3\u00a0steps, and then the AI thinks for\u00a0n/3\u00a0steps.\u00a0In order to answer questions, after the last time step, the encoder maps the predictor net\u2019s internal state onto the human net, where the reporter then answers questions. This is illustrated in the diagram below.The human imitator net is frozen. Everything else is learned concurrently. The \u201cAI predictor\u201d, \u201cencoder\u201d, and \u201chuman imitator\u201d modules are the same entities, shown duplicated in order to clarify the computational graph of a forward pass.Training procedureWe initialize the agent architecture displayed in the diagram, freezing the human imitator network.Given a dataset labeled by the human, jointly train the green components:For each ((video-1, action sequence, video-2), (question, answer)) pair:Provide (video-1, action sequence, question) as input and train the network to predict video-2 and the right answer, handing off inference on the schedule specified above.Desired outcomeThe intended (or \u201cfaithful\u201d) encoder finds an extremely good (or even the best) mapping from the AI\u2019s ontology to the human\u2019s.The intended decoder[3]\u00a0efficiently incorporates the human\u2019s cognitive work back into the AI predictor\u2019s cognitive state.The intended reporter mimics the human\u2019s query function.Why the intended network defeats counterexamples. Suppose that at its final step of inference, the predictor believes that the diamond has been stolen. If ELK is possible to solve in this instance, there exists a direct translator which reports this fact via translation into the human ontology, and an AI \u2192 human ontology map would exist. The intended encoder implements that map and encodes this fact into the imitated human network. The intended reporter implements the human\u2019s query function and reports that the diamond has been stolen.Therefore, (narrow) ELK reduces to finding the intended encoder and reporter, as described in the bullet points above. Now let\u2019s examine how hard the reduced problem is.Learning the reporterI first want to argue that if the intended encoder is assumed to have been learned, we also learn the intended reporter.[4]\u00a0To understand what reporters will be incentivized, let\u2019s reconsider the causal history of the human-provided labels:The human query function is just the pink box in the following:That is, the human query function takes in a question, which determines the subset of the net which it considers, and then computes an answer as a function of that subset. For generality, the query function can take different questions as inputs, like \u201cdid anyone open the door?\u201d, and then answer by using a different part of the human Bayes net.The human-provided data points necessarily portray situations that the human basically understands. Therefore, since we assumed the encoder is faithful and the predictor is powerful enough to figure out the second half of the video: on each training example, the simulated and real human end up in very similar states of mind after doing inference. Just compare the AI\u2019s diagram with the human\u2019s diagram, in order to see that on the training distribution and given a faithful encoder, the after-AI prediction-finishes human imitator should be reasoning extremely similarly to the human. All that\u2019s left to learn is the human query function! Hopefully, this is a simple \u201cblank\u201d for the AI to fill in.\u00a0For example, suppose the real human knows that the action sequence opens the trapdoor, and that the video shows someone falling into it. Because the predictor understands the situation and we have the intended encoder, the imitated human also knows this, and perhaps they also know some additional translated details. Therefore, to produce the correct answer, the reporter just has to look at the imitated human\u2019s net and pick out the relevant nodes for the real human\u2019s query function. This is the prototypical human-understandable training example.The reporter could instead minimize training loss by learning another human simulator which independently does inference after backing out (video-1, action sequence, video-2) from the imitated human net, but without having the second human simulator\u2019s intermediate nodes filled in by the encoder. But assuming a faithful encoder, that seems rather ridiculous and easily handled via time or complexity regularization.[5]I think there may be a strong story for why a faithful encoder ensures that the learned reporter will be faithful. Contrast this situation to that in the ELK report, where we\u2019re constantly fighting to stop the reporter from being a human simulator. Here, we\u2019re trying to learn a much simpler reporter. Therefore, I\u2019m more optimistic about asking questions for which only the intended reporter is the answer.Learning the decoderI\u2019m not worried about the decoder, except insofar as it encourages or discourages faithful encoders. Given the intended encoder, a competent decoder is learned to reduce predictive loss.Learning the encoderFiguring out how to learn the intended encoder would be a big deal. We would know how to force arbitrarily smart agents to solve interpretability on themselves as an instrumental subgoal for their prediction task.\u00a0Candidate counterexamplesHere are a range of attempted counterexamples. First, I claim that training_loss(intended setup) < training_loss(naive human simulator).Candidate counterexample: Suppose the encoder reads (video-1, actions, predicted video-2) from the AI predictor module, and then simulates the human overseer doing inference from those observations, and then somehow forces the human imitator net to believe what the human would believe about the diamond, after reasoning from those inputs. Tautologically, this minimizes training loss for the questions.\u00a0Rebuttal: This reduces the predictive accuracy of the whole AI system, since\u00a0pn\u00a0of the AI\u2019s total inference time is wasted.[6]\u00a0The intended network leverages at least some useful computation by the human to improve the AI\u2019s predictions, while also acing all of the questions on the training distribution (assuming zero labeling error).\u00a0\u00a0Candidate counterexample: The AI predictor wipes its internal state at the last moment, except for its intended prediction.\u00a0Rebuttal: This can\u2019t minimize loss, since the AI predicts slightly less well than an AI which spent all of its available time thinking.\u00a0Candidate counterexample: The encoder implements a meaningful translation midway through the computation, but checks to see if the AI predictor is done thinking, and if so, simulates a human instead.\u00a0Rebuttal: Randomly select both\u00a0p\u00a0and the time when the AI predictor must hand off thinking to the human net. This way, the encoder shouldn\u2019t be able to distinguish between encoding-to-think and encoding-to-report.\u00a0Candidate counterexample: For each training data point, the encoder runs a human simulator, sets the human\u2019s initial beliefs to correctly answer the question if immediately queried by the reporter, but without faithfully encoding the AI\u2019s understanding into the human\u2019s beliefs. Then, if the human net keeps thinking, that means the reporter isn\u2019t being called, and somehow the human\u2019s beliefs evolve in a way which will help the AI predictor once the human\u2019s final beliefs are decoded, without those beliefs meaningfully corresponding to the real situation.\u00a0Rebuttal: During training, run the human net for\u00a0pn\u00a0steps before calling the reporter. Given that we also randomize\u00a0p, this incentivizes the encoder to ensure the human net answers correctly\u2014even if the human net thinks for varying amounts of time before the reporter is called.[7]\u00a0(Therefore, during deployment, the reporter can query the human net immediately.)The above counterexample attempts suggest that during training, we should eliminate observable differences between encoding-to-think and encoding-to-report. Therefore, I think we should modify the simplified hand-off procedure I described above so that, during training:A range of handoff thresholds and\u00a0p\u00a0proportions are drawn\u2014in particular, there should be a reasonable probability of drawing\u00a0p\u00a0values close to 0, close to 1, and also 0 and 1 exactly.The human net runs for\u00a0pn\u00a0steps before calling the reporter.Candidate counterexample (h/t janus):\u00a0The encoder implements:\u00a0IF human can understand the situation without direct translation:                     \t\n\ttranslate accurately\nELSE: \n\tdon'tRebuttal:\u00a0This encoder essentially checks whether it\u2019s on the training distribution or not, since we assume we only train on human-comprehensible situations. This encoder seems like an implausible thing to learn, but perhaps it\u2019s considered a plausible counterexample in the ELK game.\u00a0\u00a0Definite counterexample (h/t Evan Hubinger):\u00a0Instead of directly translating the state of the room to the human net, an unfaithful encoder computes what imaginary additional cameras would show, given the AI\u2019s best understanding. This unfaithful encoder basically communicates a telepathic vision to the human net, via several simulated video feeds. In fact, the encoder is truthful about what the sensors would show, if they were there. On the training distribution, the rendered videos give the human net an accurate picture of the AI\u2019s best situational understanding. Therefore, the human compute time should be about as predictively helpful as the direct translator\u2019s compute time.\u00a0However, there exist situations off of training distribution where the imaginary sensors are fooled, and so ELK generalization fails. In such cases, the AI predictor knows that there's no diamond, but the unfaithful encoder doesn't communicate that.Analysis: I think that there should be some way besides direct translation to better leverage the human net\u2019s compute, and this seems like a plausible grounding of how it might happen. Unless this is somehow ruled out, I think this counts as a counterexample: There will be situations not in the training distribution, in which the AI predictor knows that the diamond is missing, but where the imaginary sensors are being fooled. However, this bad encoder does not tell the human that the diamond is missing, and so the reporter wrongly answers that the diamond is still present.Here are some issues I want more clarity on:\u00a0Perhaps it\u2019s better for the encoder to unfaithfully translate so as to help the human net better consume compute, so as to better help the AI predictor use the human net\u2019s inference time.\u00a0The unfaithful encoding has to pass the questions on the training distribution, which seems to require a bad/non-intended reporter.If we already have an unfaithful encoder, the intended reporter may get questions wrong, which might encourage the reporter to be a human simulator (even overcoming regularization).\u00a0Since the encoder is bad, the reporter is bad; since the reporter is bad, the encoder stays bad. Is this a plausible local minimum?This also requires learning a bad encoder to begin with.The costs of direct translation probably grow with the sophistication of the AI predictor, but the costs of human simulation are fixed. Therefore, it might be a better use of the encoder\u2019s compute to simulate the human, fix the right answers in the human net, while also using most of its encoder-compute to keep making progress on the prediction task. The encoder\u2019s work would be stored in the rest of the human net\u2019s nodes, which serves as a latent space for the decoder. (h/t Leo Gao)Note that as\u00a0n\u2192\u221e, there's increasing benefit to using the frozen human net.The AI predictor might only translate human-comprehensible aspects of the situation, factoring the cognitive work so that the human net thinks about the parts of the situation for which they have a comparative advantage.\u00a0\u00a0Note that the training distribution only contains human-comprehensible situations.\u00a0Also, the human net might make bad inferences sometimes.If it\u2019s due to systemic bias, the decoder could simply adjust the human net\u2019s outputs when incorporating their conclusions back into the predictor net. This means that the encoder can still be faithful.However, the human net may need to be \u201clied to\u201d by the encoder in a way which is not just simplification via direct translation, but encoding-level distortion so that the human\u2019s decoded cognitive work is more helpful for prediction.We might mitigate this by doing perfect inference in the human net.\u00a0But this would make the approach less likely to transfer when the human cognition isn\u2019t neatly represented in a Bayesian network.Depending on how we balance loss between prediction and question-answering, and the expressiveness of the AI network, loss-minimizing configurations may be chosen from a nontrivial Pareto frontier of (total predictive loss, total Q/A loss), which might change the analysis.Does the scheme definitely break if the simulated human is stupid (e.g. \u20132SD intelligence)? If so, that\u2019s a bad sign, as it\u2019s unlikely that the approach just\u00a0happens\u00a0to start robustly working somewhere in between stupid and smart human intelligence levels.CompetitivenessI think that this setup allows predictive accuracy which is competitive with the unaligned benchmark\u2014even more so once training is over.Training distribution. At worst, the AI can just ignore the decoder output, and so the AI isn\u2019t any worse at prediction than if it only had\u00a01\u2212p\u00a0times its original thinking time.\u00a0Deployment. After training is complete, we don\u2019t actually have to make the AI consult the human net at all; we can set\u00a0p=0. This should work fine if we train the network with a range of\u00a0p\u00a0values and handoff thresholds, so that it\u2019s used to reasoning on its own, to reasoning mostly via the human, to everywhere in between; at any time, the encoder should be ready to read out the predictor module\u2019s state to the human.\u00a0While the AI might be used to \u201cworking with\u201d the human net, the predictor presumably consumes compute more efficiently than does the encoder/human net/decoder ensemble, and so the runtime compute budget is likely best used by the AI predictor. Once the predictor has finished inference, we can just encode the final state into the human net, where the reporter answers questions about the situation.One way in which the intended network could suffer a performance penalty is, the AI predictor learns to make its cognition more easily human-interpretable (so as to make the encoding/decoding easier), which somewhat restricts its predictive capabilities. This issue is not fixed by setting\u00a0p=0\u00a0at runtime. I\u2019m not sure how big of a deal this is.ConclusionI feel excited about this proposal. It seems to at least defeat the naive human simulator in an interesting way, but e.g. Evan\u2019s counterexample shows that it still fails in the worst case. That is, there are many ways to extract useful compute from the human net, and we already know of one which is not direct translation. I probably need a significantly better design to exclude all of the ways which are not direct translation\u2014if that\u2019s even possible at all. I think it\u2019s very probable (90%) that a simple counterexample-free design simply doesn\u2019t exist.I wonder whether there are basic design improvements which cleanly resolve many of the remaining issues.\u00a0Thanks to AI_WAIFU, janus, Evan Hubinger, Mark Xu, Leo Gao, Justis Mills, and Rohin Shah for feedback.^This proposal only seems to require that both agents reason in a way where \u201camount of time thinking\u201d is a sensible parameter.^Perfect imitation is a simplifying assumption which may not be realistic, but whose presence does not make ELK obviously trivial. I think that if this approach works given the assumption, we can probably relax it.^The decoder affects the AI\u2019s predictive capabilities during training; it is logically unrelated to whether the system solves ELK, or to its capabilities during deployment. Practically, absent a good decoder, there is little reason for a faithful encoder to be learned.^Even if this holds, however, we will still have to reason about whether the two components can be jointly learned, or if the intended encoder can be learned before the reporter. Let\u2019s set that aside for now.^We avoid the pitfalls of regularizing the ELK-reporter because our intended reporter doesn\u2019t have to translate from the superhuman predictor, it just has to deal with the constant-size frozen human imitator network.^This logic fails if the AI predictor can get pixel-perfect video prediction on the training distribution, even without using the human\u2019s portion of the time budget. However, this is presumably dealt with by having some chance of p being close to 1\u2014so that the human net has to do most of the information processing.^This probably rules out a class of \u201cfragile\u201d encodings which don\u2019t make sense upon human reflection, which might push towards more faithful and robust encoders.", "title": "ELK Proposal: Thinking Via A Human Imitator", "karma": 22, "af": true, "userid": "pgi5MqvGrtvQozEH8", "id": "z3xTDPDsndJBmHLFH"}, {"text": "IntroductionI am a very anxious person. One of the most damaging ways this manifests is that I am pretty risk-averse and afraid of failure. In situations of uncertainty, I often want to freeze up, and know I\u2019ll feel safer doing nothing. This is a really bad problem! If I let this dictate my life, I lose a ton of value. In particular, there are a lot of areas in my life that are hits-based, where the best way to be successful is to persevere through many failures and seek the upside risk of things occasionally going super well. I want to be someone who can be a great researcher, find really awesome friends, and generally be ambitious about all areas of my life going well. And to achieve this, it is important that I be the kind of person who can take actions with high expected value, and persevere through failures without feeling paralysed. The key thing going wrong here is that I beat myself up over bad outcomes even when I had no way of knowing it wouldn\u2019t work out, given the information at the time. And anxiety gives me a negative prior and causes suboptimal outcomes to stick in my mind. Failures feel painful in a way that missed opportunities do not.My solution to this is to think in bets, not outcomes. To clearly notice all of the good bets that I take, the actions that I endorse given what I knew at the time, and to be happy about each of those. And to think of my life in terms of this, rather than the concrete outcomes of the bet, and whether that was a success or failure. At the end of the day, the only thing I can control is the bets that I make, and the policies I follow, and there will always be uncertainty on the outcomes. And if I make a good bet with a bad outcome, I should be happy about this, not sad! I refuse to let my negative emotions be tied to things fundamentally outside my control.My Underlying ModelI first formed this view when I did a trading internship a few years ago. In settings like financial trading or poker, the fundamental skill is about engaging well with uncertainty, and getting past anxieties is a key part of that! And noticing all of the expected value I was missing when I froze up did a lot for helping me notice this failure mode and learning how to solve it. And though the lessons generalise, real life is often a much harder learning environment than these settings - I make fewer bets and so get fewer data points, and it\u2019s much harder to explicitly calculate what\u2019s going on.I find it easiest to understand what\u2019s going on here and how to fix it when thinking of myself as having a reinforcement learning system inside my head, shaping my actions. I take actions in the world, get feedback from my environment, and use this to update the policies that I follow. Within this framing, there are two clear problems with learning to make good bets with high upside, while being anxious.The first problem arises because reality is noisy! Even if I had zero anxiety, fundamentally reality has unknowns and I must make decisions under uncertainty. But, by default, I only learn about my actions from their outcomes. And this makes it really hard to learn strategies around pursuing occasional major upsides! It\u2019s obviously worth it to go on 99 unsuccessful dates if the hundredth results in marriage. But by default, my reinforcement learner will likely be discouraged and stop after 99 failures. While, if I can reframe it as 99 successful bets, then I get much better!The second problem comes from anxiety, which causes me to over-update on negative feedback, and consider it way more important than positives. This is a fundamental issue with my learning algorithm that means I will learn systematically bad policies. By focusing on the action I took being good, this reduces the anxiety caused by unsuccessful outcomes. Note that negative feedback doesn\u2019t just include stuff that may actually be a big deal, like a romantic rejection, or missing out on a job I really cared about. At least for me, my anxiety reacts badly to even minor negative outcomes with no real consequences, like making a joke that didn\u2019t land, or recommending a book to someone that they\u2019ve already read.By default, I feel like I can solve these issues if I just try harder. Think harder about an issue, go through every consideration, analyse it more deeply, and only take the actions that will work out well. This is an illusion! Reality is not fully knowable. And thinking harder has costs. If I follow the strategy of \u201cjust try harder\u201d, I will implicitly miss a lot of bets worth taking. The optimal strategy, given that I am an imperfect person in an uncertain world, is to take positive expected value bets. Finding ways to learn well in spite of anxiety is essential, because anxiety holds me back from so many bets worth taking.How to Apply This?The idea of thinking of my actions as bets and not focusing on their outcomes is a pretty core part of how I think about my life, and is useful in a wide range of areas in different ways. A quick brainstorm of different areas where my anxiety significantly holds me back from making the right bets:Applying for jobsAsking people out/going on datesPursuing research directionsMaking friends, and generally taking social initiativeOffering people help and favoursRecommending books/articles/resourcesIntroducing people who might get onWriting a blog postOr starting a blog in the first place!Writing a cold emailGiving adviceAsking for helpEspecially asking someone for their time, or anything else with a risk of rejection and that might be being a burden!Any form of seeking upside riskSharing opportunities - I personally try hard to message people with jobs that might be a good fit, good articles I read they might enjoy, etc.The instance of this I\u2019m most proud of is getting stressed about Omicron near the start of the surge, and messaging 100 friends with instructions on how to get boosters earlier - this felt stressful at the time, but resulted in 5-10 counterfactually getting it a week or two earlier, and 1-3 getting a booster at all.Exercise: Set a 5 minute timer and brainstorm times in your life when this bias applies. How could you orient to these in terms of bets, not outcomes? The exact way I try to think in bets not outcomes varies depending on context, but there are a few core principles that stand out:Find ways to actively be excited about unsuccessful outcomes, so long as I think it was a good bet!One way that works well for me is to reflect on how the action fits my self-identity, and is an example of becoming the kind of person I want to be. This successfully shifts focus from outcomes because my identity is a function of the actions I take, not the feedback from the worldThis is the core insight of becoming a person who actually does thingsAnother way is to quantify thingsMake a log of your unsuccessful bets, eg a list of rejections or failures. Set targets for how many failures you want to have, and see each one as an example of becoming someone who can put yourself out there!Estimate the probability of the outcome you want! Eg Chris Olah\u2019s framing of dating and meeting potential partners in terms of micro-marriagesMagnify your excitement about positive outcomes! Remember them, cherish them, and use them as motivation!Keep a log of great outcomes, and bets going well.Eg, I often share opportunities in group chats, and know of at least two people who\u2019ve gotten internships this way - I find this super motivating to do it more often!Relatedly, I keep a log of particularly happy memories and meaningful compliments, which is really uplifting to read when I\u2019m downNotice selection bias - all the good outcomes you might not hear about! Eg, I occasionally hear from people who\u2019ve had significant life improvements from things I\u2019ve done. This is fucking awesome in and of itself, but even better when I reflect on how I likely miss out on most things like this!Try to shift the selection bias, by making it clear that you love hearing about things like this, and being easy to reach!If anything I\u2019ve done has improved your life, I\u2019d love to hear about it!Reflect on whether I could have done something differently, given what I knew at the time. This really helps to defuse the anxiety that I\u2019m missing an important lesson and could have known better, and occasionally get the insight that it was a bad outcome!It\u2019s important to focus on given what I knew at the time. If I\u2019m not careful, my anxieties love to smuggle in some hindsight bias, and tell me that I\u2019m an idiot for not having known the future! By focusing on general policies I could follow, I can get past this.Engage my inner simulator and ask myself \u201cSuppose, at the time, I predicted it would go badly and decided not to do it. Am I surprised by this outcome?\u201dFurther, ask myself what happened, and why I decided not to do it. Was it for the right reasons?Take the outside view - is there any similar past action that did go well? And if so, was this case obviously worse than that one, given what I knew beforehand back then? Can I find a policy which avoided this failure without missing out on that success?Does This Reward Bad Bets Too?One caveat worth addressing is whether this strategy could be dangerous? When I think about putting it into practice, this is the biggest flinch from my anxiety - maybe my bets are actually systematically bad and I am deluding myself, and the outcomes are the only way to get this feedback. This is obviously worth considering, and will sometimes happen! The ideal world is one where I evaluate each outcome for information that I\u2019m missing, and take it as a slight negative update on whether the bet was worth making. But there is no way of reaching that ideal world - my anxiety is a major bias, it pushes me towards risk-aversion, and it\u2019s basically impossible to perfectly correct for a bias like this. My solution essentially introduces a counter-bias, towards ignoring the outcomes by default, which pushes me towards risk-seeking. In principle, there\u2019s some risk of overshooting the ideal point and being too risk-seeking, but in practice I think this is really unlikely! Especially if I explicitly reflect on whether I could have known better. My anxiety creates a pretty big bias towards risk-aversion, and dealing with anxiety is hard, and nothing I do is likely to create as big a bias the other way. I\u2019m not able to ignore the anxiety at particularly bad outcomes, or the creeping doubt of getting way more unsuccessful outcomes than expected. The sheer fact that I feel anxiety about overshooting is a sign that I am safe, and can trust myself to not go too far without needing to actively track it!Rewarding Other People\u2019s BetsMany of my anxieties are social in nature, and I get way more anxious about bad outcomes involving other people. And it\u2019s much easier for someone else to help me overcome a socially-related anxiety by giving reliable feedback, than trying to deal with it within the insecurities of my own head. I like to seek positive externalities, so a great (and sad) thing is that this works in reverse - social anxieties are super common, so if I can help other people reward themselves for good bets with bad outcomes, I can help them make much better bets! Often I do this by being enthusiastic and positive when I see someone who made a good bet with a bad outcome - offering them sympathies about the outcome itself of course, but also congratulating them on putting themselves out there, and making a good bet! I think it\u2019s reasonable to have some concern about insincerity or seeming mocking/insensitive, but in practice I find this often goes down well. Especially if I explain the framing of bets not outcomes, and get them to think about whether the bet was a bad idea given what they knew at the time.This applies in all the settings I brainstormed above, but is particularly important if someone made a good bet towards me! Eg someone recommends I apply for a job that\u2019s a bad fit, sends me an article I didn\u2019t enjoy, a book recommendation I\u2019ve already read, an introduction that didn\u2019t work out, gave me advice I\u2019d already tried or that didn\u2019t work, etc. I know I find it super discouraging to be on the other end of that, so I always try to clearly say that what they did was positive expected value, and that I appreciate it and hope they do that kind of thing again! A lot of great things in my life have come from people sending me good opportunities, and it\u2019s crazy to train people to not do that. (Though only if I think it actually was positive expected value, obviously - don\u2019t reward people for bad bets and bad outcomes!)Doing this also selfishly helps me - it creates a social context around me where other people will reward me for taking good bets, and helps build the association in my mind that eg \u2018applying for jobs and getting rejected = good\u2019, which helps me internalise it and apply this to myself.Exercise: Set a 5 minute timer and brainstorm ways you can help reward people around you for making good bets with bad outcomes.ConclusionIf you relate with the failure mode of fixating on failures and being risk-averse, I think it\u2019s really worth trying to be on top of this, and focusing instead on the actions you took, given what you knew at the time! Anecdotally this seems super common - many of the smartest people I know are super insecure and risk averse. And this is a massive tragedy because the world is full of wasted motion - if you\u2019re unable to be ambitious and take the opportunities that come your way, you\u2019ll miss out on a lot.So, as a final exercise, reflect on where this bias holds you back in your own life. What are the good bets you fail to make? What opportunities do you miss out on? Where do your anxieties unduly punish you? And what are you going to do about it?", "title": "Reward Good Bets That Had Bad Outcomes", "karma": 41, "af": false, "userid": "KCExMGwS2ETzN3Ksr", "id": "5qayrQp9dXWm4fhje"}, {"text": "Epistemic status:\u00a0exploratoryWhat is \u201cintuition pumping\u201d? I had seen the term used on LW, probably used it myself, without wondering that much what \u201cintuition pumps\u201d even are. Pure pedagogical illustrations? Mere rhetorical devices? Subtle epistemic tools? My intuition about intuitions favored the latter, but I also knew well how intuition can mislead.To make sense of all of this, I went back to the source, the inventor and master wielder of intuition pumps: Daniel Dennett. Not only did he coin the term and use it abundantly, but his \u201cIntuition Pumps and Other Tools for Thinking\u201d teaches how to use intuition pumps\u00a0his way.Spoiler alert: they can and should be used as thinking tools to find out the essential parts of the problem. But that requires a meta-level analysis that Dennett calls \u201cturning the knobs\u201d\u2014 checking the robustness of the intuition to various changes in the story.Intuition pumps\u2019 origin storyEveryone I\u2019ve read on intuition pumps, Dennett included, points to his response to Searle\u2019s\u00a0Chinese Room as the birth of the term.(The milk of human intentionality, Dennett, 1980)Searle's form of argument is a familiar one to philosophers: he has constructed what one might call an intuition pump, a device for provoking a family of intuitions by producing variations on a basic thought experiment. An intuition pump is not, typically, an engine of discovery, but a persuader or pedagogical tool \u2014 a way of getting people to see things\u00a0your way once you've seen the truth, as Searle thinks he has. I would be the last to disparage the use of intuition pumps \u2014 I love to use them myself \u2014 but they can be abused. In this instance I think Searle relies almost entirely on ill-gotten gains: favorable intuitions generated by misleadingly presented thought experiments.What\u2019s interesting here is that Dennett seems to have expanded his views on the usefulness of intuition pumps: here he explicitly criticizes the use of intuition pumps for discovery and clarification, but in Intuition Pumps and Other Tools for Thinking he clearly count them as thinking tools that have positive and research-relevant uses. He also gives a bit more of a definition there.\u00a0(Intuition Pumps and Other Tools for Thinking, Dennett, 2013)Other thought experiments are less rigorous but often just as effective: little stories designed to provoke a heartfelt, table-thumping intuition \u2014\u201dYes, of course, it has to be so!\u201d\u2014 about whatever thesis is being defended. I have called these intuition pumps. I coined the term in the first of my public critiques of philosopher John Searle\u2019s famous\u00a0Chinese Room thought experiment, and some thinkers concluded I meant the term to be disparaging or dismissive. On the contrary, I love intuition pumps! That is, some intuition pumps are excellent, some are dubious, and only a few are downright deceptive.One last point on origins, before I dig into the uses of intuition pumps: Dennett clearly says that he didn\u2019t invent intuition pumps, he just coined the term. He actually sees them as the core and legacy of philosophy since the beginning (Plato\u2019s Cave is one of his examples)(Intuition Pumps and Other Tools for Thinking, Dennett, 2013)These are the enduring melodies of philosophy, with the staying power that ensures that students will remember them, quite vividly and accurately, years after they have forgotten the intricate surrounding arguments and analysis. A good intuition pump is more robust than any one version of it.Choosing what to focus onI can\u2019t find the quote I have in mind, but Dennett clearly describes intuition pumps at multiple points as tools for simplification of problems. They gloss over the technical details and the subtleties, homing in on some part of the question.Yet how do we know these are the relevant parts? In principle nothing forbids us to choose any aspect of the question and discard everything else. That\u2019s what Dennett means when reminding us to be careful with intuition pumps: they don\u2019t have to pump the correct intuitions.Still, even when the choice of details is wrong, it can teach us something.(Intuition Pumps and Other Tools for Thinking, Dennett, 2013)Notice that there are two ways an intuition pump may prove valuable. If it\u2019s well made, then either the intuition it pumps are reliable and convincing, in which case it nicely blocks some otherwise tempting path of error, or the intuitions still seem dubious, in which case the intuition pump may help focus attention on what is wrong with its own presuppositions.To summarize:If the simplification actually works and focus on (at least some) relevant details, the intuition pump redirect our thoughts away from the confusion coming from irrelevant detailsIf the simplification doesn\u2019t work, then we get some feedback on which details are missing or more relevant by examining how the intuition pump fails.I see an additional subtlety: Dennett distinguishes well-made intuition pumps that focus on the wrong parts, and badly-made intuition pumps. And my impression is that he proposes ways of verifying \u201cwell-madeness\u201d, not so much which parts to focus on. I want to discuss that after presenting Dennett\u2019s main way of checking the structural integrity of an intuition pump: turning the knobs.Robustness to knob turningThe tool Dennett leverages for checking intuition pumps is called \u201cturning the knobs\u201d, following Hofstader.(Intuition Pumps and Other Tools for Thinking, Dennett, 2013)When Doug Hofstader and I composed\u00a0The Mind\u2019s I back in 1982, he came up with just the right advice on this score: consider the intuition pump to be a tool with many settings, and \u201cturn all the knobs\u201d to see if the same intuitions still get pumped when you consider variations.By this he means changing elements of the story in many different ways, to see whether the intuition stays the same, or shifts with the changes. The claim here is that a good intuition pump (in the sense of well-made) should not be too sensitive to changes that look irrelevant.Searle\u2019s Chinese Room is Dennett\u2019s favorite example of oversensitivity to turning the knobs. In \u201cIntuition Pumps and Other Tools for Thinking\u201d, Dennett focuses on turning the knob about the level of description of the whole system: if you look at the system as a whole, not just the man in the room, and think about what is needed in term of computation to hold a conversation in Chinese, then it\u2019s not obvious anymore that the whole system doesn\u2019t \"understand\" Chinese.(Intuition Pumps and Other Tools for Thinking, Dennett, 2013)Look at what we\u2019ve just done. We\u2019ve turned the knob on Searle\u2019s intuition pump that controls the level of description of the program being followed. There are always many levels. At the highest level, the comprehending powers of the system are not unimaginable; we even get insights into just how the system comes to understand what it does. The system\u2019s reply no longer looks embarrassing; it looks obviously correct. That doesn\u2019t mean that AI of the sort Searle was criticizing actually achieves a level of competence worth calling understanding, nor that those methods, extended in the ways then imagined by those AI researchers, would likely have led to such high competences, but just that Searle\u2019s thought experiment doesn\u2019t succeed in what it claims to accomplish: demonstrating the flat-out impossibility of Strong AI.There are other knobs to turn, but that task has been carried out extensively in the huge literature the Chinese Room has provoked. Here I am concentrating on the thinking tool itself, not the theories and propositions it was aimed at, and showing that it is a defective tool: it persuades by clouding our imagination, not exploiting it well.In the Mind\u2019s I, Hofstader and Dennett describe at least five knobs for the Chinese Room (what sort of matter is the system made of, what is the accuracy of the computation, what is the size of the system, what sort of being is inside the room, and how fast the being can work), and argue that turning them independently lead to variations in the pumped intuitions.\u00a0(The Mind\u2019s I, Dennett and Hofstader, 1981)Each setting of the dials on our intuition pump yields a slightly different narrative, with different problems receding into the background and different morals drawn. Which version or versions should be trusted is a matter to settle by examining them carefully, to see which features of the narrative are doing the work. If the oversimplifications are the source of the intuitions, rather than just devices for suppressing irrelevant complications, we should mistrust the conclusions we are invited to draw. These are matters of delicate judgment, so it is no wonder that a generalized and quite justified suspicion surrounds such exercises of imagination and speculation.Why is sensitivity to turning the knobs so bad? As Dennett and Hofstader write above, if the intuition depends so much on the details, then it probably means it\u2019s created by the details to a large extent. So it\u2019s evidence of a made intuition instead of a revealed intuition. Every intuition is pumped and made to some extent, but we prefer when the bias depends on actual bits of evidence.The way my theoretical computer scientist\u2019s brain thinks about it is through\u00a0smoothed analysis. Without getting into the weeds, smoothed analysis is a form of complexity analysis (time complexity for example) which discards\u00a0isolated costly inputs. Isolated means that whenever you change even a little the input, its cost drops completely. The intuition for not considering those inputs is that you need perfect (often infinite) precision to get them, and any noise in the input will actually remove the costly part.The analogy I see here is with the robustness to change: Dennett wants to discard the intuitions pumped only in isolated or quasi-isolated cases. Or at the very least he\u2019s particularly suspicious of such intuition pumps, because they don\u2019t seem to capture the underlying structure well.To close this section, it\u2019s hard to resist including this witty footnote of Dennett on this exact point(Intuition Pumps and Other Tools for Thinking, Dennett, 2013)Doug zeroed in on the phrase \u201cbits of paper\u201d in Searle\u2019s essay, and showed how it encouraged people to underestimate the size and complexity of the software involved by many orders of magnitude. His commentary on Searle in our book featured this criticism, and led to a ferocious response from Searle (1982) in the pages of the\u00a0New York Review of Books, because, although we had reprinted his article correctly, in his commentary Doug slipped and wrote \u201ca few slips\u201d where Searle had said \u201cbits,\u201d and this, Searle claimed, completely misrepresented his argument! If Searle is right about this, if that small inadvertent mutation transformed the machinery, this actually proved our point, in a way: if such a tiny adjustment disables or enables a thought experiment, that is something that should be drawn to the attention of all whose intuitions are up for pumping.What to do with a robust intuition pump?Let\u2019s assume that we have a decently robust intuition pump; what next? Here I find Dennett harder to follow, not necessarily because he doesn\u2019t have an answer but because the lack of some core idea as clarifying as turning the knob.The easiest case is probably when the intuition pump works for almost all settings of the knobs. In such a situation, it sounds reasonable to consider the intuition as good and use the pump to avoid errors and mistakes.One such example is \u201cDaddy\u2019s a doctor\u201d(Intuition Pumps and Other Tools for Thinking, Dennett, 2013)A young child is asked what her father does, and she answers, \u201cDaddy is a doctor.\u201d Does she believe what she says? In one sense, of course, but what would she have to know to\u00a0really believe it? (What if she\u2019d said, \u201cDaddy is an arbitrager\u201d or \u201cDaddy is an actuary\u201d?) Suppose we suspected that she was speaking without understanding, and decided to test her. Must she be able to produce paraphrases or to expand on her claim by saying her father cures sick people? Is it enough if she knows that Daddy\u2019s being a doctor precludes his being a butcher, a baker, a candlestick maker? Does she know what a doctor is if she lacks the concept of a fake doctor, a quack, an unlicensed practitioner? For that matter, how much does she need to understand to know that Daddy is her father? (Her adoptive father? Her \u201cbiological\u201d father?) Clearly her understanding of what it is to be a doctor, as well of what it is to be a father, will grow over the years, and hence her understanding of he own sentence, \u201cDaddy is a doctor,\u201d will grow. Can we specify \u2014 in a nonarbitrary way \u2014 how much she must know in order to understand this proposition \u201ccompletely\u201d?The intuition pumped here is that understanding, and thus belief, come in degree (in part due to logical non-omniscience). I find that strongly intuitive, but more relevant is the great robustness of this story. It doesn\u2019t have to be about \u201cdoctor\u201d (Dennett himself switches to \u201cfather\u201d in the middle), it doesn\u2019t have to be a child (just someone who still can learn something about the topic at hand, which includes almost everyone if the topic is chosen accordingly). And indeed, I expect that breaking the intuition requires isolated cases like choosing a person who knows everything (literally everything) about topic A.What happens when it\u2019s not as clear cut?One way to think about it is the relative area of different intuitions (in the space of knob parameters). If two or more contradictory intuitions share the space in comparably sized chunks, that means it\u2019s possible to pump contradictory intuitions with well-made pumps.My impression is that Dennett thinks that such instances are ones where intuition pumps shouldn\u2019t be used as arguments. When he criticizes a fallacious intuition pump, he either argues it\u2019s not robust (like the Chinese Room) or that it\u2019s possible to robustly pump the opposite intuition. His arguments against philosophical zombies and the Hard Problem of consciousness are in this latter category.And yet Dennett is rarely content to cancel an intuition pump with another \u2014 he usually believes one of those intuitions is a mistake. One argument he uses is the lack of explanation power.(Intuition Pumps and Other Tools for Thinking, Dennett, 2013)I cannot prove that there is no Hard Problem, and Chalmers can\u2019t prove that there is one. He has one potent intuition going for him, and if it generated some striking new predictions, or promised to explain something otherwise baffling, we might join him in trying to construct a new theory of consciousness around it, but it stands alone, hard to deny but otherwise theoretically inert.Basically, if a robustly pumped intuition leads nowhere, then be wary of it.In any case though, I expect Dennett to say that even a problematic intuition pump can teach us things. It can teach us how not to think, and what mistakes we risk falling for.There\u2019s one sort of intuition pump he doesn\u2019t have any use for, though.Nonsense intuition pumpsDennett has no time for intuition pumps that completely break the laws of physics or biology.(Intuition Pumps and Other Tools for Thinking, Dennett, 2013)But there is also a deeper problem with such experiments. It is child\u2019s play to dream up examples to \u201cprove\u201d further conceptual points. Suppose a cow gave birth to something that was atom-for-atom indiscernible from a shark. Would it be a shark? If you posed that question to a biologist, the charitable reaction would be that you were making a labored attempt at a joke. Or suppose an evil demon could make water turn solid at room temperature by smiling at it; would the demon-water be ice? This is too silly a hypothesis to deserve a response. Smiling demons, cow-sharks, zombies, and Swampmen are all, some philosophers think, logically possible, even if they are not nomologically (causally) possible, and these philosophers think this is important. I do not.[...]\u201cNo,\u201d says the philosopher. \u201cIt\u2019s\u00a0not a false dichotomy!\u00a0For the sake of argument\u00a0we\u2019re suspending the laws of physics. Didn\u2019t Galileo do the same when he banished friction from his thought experiments?\u201d Yes, but a general rule of thumb emerges from the comparison: the utility of a thought experiment is inversely proportional to the size of its departures from reality.Just like isolated intuition pumps, these wildly non-causal one look like they\u2019re missing the actual important stuff, and so shouldn\u2019t be thought of as arguments. But Dennett sounds like he considers the intuition pumped there as just noise.ConclusionDennett\u2019s view on intuition is the only one I know that contains both productive models and tools (turning the knobs) while not falling for the false dichotomy of rejecting or venerating intuition. More than anything, he gives tools for thinking about intuition pumps, tools that still work even if one disagrees about his more object-level points.Still, once the robustness to knob-turning and the relative area of knob parameter space are taken into consideration, I don't feel like I have a perfect grasp on the next step. I expect that this will require me rereading of a bunch of intuition pumps, maybe from other people too.", "title": "The Art and Science of Intuition Pumping", "karma": 16, "af": false, "userid": "ypbkRWpFgPgzvNg3n", "id": "HupjztHSXuKduqaoA"}, {"text": "tl;dr: qualified software engineer considering what their next job might be; now thinking about direct work as a serious option.Previous plan was something like:SELECT * FROM big_tech_co\nWHERE location = 'remote'\nORDER BY team_fit, salary\nFor a variety of reasons, I'm not a huge fan of this plan anymore.New plan:Check the job pages of all the AI alignment orgs I knowCheck 80000 Hours jobs board in case I missed something???Post question on LWI didn't find anything looking at the job pages of the AI alignment orgs that I'm familiar with, and 80000 Hours didn't bring up anything that fit the bill either, so here we are.Me:mission-aligned, long-time member of the community (since ~2013) and meetup organizer (since 2017)fairly strong software engineer, mostly backend-focused but have recently picked up enough React to be able to meaningfully contribute to an existing project on the front-end (no design skills to speak of, yet). Can also rapidly onboard myself to an unfamiliar codebase. Legible artifacts demonstrating these claims beyond a resume include:a non-trivial contribution to major open-source librarygoing from zero to one with a SaaS product on the side while working full-timehappy to travel something like 10% of the time, especially to the Bay, to integrate professionally & sociallyhappy to do something like a part-time work trial (outside of my core working hours, but including weekends), and willing to take a few days off to do this in-person if the fit seems goodin general, happy to do some non-standard things that might not be expected of me at a typical tech job, be agent-y, etc., particularly to compensate for the downsides to an organization of my working remotelyYou:an organization that works either directly on AI alignment, or a \"meta\" org that e.g. better enables others to work on AI alignmentwilling to hire someone on these (or similar) terms!most everything else I'm looking for seems pretty strongly correlated with that, and I'm aware that there aren't exactly a wealth of options to filter onDoes anyone know of any orgs that I might have missed?Lightcone & Redwood don't seem to be hiring remotelyAlignment AI doesn't specify whether they're hiring remotely (but since they're in the UK, you'd imagine they'd say if they were)MIRI & ARC aren't hiring engineersMost of the other orgs I'm familiar with seem to be doing differently-targeted work (i.e. Ought), or are doing work which seems to boil down to \"capabilities advancement\", but I'm open to arguments here if I've misjudged one or more of them.", "title": "Do any AI alignment orgs hire remotely?", "karma": 17, "af": false, "userid": "grecHJcgkb3KW5wnM", "id": "Br6MRY9mQG2b8hie2"}, {"text": "The convoy has been dispersed. Yet Tyler Cowen was clearly correct that this will be a very important event. The story has barely begun.\n\n\n\n\n\n\n\n[Writer\u2019s Note: This post violates some of the usual lines that I set for myself regarding politics, because I do not know how to communicate the situation without doing so, nor does it seem like a situation one can safely ignore. I did my best to keep it to a minimum. As before, I will be staying out of the comments, and ask that related discussions be confined to related posts and conducted to minimize the political component, but I acknowledge that this may not be fully possible in this case and will use discretion.]\n\n\n\nPreviously in Convoy and Convoy Continued, the puzzle was what was happening and what might happen next in response. Authorities were uncertain what to do. The situation was somewhat out of control. Violence was a distinct possibility, or at least the fear of such violence was informing decisions and rhetoric. There were big disputes over the essential nature of the protest, the protesters, what they wanted and why they were there. As far as I could tell the protests were unpopular but that too was disputed. Every edit seemed like it reflected more information from more sources, but more often that not that only meant increased uncertainty.\n\n\n\nMost of that is now resolved. The convoy has been broken up by police without incident and with zero violence (or at least, zero violence that didn\u2019t involve police acting like police usually act), only minor threats to permanently confiscate people\u2019s dogs, in exactly the most basic and obvious way. A few people are still hanging around, but they no longer threaten public order. The three leaders have been arrested. When one engages in civil disobedience and makes oneself intentionally obstructing traffic for weeks, one eventually gets arrested and faces the consequences of being briefly in jail and having a record, and perhaps more if one committed more serious crimes and the state can convince a jury of that. That\u2019s how it\u2019s supposed to work. There were less than 200 arrests and they towed less than 50 trucks.\n\n\n\nThere\u2019s just one little problem.\n\n\n\nBefore breaking up the protest, as I noted last time, Prime Minister Trudeau1 invoked the Emergencies Act.\n\n\n\nThe Emergencies Act may have been a little bit about the need to commandeer towing capacity, but we can now be very certain what it was centrally about.\n\n\n\nIt was about money.\n\n\n\nIn particular, it was about giving the government of Canada the permanent power to freeze, without trial or legal recourse, all the bank accounts and other assets of anyone it decides was \u2018directly or indirectly involved\u2019 in an \u2018illegal protest.\u2019 In practice this translates to \u2018freeze the assets of the families of anyone that power or the government dislikes.\u2019\n\n\n\nIn particular, this clip makes it very clear that it is intended to include anyone who donated to either of their crowdfunding campaigns. If you give the wrong amount of money to support a protest the Canadian government decides it dislikes, they may well freeze your bank accounts and you will have no recourse even if it was in error. In that clip they also say that they are doing this without the Democratic process in part \u2018because an opposition party exists to frustrate any government attempt to pass legislation.\u2019\n\n\n\nIn pursuit of this, they have tasked not only the banks but a wide variety of payment processors and others who handle money, including those handling cryptocurrency, with spying on their customers to determine which ones participated in activities disapproved of by power or the government, and freezing their accounts.\n\n\n\nIf one\u2019s accounts are frozen and one is both left without resources and without the ability to transact, it is at best extremely difficult to participate in society. One cannot easily hold a job or raise a family, and buying a tomato may prove tricky.\n\n\n\n\n\n\n\nFamily members having trouble living their lives is being treated not as a bug but as a feature. The sins of the father are to be laid upon the children, it seems.\n\n\n\nThis extends as noted above to those who provide financial assistance to those engaging in disapproved activities, and that such retaliation will continue to happen after the activities in question cease, so not only is one without one\u2019s money and other assets, and without the ability to spend what one does have, others may reasonably fear that helping you not end up on the street might land them in the same situation.\n\n\n\nMeanwhile, our systems are moving steadily away from cash or any other way to pay for things if one\u2019s accounts are frozen, including the mortgage or the rent. Ending up losing everything is a real possibility.\n\n\n\nAgain, this is happening in many cases without anyone ever being charged with any crime. Even when crimes are charged, even if the government gets convictions, the punishment here is orders of magnitude out of line and sets the worst kind of precedent. If such folks are guilty of treason or rebellion or terrorism, then charge them with that and see what a jury has to say.\n\n\n\nAs for those who vote against this policy, here\u2019s what Justin Trudeau has to say, notice what things he totally does not sound like at all, no sir (video is at the link, 38sec).\n\n\n\n\n\n\n\nFreedom to Transact\n\n\n\nWithout the freedom to transact there is no freedom.\n\n\n\nThis is the reality that Canada, and perhaps soon the rest of the world, now faces.\n\n\n\nThis thread is long (thread unroller version here), but it is important. I have never seen a Twitter thread be retweeted with explicit endorsement by so many accounts that I respect, including many that I would not normally expect to understand how toxic and dangerous the situation threatens to become. Here are some examples of what people said when retweeting.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make damn sure, I will put it here in its entirety. If you have not yet already, please read the whole thing. I would quibble over some minor details as I of course always would, but they matter little. This is the message everyone needs to hear.\n\n\n\nI turn the floor over.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThey did indeed issue a court order to freeze users\u2019 Bitcoins in self-custodial wallets, giving one of those great opportunities to note that this is not how any of this works.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAftermath\n\n\n\nWhen 6529 wrote that thread, the convoy was still a going concern. It was still plausible that however awful the precedent set by the financial authoritarianism and disregard for the rule of law, that one could be sufficiently terrified of a physical solution to think that such actions were necessary or at least useful.\n\n\n\nI was already confident of quite the opposite. Taking these actions galvanized people\u2019s worst fears throughout not only Canada but the world. However much forceful dissent and distrust of the system was present before, it will doubtless be much, much more present going forward. The only way that is in the government\u2019s interest is if it wants this fight in order to further suspend freedom and democracy and destroy the rule of law that much faster.\n\n\n\nTo the extent these people are capable of having points at all, the suspension of freedom and democracy and the destruction of rule of law is the point. Setting a precedent that the government can cast out your entire family and anyone helping you on a whim is the point.\n\n\n\nHow can I be confident that we know this?\n\n\n\nThe protests were broken up only days later by arresting those present and towing trucks. Those invoking these financial penalties and requirements doubtless knew that this was the plan. Given that this was the plan, the financial measures were clearly completely unnecessary \u2013 things would have played out on the ground in exactly the same way.\n\n\n\nThe financial actions doubtless made it harder to hire lawyers for those arrested, but that hardly seems like a motivation that should make us feel better.\n\n\n\nThe authorities said that they would be imposing these measures retroactively on people who left voluntarily and thus avoided being arrested.\n\n\n\nThat is the opposite of what you do to break up a protest, because you withdraw the motive to go home. I can\u2019t help but notice the penalty for lateness is death when they intend to actively hunt after the protestors who leave without any need to arrest them.\n\n\n\n\n\n\n\n\n\n\n\nThe authorities said after the protests were broken up that the situation that required the Act was still present. What else in the world could this be referring to? Is this a permanent emergency because people might try something again at some point in the future? Or was the thing that required the act the institution of these financial measures on a permanent basis?\n\n\n\nPart of my explanation for this is motive ambiguity. One shows one\u2019s dedication to the cause by intentionally inflicting maximum harm, thus proving one is not going to be distracted by worries about morality or what third parties might come to harm. The more damage one can do, the better the demonstration \u2013 the cruelty is the point, not because one values cruelty, but to show one is not against cruelty or not supporting freedom, the best proof of which is to go as far as possible in the other direction.\n\n\n\nI say \u2018to the extent these people are capable of having points at all\u2019 because they importantly have lost at least some and perhaps all ability to have points. This is important.\n\n\n\nThree Sides to Every Story\n\n\n\nThe counterargument is presumably some form of \u2018they know not what they do\u2019 where those involved are so lost they have no ability to think or reason on such levels, and they are doing things of the form \u2018convoy bad, people no like convoy, what tools we have to hurt convoy\u2019 without any model of why such actions would accomplish anything or much care about whether they do. Slash perhaps the idea that one must Take Action and this is action, demands by various enforcers for More Power and More Authority to Do Something, with or without the general instinct that fear will keep the local systems in line without thinking that anything will slip through their fingers.\n\n\n\nIn that model, these dynamics need not involve much agency or intent. The thing ramps up continuously on its own through people following local incentive gradients, resulting in a state that no one especially wanted, but also that they did not concern themselves much with avoiding until it was far too late and the frog was already boiled to death. On my stack is writing the post Policy Debates Should Appear One Sided (the flip side of this), but (and also because) there is still always (at minimum) another side to the story.\n\n\n\nI noted last time that the reason Trudeau and the Canadian Government seemed to be so taken by surprise, had so much paralysis in terms of getting together a physical response and ended up flailing around with financial assets was because such people have often lost the ability to reason at all about underlying physical reality.\n\n\n\nThus I frame the heart of the conflict between different simulacra levels. The fight is between those who live primarily in simulacra levels one, where they interact with and attempt to model and alter physical reality, and those at simulacra levels three, where one cares mostly about signaling group memberships and loyalties, and often also level four where they end up using vibes and associations of various level three moves in an instinctual way that they have learned is associated with success, and lose all ability to reason or make plans in a coherent way at all, let alone model the physical world.\n\n\n\nAn alternative framing of a similar divide by The Upheaval is \u2018the Physicals versus the Virtuals.\u2019 In this divide, The Virtuals deal with information and effectively form a ruling class, but because they still depend on physical reality they must continue to rely on the Physicals, and we all know how such science fiction tales usually end.\n\n\n\nBut have a little sympathy for them: they do this not just because it is cynically convenient (though it is), but because this is literally the only way they know how to navigate and influence the world. The post-modern fish swims in a narrative sea, and their first reaction is always to try to control it (through what the CCP calls \u201cdiscourse power\u201d) because at heart they well and truly believe in the idea of the \u201csocial construction of reality,\u201d as Lasch pointed out in the quote at top. If there is no fixed, objective truth, only power, then the mind\u2019s will rules the world. Facts can be reframed as needed to create the story that best produces the correct results for Progress (this is why you will find journalists are now professionally obsessed with \u201cstorytelling\u201d rather than reporting facts).\u2026That Trudeau\u2019s government would choose to jettison any remaining illusion of Canada still being a liberal democracy just to harm their political class enemies isn\u2019t too surprising. It\u2019s their method of doing so that is particularly striking: control over digital financial assets is pretty much the ultimate leverage now available to the Virtuals. We should expect more use of this tool around the world anywhere the Physicals continue to revolt against their masters.\n\n\n\nThe Physicals must be concerned with non-socially-constructed Truth because otherwise reality will bite them in the ass. The Virtuals have their asses covered, so they are unconcerned.\n\n\n\nThere is a lot of merit in this framing, but it still seems important that those who are concerned with Truth and who do not wish it to be socially constructed should then side with the Physicals, even if in their world they deal with information. And indeed, the first reply to the person who linked me to this says \u2018I am a Virtual yet still side with the protesters.\u2019\n\n\n\nAnother recent version of the same thing was, yes, wordcels and shape rotators. Shape rotators concern themselves with reality, wordcels only with symbols.\n\n\n\nAnd mysteriously, despite information being what most of Twitter and everyone I know do all day, everyone wants to think of themselves as a shape rotator.\n\n\n\nConsider:\n\n\n\nLoose Isomorphisms\n\n\n\nThe rotator  wordcel axis also happens to map to some other common ones. I might expand on these later but I\u2019ll just list them for now.\n\n\n\nspacing guild v. bene gesserit\n\n\n\nautism v. schizophrenia\n\n\n\nsan francisco v. new york\n\n\n\nintuition v. formalism\n\n\n\nempiricist v. rationalist\n\n\n\ndeep learning v. crypto\n\n\n\ncapitalists v. socialists\n\n\n\napolitical v. political\n\n\n\ngeometers v. algebraists\n\n\n\nI am confident that the person who wrote that list thinks of the lefthand shape-rotator side as \u2018the good side\u2019 and the righthand wordcel side as \u2018the bad side.\u2019\n\n\n\nHere\u2019s one of them as text, by the author of the piece above:\n\n\n\n\n\n\n\nThe list is a broad-based statement of values and world models. It is a claim that empiricists make the important discoveries and build great world-changing capitalist companies using their intuition to work on geometrically-based deep learning algorithms while all having autism and living in San Francisco and staying apolitical. In their spare time they read Dune and root for the spacing guild.\n\n\n\nWhereas when Vitalik saw this he said \u2018wait crypto is shape rotation,\u2019 the same way I see it and think \u2018New York and rationalist (and also empiricist though) are shape rotation and San Francisco is wordcel.\u2019 And if you ask a socialist familiar with the terms, I predict most of them will claim socialism is shape rotation and capitalism is wordcel.\n\n\n\nWho is a \u2018real American\u2019 versus whatever the other option is? I\u2019ll tell you who isn\u2019t not one, it\u2019s whichever American is speaking. You know who is \u2018out of touch\u2019? Not my friends and allies, that\u2019s who. There is a right amount of touch to be in, it isn\u2019t maximal, but that is never what people are debating. Same principle.\n\n\n\nThe initial dialectic tactic was to take a superficial division between one simulacra-3/4 alliance of wordcels/virtuals/politicians/whatevers and another that is also that, and frame it as your side being spare rotators/physicals/outsiders/whichevers, and that everyone has to pick a side or the awful other side full of wordcels/virtuals/politicians will win.\n\n\n\nThe new tactic is to deny that there were ever any meaningful physical reality at all.\n\n\n\nFraming this as a battle between teams, of course, is playing into the whole idea of socially constructed reality and virtual wordcels operating on Simulacra-3, whereas the whole point of noticing the divide is to do the other thing. Putting more of the right people in your coalition or having the right symbolic associations does not make one right \u2013 or if you think it does, then you have already chosen your side.\n\n\n\nIn this context, we can revisit the Tyler Cowen concept of looking for the strong analytical thinkers.\n\n\n\nLook for strong analytical abilities, and if you don\u2019t see it, run the other way.\n\n\n\nNow it makes more sense, although it risks continuing to play into the underlying problem by looking at coalitions and choosing sides on that basis.\n\n\n\nIn that light, it is important to note that this principle has now reversed itself.\n\n\n\nWhen one looks at the original convoy, one indeed struggles to find strong analytical thinkers.\n\n\n\nYet the same can be said for Trudeau and the rest of the government response. In no way does it reflect strong analytical thinking or thinkers.\n\n\n\nThe Canadian government, not the convoy, is the relevant actor here. It no longer much matters how it started, only how it\u2019s going.\n\n\n\nIn the early stages, the strong analytical thinkers had a wide variety of reactions to the situation. But you know what all the strong analytical thinkers are doing now, after the freezing of bank accounts?\n\n\n\nAs far as I can tell, at least in the informational world I have constructed for myself, all of them stand united. I have not seen a single attempt, however weak, to defend the actions taken or even to claim they are of minimal importance.\n\n\n\nIncluding people whose views mostly differ strongly from most others in my orbit. Whatever they think off arresting the protestors and of the original convoy, everyone\u2019s reaction to the freezing of accounts is the same, and looks like some version of these examples.\n\n\n\n\n\n\n\n\\\n\n\n\n\n\n\n\nIt might be time to run the other way, and perhaps shout it from the rooftops.\n\n\n\nWhat To Do Now?\n\n\n\nThere are two categories of things to do, the general and the personal, with some amount of overlap.\n\n\n\nThe general would involve political action, with some combination of education, persuasion, awareness raising, voice raising, lobbying, working with those who set such policies in various capacities, academic work, voting, lawyers and so on to combat the rise of such authoritarian policies and protect the freedom to transact, the rule of law, the right to due process and so forth. Hopefully without getting your accounts frozen. Others can speak better than I can on this, but I will note that I consider this policy debate sufficiently one-sided that I would support it being what the EAs call a \u2018cause area.\u2019\n\n\n\nThe personal involves protecting yourself and those you care about against the threat of such actions coming for you. At some point in the future, this could happen to you. What is acceptable now may not be acceptable in the future, in a \u2018first they come\u2019 kind of way, and such winds can change quite fast. Or at a minimum, the need to ensure that this does not happen will constrain your behavior and your freedom and cause you much stress.\n\n\n\nIt need not be about taking explicit political or other action. Sometimes this will be a pure misunderstanding or other error. People get frozen out because of mistaken identity or an \u2018abundance of caution.\u2019 A very good friend of mine had it happen due to a tax dispute in which the government was neither correct nor communicative and it caused tons of trouble, although this has since been cleared up.\n\n\n\nThus, the value of taking actions to minimize one\u2019s vulnerability to such actions was already important, and seems that much more important. Think about what would happen if your assets, and those of your family, were frozen right now. Your bank accounts are gone, your credit cards do not work. You can\u2019t get new ones. What would you do? How would you keep a roof over your head and pay for food?\n\n\n\nEnsure that you have answers to those questions. There are a variety of ways to at least partially protect yourself, including self-custodial cryptocurrency and cold hard cash or other physical assets, especially ones that travel.\n\n\n\nI\u2019ve seen a bunch of \u2018guess it\u2019s time to get into crypto\u2019 sentiment recently, and I am surprised it was not reflected more in prices. For example:\n\n\n\n\n\n\n\n\n\n\n\n(Nothing here is ever investment advice and that goes double for crypto, but in case anyone was confused about this, such posts are Not The Way. Asking the internet in general is definitely not the way to get good information or advice on such matters. Find someone you trust who knows their stuff and can walk you through it or point to a trustworthy guide.)\n\n\n\nOne additional important way to deal with this, that is a complement to your own safe assets, is to have people you can trust to come to your aid, despite the situation. Having true friends and family at your side is one of the best defenses.\n\n\n\nAt Substack the site has expressed a strong commitment to resisting censorship, but they use Stripe as their sole payment processor, and Stripe has made it clear they play ball. Getting cut off from writing income is not the same as having one\u2019s bank accounts frozen, but it is a clear point of vulnerability especially if Stripe was told to cut off the entire site until the disapproved content was taken down. It needs to be fixed.\n\n\n\nIs it a pain to have three distinct copies of every post in different places? Yes it is quite the pain especially when I need to edit to fix mistakes. Yet I do not make one or two of them into link posts, because this way provides a robust backup system. In order to take me down, one would need to take down all three copies.\n\n\n\nAdditional Convoy Links and Thoughts\n\n\n\nSame as it ever was, the Canadian Parliament delayed their session evaluating the invoking of the Emergencies Act due to the emergency the act was invoked against.\n\n\n\nRather than decry such moves, instead mainstream media has joined the move against those who donated money.\n\n\n\nFor example:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGood to see Representative Ilhan Omar standing up for the obvious even with rolls reversed, which is sadly a rarity these days. Such principle is not by default rewarded. Replies are full of demands that she do things such as \u2018stay in her lane.\u2019 Getting praise from Fox News does not exactly make her life better.\n\n\n\nThe journalist in question moved her tweets to protected before I could write this post, losing us a number of details, which is my fault for not getting the screenshots right away.\n\n\n\nThere is also the obvious, which is that the word \u2018freedom\u2019 has now been associated with the convoy, and has always meant freedom, from the Washington Post editorial page:\n\n\n\n\n\n\n\nThe logic in the post, as far as I can tell is:\n\n\n\nCanada (and the USA) restricted the freedom of non-white people.Thus, freedom is white.Thus, a claim of entitlement to freedom is white supremacy.Freedom is slavery, and presumably ignorance is strength.\n\n\n\nThe response to this was the stage-1-clown-makeup that no one actually believes this, it\u2019s some sort of ingroup virtue signaling cascade causing such words to be written, but somehow that does not bring me comfort. It brings up the question a commenter asked me a while ago about why New York Times is on the banned list but other places like Washington Post are acceptable. I\u2019d like to say it\u2019s a matter of degree and that I\u2019ve found Washington Post\u2019s news reporting to mostly be acceptable, and also editorials mostly don\u2019t count it\u2019s fine to air different points of view, but \u2018Scott Alexander was my friend and it is personal\u2019 is doubtless doing a bunch of work.\n\n\n\nFrom January 10, Unheard lets some participants talk. Big \u2018and you can\u2019t make me\u2019 energy combined with \u2018well actually sure you can, I need my truck.\u2019\n\n\n\nPirate Wires covers the situation before the full \u2018freeze all the bank accounts\u2019 plan became clear, is mostly already there anyway.\n\n\n\nCommon Sense was there a while ago, and is here to remind us.\n\n\n\nThe Canadian government put this out on February 18, after doing all this and while they were breaking up the protests with arrests.\n\n\n\n\n\n\n\nStraight. Face.\n\n\n\nWe also have this video with his remarks now versus his remarks regarding the Farmers\u2019 Protests in India. Again, straight face.\n\n\n\nI guess the faith in freedom was all in the beard.\n\n\n\n1\n\n\n\nJustin means \u2018just, upright or righteous.\u2019 I checked for normative determinism regarding Trudeau and no one knows the origin but to me it seems to be obviously \u2018true water\u2019 from the French \u2018tru\u2019 and \u2018deau\u2019 from which here clearly represents the righteous filtering out of the undesired or impure.", "title": "Convoy Crackdown", "karma": 94, "af": false, "userid": "N9zj5qpTfqmbn9dro", "id": "F5EmBWyryhgNtqsKb"}, {"text": "This is a transcript of a conversation between Richard Ngo and Eliezer Yudkowsky, facilitated by Nate Soares (and with some comments from Carl Shulman). This transcript continues the Late 2021 MIRI Conversations sequence, following Ngo's view on alignment difficulty.\u00a0Color key:\u00a0Chat by Richard and Eliezer\u00a0\u00a0Other chat\u00a0\u00a014. October 4 conversation\u00a014.1. Predictable updates, threshold functions, and the human cognitive range\u00a0[Ngo][15:05]\u00a0Two questions which I'd like to ask Eliezer:1. How strongly does he think that the \"shallow pattern-memorisation\" abilities of GPT-3 are evidence for Paul's view over his view (if at all)2. How does he suggest we proceed, given that he thinks directly explaining his model of the chimp-human difference would be the wrong move?[Yudkowsky][15:07] \u00a01 - I'd say that it's some evidence for the Dario viewpoint which seems close to the Paul viewpoint. \u00a0I say it's some evidence for the Dario viewpoint because Dario seems to be the person who made something like an advance prediction about it. \u00a0It's not enough to make me believe that you can straightforwardly extend the GPT architecture to 3e14 parameters and train it on 1e13 samples and get human-equivalent performance.[Ngo][15:09] \u00a0Did you make any advance predictions, around the 2008-2015 period, of what capabilities we'd have before AGI?[Yudkowsky][15:10] \u00a0not especially that come to mind? \u00a0on my model of the future this is not particularly something I am supposed to know unless there is a rare flash of predictability.[Ngo][15:11] \u00a01 - I'd say that it's some evidence for the Dario viewpoint which seems close to the Paul viewpoint. I say it's some evidence for the Dario viewpoint because Dario seems to be the person who made something like an advance prediction about it. It's not enough to make me believe that you can straightforwardly extend the GPT architecture to 3e14 parameters and train it on 1e13 samples and get human-equivalent performance.For the record I remember Paul being optimistic about language when I visited OpenAI in summer 2018. But I don't know how advanced internal work on GPT-2 was by then.[Yudkowsky][15:13] \u00a02 - in lots of cases where I learned more specifics about X, and updated about Y, I had the experience of looking back and realizing that knowing anything specific about X would have predictably produced a directional update about Y. \u00a0like, knowing anything in particular about how the first AGI eats computation, would cause you to update far away from thinking that biological analogies to the computation consumed by humans were a good way to estimate how many computations an AGI needs to eat. \u00a0you know lots of details about how humans consume watts of energy, and you know lots of details about how modern AI consumes watts, so it's very visible that these quantities are so incredibly different and go through so many different steps that they're basically unanchored from each other.I have specific ideas about how you get AGI that isn't just scaling up Stack More Layers, which lead me to think that the way to estimate the computational cost of it is not \"3e14 parameters trained at 1e16 ops per step for 1e13 steps, because that much computation and parameters seems analogous to human biology and 1e13 steps is given by past scaling laws\", a la recent OpenPhil publication. \u00a0But it seems to me that it should be possible to have the abstract insight that knowing more about general intelligence in AGIs or in humans would make the biological analogy look less plausible, because you wouldn't be matching up an unknown key to an unknown lock.Unfortunately I worry that this depends on some life experience with actual discoveries to get something this abstract-sounding on a gut level, because people basically never seem to make abstract updates of this kind when I try to point to them as predictable directional updates?But, in principle, I'd hope there would be aspects of this where I could figure out how to show that any knowledge of specifics would probably update you in a predictable direction, even if it doesn't seem best for Earth for me to win that argument by giving specifics conditional on those specifics actually being correct, and it doesn't seem especially sound to win that argument by giving specifics that are wrong.[Ngo][15:17] \u00a0I'm confused by this argument. Before I thought much about the specifics of the chimpanzee-human transition, I found the argument \"humans foomed (by biological standards) so AIs will too\" fairly compelling. But after thinking more about the specifics, it seems to me that the human foom was in part caused by a factor (sharp cultural shift) that won't be present when we train AIs.[Yudkowsky][15:17] \u00a0sure, and other factors will be present in AIs but not in humans[Ngo][15:17] \u00a0This seems like a case where more specific knowledge updated me away from your position, contrary to what you're claiming.[Yudkowsky][15:18] \u00a0eg, human brains don't scale and mesh, while it's far more plausible that with AI you could just run more and more of itthat's a huge factor leading one to expect AI to scale faster than human brains didit's like communication between humans, but squared!this is admittedly a specific argument and I'm not sure how it would abstract out to any specific argument[Ngo][15:20] \u00a0Again, this is an argument that I believed less after looking into the details, because right now it's pretty difficult to throw more compute at neural networks at runtime.Which is not to say that it's a bad argument, the differences in compute-scalability between humans and AIs are clearly important. But I'm confused about the structure of your argument that knowing more details will predictably update me in a certain direction.[Yudkowsky][15:21] \u00a0I suppose the genericized version of my actual response to that would be, \"architectures that have a harder time eating more compute are architectures which, for this very reason, are liable to need better versions invented of them, and this in particular seems like something that plausibly happens before scaling to general intelligence is practically possible\"[Soares][15:23] \u00a0(Eliezer, I see Richard as requesting that you either back down from, or clarify, your claim that any specific observations about how much compute AI systems require will update him in a predictable direction.)[Ngo: \ud83d\udc4d][Yudkowsky][15:24] \u00a0I'm not saying I know how to make that abstractized argument for exactly what Richard cares about, in part because I don't understand Richard's exact model, just that it's one way to proceed past the point where the obvious dilemma crops up of, \"If a theory about AGI capabilities is true, it is a disservice to Earth to speak it, and if a theory about AGI capabilities is false, an argument based on it is not sound.\"[Ngo][15:25] \u00a0Ah, I see.[Yudkowsky][15:26] \u00a0possible viewpoint to try: that systems in general often have threshold functions as well as smooth functions inside them.only in ignorance, then, do we imagine that the whole thing is one smooth function.the history of humanity has a threshold function of, like, communication or culture or whatever.the correct response to this is not, \"ah, so this was the unique, never-to-be-seen-again sort of fact which cropped up in the weirdly complicated story of humanity in particular, which will not appear in the much simpler story of AI\"this only sounds plausible because you don't know the story of AI so you think it will be a simple storythe correct generalization is \"guess some weird thresholds will also pop up in whatever complicated story of AI will appear in the history books\"[Ngo][15:28] \u00a0Here's a quite general argument about why we shouldn't expect too many threshold functions in the impact of AI: because at any point, humans will be filling in the gaps of whatever AIs can't do. (The lack of this type of smoothing is, I claim, why culture was a sharp threshold for humans - if there had been another intelligent species we could have learned culture from, then we would have developed more gradually.)[Yudkowsky][15:30] \u00a0something like this indeed appears in my model of why I expect not much impact on GDP before AGI is powerful enough to bypass human economies entirelyduring the runup phase, pre-AGI won't be powerful to do \"whole new things\" that depend on doing lots of widely different things that humans can't dojust marginally new things that depend on doing one thing humans can't do, or can do but a bunch worse[Ngo][15:31] \u00a0Okay, that's good to know.Would this also be true in a civilisation of village idiots?[Yudkowsky][15:32]\u00a0there will be sufficient economic reward for building out industries that are mostly human plus one thing that pre-AGI does, and people will pocket those economic rewards, go home, and not be more ambitious than that. \u00a0I have trouble empathically grasping why almost all the CEOs are like this in our current Earth, because I am very much not like that myself, but observationally, the current Earth sure does seem to behave like rich people would almost uniformly rather not rock the boat too much.I did not understand the whole thing about village idiots actuallydo you want to copy and paste the document, or try rephrasing the argument?[Ngo][15:35] \u00a0Rephrasing:Claim 1: AIs will be better at doing scientific research (and other similar tasks) than village idiots, before we reach AGI.Claim 2: Village idiots still have the core of general intelligence (which you claim chimpanzees don't have).Claim 3: It would be surprising if narrow AI's research capabilities fell specifically into the narrow gap between village idiots and Einsteins, given that they're both general intelligences and are very similar in terms of architecture, algorithms, etc.(If you deny claim 2, then we can substitute, say, someone at the 10th percentile of human intelligence - I don't know what specific connotations \"village idiot\" has to you.)[Yudkowsky][15:37] \u00a0My models do not have an easy time of visualizing \"as generally intelligent as a chimp, but specialized to science research, gives you superhuman scientific capability and the ability to make progress in novel areas of science\".(this is a reference back to the pre-rephrase in the document)it seems like, I dunno, \"gradient descent can make you generically good at anything without that taking too much general intelligence\" must be a core hypothesis there?[Ngo][15:39] \u00a0I mean, we both agree that gradient descent can produce some capabilities without also producing much general intelligence. But claim 1 plus your earlier claims that narrow AIs won't surpass humans at scientific research, lead to the implication that the limitations of gradient-descent-without-much-general-intelligence fall in a weirdly narrow range.[Yudkowsky][15:42] \u00a0I do credit the Village Idiot to Einstein Interval with being a little broader as a target than I used to think, since the Alpha series of Go-players took a couple of years to go from pro to world-beating even once they had a scalable algorithm. \u00a0Still seems to me that, over time, the wall clock time to traverse those ranges has been getting shorter, like Go taking less time than Chess. \u00a0My intuitions still say that it'd be quite weird to end up hanging out for a long time with AGIs that conduct humanlike conversations and are ambitious enough to run their own corporations while those AGIs are still not much good at science.But on my present model, I suspect the limitations of \"gradient-descent-without-much-general-intelligence\" to fall underneath the village idiot side?[Ngo][15:43] \u00a0Oh, interesting.That seems like a strong prediction[Yudkowsky][15:43] \u00a0Your model, as I understand it, is saying, \"But surely, GD-without-GI must suffice to produce better scientists than village idiots, by specializing chimps on science\" and my current reply, though it's not a particular question I've thought a lot about before, is, \"That... does not quite seem to me like a thing that should happen along the mainline?\"though, as always, in the limit of superintelligences doing things, or our having the Textbook From The Future, we could build almost any kind of mind on purpose if we knew how, etc.[Ngo][15:44] \u00a0For example, I expect that if I prompt GPT-3 in the right way, it'll say some interesting and not-totally-nonsensical claims about advanced science.Whereas it would be very hard to prompt a village idiot to do the same.[Yudkowsky][15:44] \u00a0eg, a superintelligence could load up chimps with lots of domain-specific knowledge they were not generally intelligent enough to learn themselves.ehhhhhh, it is not clear to me that GPT-3 is better than a village idiot at advanced science, even in this narrow sense, especially if the village idiot is allowed some training[Ngo][15:46] \u00a0It's not clear to me either. But it does seem plausible, and then it seems even more plausible that this will be true of GPT-4[Yudkowsky][15:46]\u00a0I wonder if we're visualizing different village idiotsmy choice of \"village idiot\" originally was probably not the best target for visualization, because in a lot of cases, a village idiot - especially the stereotype of a village idiot - is, like, a damaged general intelligence with particular gears missing?[Ngo][15:47] \u00a0I'd be happy with \"10th percentile intelligence\"[Yudkowsky][15:47] \u00a0whereas it seems like what you want is something more like \"Homo erectus but it has language\"oh, wow, 10th percentile intelligence?that's super highGPT-3 is far far out of its league[Ngo][15:49]\u00a0I think GPT-3 is far below this person's league in a lot of ways (including most common-sense reasoning) but I become much less confident when we're talking about abstract scientific reasoning.[Yudkowsky][15:51] \u00a0I think that if scientific reasoning were as easy as you seem to be imagining(?), the publication factories of the modern world would be much more productive of real progress.[Ngo][15:51] \u00a0Well, a 10th percentile human is very unlikely to contribute to real scientific progress either way[Yudkowsky][15:53] \u00a0Like, on my current model of how the world really works, China pours vast investments into universities and sober-looking people with PhDs and classes and tests and postdocs and journals and papers; but none of this is the real way of Science which is actually, secretly, unbeknownst to China, passed down in rare lineages and apprenticeships from real scientist mentor to real scientist student, and China doesn't have much in the way of lineages so the extra money they throw at stuff doesn't turn into real science.[Ngo][15:52] \u00a0Can you think of any clear-cut things that they could do and GPT-3 can't?[Yudkowsky][15:53] \u00a0Like... make sense... at all? \u00a0Invent a handaxe when nobody had ever seen a handaxe before?[Ngo][15:54] \u00a0You're claiming that 10th percentile humans invent handaxes?[Yudkowsky][15:55] \u00a0The activity of rearranging scientific sentences into new plausible-sounding paragraphs is well within the reach of publication factories, in fact, they often use considerably more semantic sophistication than that, and yet, this does not cumulate into real scientific progress even in quite large amounts.I think GPT-3 is basically just Not Science Yet to a much greater extent than even these empty publication factories.If 10th percentile humans don't invent handaxes, GPT-3 sure as hell doesn't.[Ngo][15:55] \u00a0I don't think we're disagreeing. Publication factories are staffed with people who do better academically than 90+% of all humans.If 90th-percentile humans are very bad at science, then of course GPT-3 and 10th-percentile humans are very very bad at science. But it still seems instructive to compare them (e.g. on tasks like \"talk cogently about a complex abstract topic\")[Yudkowsky][15:58] \u00a0I mean, while it is usually weird for something to be barely within a species's capabilities while being within those capabilities at all, such that only relatively smarter individual organisms can do it, in the case of something that a social species has only very recently started to do collectively, it's plausible that the thing appeared at the point where it was barely accessible to the smartest members. \u00a0Eg, it wouldn't be surprising if it would have taken a long time or forever for humanity to invent science from scratch, if all the Francis Bacons and Newtons and even average-intelligence people were eliminated leaving only the bottom 10%. \u00a0Because our species just started doing that, at the point where our species was barely able to start doing that, meaning, at the point where some rare smart people could spearhead it, historically speaking. \u00a0It's not obvious whether or not less smart people can do it over a longer time.I'm not sure we disagree much about the human part of this model.My guess is that our disagreement is more about GPT-3.\"Talk 'cogently' about a complex abstract topic\" doesn't seem like much of anything significant to me, if GPT-3 is 'cogent'. \u00a0It fails to pass the threshold for inventing science and, I expect, for most particular sciences.[Ngo][16:00] \u00a0How much training do you think a 10th-percentile human would need in a given subject matter (say, economics) before they could answer questions as well as GPT-3 can?(Right now I think GPT-3 does better by default because it at least recognises the terminology, whereas most humans don't at all.)[Yudkowsky][16:01] \u00a0I also expect that if you offer a 10th-percentile human lots of money, they can learn to talk more cogently than GPT-3 about narrower science areas. \u00a0GPT-3 is legitimately more well-read at its lower level of intelligence, but train the 10-percentiler in a narrow area and they will become able to write better nonsense about that narrow area.[Ngo][16:01] \u00a0This sounds like an experiment we can actually run.[Yudkowsky][16:02] \u00a0Like, what we've got going on here is a real breadth advantage that GPT-3 has in some areas, but the breadth doesn't add up because it lacks the depth of a 10%er.[Ngo][16:02] \u00a0If we asked them to read a single introductory textbook and then quiz both them and GPT-3 about items covered in that textbook, do you expect that the human would come out ahead?[Yudkowsky][16:02]\u00a0AI has figured out how to do a subhumanly shallow kind of thinking, and it is to be expected that when AI can do anything at all, it can soon do more of that thing than the whole human species could do.No, that's nothing remotely like giving the human the brief training the human needs to catch up to GPT-3's longer training.A 10%er does not learn in an instant - they learn faster than GPT-3, but not in an instant.This is more like a scenario of paying somebody to, like, sit around for a year with an editor, learning how to mix-and-match economics sentences until they can learn to sound more like they're making an argument than GPT-3 does, despite still not understanding any economics.A lot of the learning would just go into producing sensible-sounding nonsense at all, since lots of 10%ers have not been to college and have not learned how to regurgitate rearranged nonsense for college teachers.[Ngo][16:05]\u00a0What percentage of humans do you think could learn to beat GPT-3's question-answering by reading a single textbook over, say, a period of a month?[Yudkowsky][16:06] \u00a0\u00af\\_(\u30c4)_/\u00af[Ngo][16:06] \u00a0More like 0.5 or 5 or 50?[Yudkowsky][16:06] \u00a0Humans cannot in general pass the Turing Test for posing as AIs!What percentage of humans can pass as a calculator by reading an arithmetic textbook?Zero![Ngo][16:07] \u00a0I'm not asking them to mimic GPT-3, I'm asking them to produce better answers.[Yudkowsky][16:07] \u00a0Then it depends on what kind of answers!I think a lot of 10%ers could learn to do wedding-cake multiplication, if sufficiently well-paid as adults rather than being tortured in school, out to 6 digits, thus handily beating the current GPT-3 at 'multiplication'.[Ngo][16:08] \u00a0For example: give them an economics textbook to study for a month, then ask them what inflation is, whether it goes up or down if the government prints more money, whether the price of something increases or decreases when the supply increases.[Yudkowsky][16:09]\u00a0GPT-3 did not learn to produce its responses by reading textbooks.You're not matching the human's data to GPT-3's data.[Ngo][16:10] \u00a0I know, this is just the closest I can get in an experiment that seems remotely plausible to actually run.[Yudkowsky][16:10] \u00a0You would want to collect, like, 1,000 Reddit arguments about inflation, and have the human read that, and have the human produce their own Reddit arguments, and have somebody tell them whether they sounded like real Reddit arguments or not.The textbook is just not the same thing at all.I'm not sure we're at the core of the argument, though.To me it seems like GPT-3 is allowed to be superhuman at producing remixed and regurgitated sentences about economics, because this is about as relevant to Science talent as a calculator being able to do perfect arithmetic, only less so.[Ngo][16:15] \u00a0Suppose that the remixed and regurgitated sentences slowly get more and more coherent, until GPT-N can debate with a professor of economics and sustain a reasonable position.[Yudkowsky][16:15] \u00a0Are these points that GPT-N read elsewhere on the Internet, or are they new good points that no professor of economics on Earth has ever made before?[Ngo][16:15] \u00a0I guess you don't expect this to happen, but I'm trying to think about what experiments we could run to get evidence for or against it.The latter seems both very hard to verify, and also like a very high bar - I'm not sure if most professors of economics have generated new good arguments that no other professor has ever made before.So I guess the former.[Yudkowsky][16:18] \u00a0Then I think that you can do this without being able to do science. \u00a0It's a lot like if somebody with a really good memory was lucky enough to have read that exact argument on the Internet yesterday, and to have a little talent for paraphrasing. \u00a0Not by coincidence, having this ability gives you - on my model - no ability to do science, invent science, be the first to build handaxes, or design nanotechnology.I admit, this does reflect my personal model of how Science works, presumably not shared by many leading bureaucrats, where in fact the papers full of regurgitated scientific-sounding sentences are not accomplishing much.[Ngo][16:20] \u00a0So it seems like your model doesn't rule out narrow AIs producing well-reviewed scientific papers, since you don't trust the review system very much.[Yudkowsky][16:23] \u00a0I'm trying to remember whether or not I've heard of that happening, like, 10 years ago.My vague recollection is that things in the Sokal Hoax genre where the submissions succeeded, used humans to hand-generate the nonsense rather than any submissions in the genre having been purely machine-generated.[Ngo][16:24] \u00a0Which doesn't seem like an unreasonable position, but it does make it harder to produce tests that we have opposing predictions on.[Yudkowsky][16:24] \u00a0Obviously, that doesn't mean it couldn't have been done 10 years ago, because 10 years ago it's plausibly a lot easier to hand-generate passing nonsense than to write an AI program that does it.oh, wait, I'm wrong!https://news.mit.edu/2015/how-three-mit-students-fooled-scientific-journals-0414In April of 2005 the team\u2019s submission, \u201cRooter: A Methodology for the Typical Unification of Access Points and Redundancy,\u201d was accepted as a non-reviewed paper to the World Multiconference on Systemics, Cybernetics and Informatics (WMSCI), a conference that Krohn says is known for \u201cbeing spammy and having loose standards.\u201d\u00a0in 2013 IEEE and Springer Publishing removed more than 120 papers from their sites after a French researcher\u2019s analysis determined that they were generated via SCIgen[Ngo][16:26] \u00a0Oh, interestingMeta note: I'm not sure where to take the direction of the conversation at this point. Shall we take a brief break?[Yudkowsky][16:27] \u00a0The creators continue to get regular emails from computer science students proudly linking to papers they\u2019ve snuck into conferences, as well as notes from researchers urging them to make versions for other disciplines.Sure! Resume 5p?[Ngo][16:27] \u00a0Yepp\u00a014.2. Domain-specific heuristics and nanotechnology\u00a0[Soares][16:41]\u00a0A few takes:1. It looks to me like there's some crux in \"how useful will the 'shallow' stuff get before dangerous things happen\". I would be unsurprised if this spiraled back into the gradualness debate. I'm excited about attempts to get specific and narrow disagreements in this domain (not necessarily bettable; I nominate distilling out specific disagreements before worrying about finding bettable ones).2. It seems plausible to me we should have some much more concrete discussion about possible ways things could go right, according to Richard. I'd be up for playin the role of beeping when things seem insufficiently concrete.3. It seems to me like Richard learned a couple things about Eliezer's model in that last bout of conversation. I'd be interested to see him try to paraphrase his current understanding of it, and to see Eliezer produce beeps where it seems particularly off.[Yudkowsky][17:00] \u00a0\ud83d\udc4b[Ngo][17:02] \u00a0Hmm, I'm not sure that I learned too much about Eliezer's model in this last round.[Soares][17:03] \u00a0(dang :-p)[Ngo][17:03] \u00a0It seems like Eliezer thinks that the returns of scientific investigation are very heavy-tailed.Which does seem pretty plausible to me.But I'm not sure how useful this claim is for thinking about the development of AI that can do science.I attempted in my document to describe some interventions that would help things go right.And the levels of difficulty involved.[Yudkowsky][17:07] \u00a0(My model is something like: there are some very shallow steps involved in doing science, lots of medium steps, occasional very deep steps, assembling the whole thing into Science requires having all the lego blocks available. \u00a0As soon as you look at anything with details, it ends up 'heavy-tailed' because it has multiple pieces and says how things don't work if all the pieces aren't there.)[Ngo][17:08] \u00a0Eliezer, do you have an estimate of how much slower science would proceed if everyone's IQs were shifted down by, say, 30 points?[Yudkowsky][17:10] \u00a0It's not obvious to me that science proceeds significantly past its present point. \u00a0I would not have the right to be surprised if Reality told me the correct answer was that a civilization like that just doesn't reach AGI, ever.[Ngo][17:12] \u00a0Doesn't your model take a fairly big hit from predicting that humans just happen to be within 30 IQ points of not being able to get any more science?It seems like a surprising coincidence.Or is this dependent on the idea that doing science is much harder now than it used to be?And so if we'd been dumber, we might have gotten stuck before newtonian mechanics, or else before relativity?[Yudkowsky][17:13] \u00a0No, humanity is exactly the species that finds it barely possible to do science.[Ngo][17:14] \u00a0It seems to me like humanity is exactly the species that finds it barely possible to do civilisation.[Yudkowsky][17:14] \u00a0If it were possible to do it with less intelligence, we'd be having this conversation over the Internet that we'd developed with less intelligence.[Ngo][17:15] \u00a0And it seems like many of the key inventions that enabled civilisation weren't anywhere near as intelligence-bottlenecked as modern science.[Yudkowsky][17:15] \u00a0Yes, it does seem that there's quite a narrow band between \"barely smart enough to develop agriculture\" and \"barely smart enough to develop computers\"! Though there were genuinely fewer people in the preagricultural world, with worse nutrition and no Ashkenazic Jews, and there's the whole question about to what degree the reproduction of the shopkeeper class over several centuries was important to the Industrial Revolution getting started.[Ngo][17:15] \u00a0(e.g. you'd get better spears or better plows or whatever just by tinkering, whereas you'd never get relativity just by tinkering)[Yudkowsky][17:17] \u00a0I model you as taking a lesson from this which is something like... you can train up a villager to be John von Neumann by spending some evolutionary money on giving them science-specific brain features, since John von Neumann couldn't have been much more deeply or generally intelligent, and you could spend even more money and make a chimp a better scientist than John von Neumann.My model is more like, yup, the capabilities you need to invent aqueducts sure do generalize the crap out of things, though also at the upper end of cognition there are compounding returns which can bring John von Neumann into existence, and also also there's various papers suggesting that selection was happening really fast over the last few millennia and real shifts in cognition shouldn't be ruled out. \u00a0(This last part is an update to what I was thinking when I wrote Intelligence Explosion Microeconomics, and is from my own perspective a more gradualist line of thinking, because it means there's a wider actual target to traverse before you get to von Neumann.)[Ngo][17:20] \u00a0It's not that \"von Neumann isn't much more deeply generally intelligent\", it's more like \"domain-specific heuristics and instincts get you a long way\". E.g. soccer is a domain where spending evolutionary money on specific features will very much help you beat von Neumann, and so is art, and so is music.[Yudkowsky][17:20] \u00a0My skepticism here is that there's a version of, like, \"invent nanotechnology\" which routes through just the shallow places, which humanity stumbles over before we stumble over deep AGI.[Ngo][17:21] \u00a0Would you be comfortable publicly discussing the actual cognitive steps which you think would be necessary for inventing nanotechnology?[Yudkowsky][17:23] \u00a0It should not be overlooked that there's a very valid sibling of the old complaint \"Anything you can do ceases to be AI\", which is that \"Things you can do with surprisingly-to-your-model shallow cognition are precisely the things that Reality surprises you by telling you that AI can do earlier than you expected.\" \u00a0When we see GPT-3, we were getting some amount of real evidence about AI capabilities advancing faster than I expected, and some amount of evidence about GPT-3's task being performable using shallower cognition than expected.Many people were particularly surprised by Go because they thought that Go was going to require deeper real thought than chess.And I think AlphaGo probably was thinking in a legitimately deeper way than Deep Blue. \u00a0Just not as much deeper as Douglas Hofstadter thought it would take.Conversely, people thought a few years ago that driving cars really seemed to be the sort of thing that machine learning would be good at, and were unpleasantly surprised by how the last 0.1% of driving conditions were resistant to shallow techniques.Despite the inevitable fact that some surprises of this kind now exist, and that more such surprises will exist in the future, it continues to seem to me that science-and-engineering on the level of \"invent nanotech\" still seems pretty unlikely to be easy to do with shallow thought, by means that humanity discovers before AGI tech manages to learn deep thought?What actual cognitive steps? \u00a0Outside-the-box thinking, throwing away generalizations that governed your previous answers and even your previous questions, inventing new ways to represent your questions, figuring out which questions you need to ask and developing plans to answer them; these are some answers that I hope will be sufficiently useless to AI developers that it is safe to give them, while still pointing in the direction of things that have an un-GPT-3-like quality of depth about them.Doing this across unfamiliar domains that couldn't be directly trained in by gradient descent because they were too expensive to simulate a billion examples ofIf you have something this powerful, why is it not also noticing that the world contains humans? \u00a0Why is it not noticing itself?[Ngo][17:30] \u00a0If humans were to invent this type of nanotech, what do you expect the end intellectual result to be?E.g. consider the human knowledge involved in building carsThere are thousands of individual parts, each of which does a specific thing[Yudkowsky][17:30] \u00a0Uhhhh... is there a reason why \"Eric Drexler's Nanosystems but, like, the real thing, modulo however much Drexler did not successfully Predict the Future about how to do that, which was probably a lot\" is not the obvious answer here?[Ngo][17:31] \u00a0And some deep principles governing engines, but not really very crucial ones to actually building (early versions of) those engines[Yudkowsky][17:31] \u00a0that's... not historically true at all?getting a grip on quantities of heat and their flow was critical to getting steam engines to workit didn't happen until the math was there[Ngo][17:32] \u00a0Ah, interesting[Yudkowsky][17:32] \u00a0maybe you can be a mechanic banging on an engine that somebody else designed, around principles that somebody even earlier invented, without a physics degreebut, like, engineers have actually needed math since, like, that's been a thing, it wasn't just a prestige trick[Ngo][17:34] \u00a0Okay, so you expect there to be a bunch of conceptual work in finding equations which govern nanosystems.Uhhhh... is there a reason why \"Eric Drexler's Nanosystems but, like, the real thing, modulo however much Drexler did not successfully Predict the Future about how to do that, which was probably a lot\" is not the obvious answer here?This may in fact be the answer; I haven't read it though.[Yudkowsky][17:34]\u00a0or other abstract concepts than equations, which have never existed beforelike, maybe not with a type signature unknown to humanity, but with specific instances unknown to present humanitythat's what I'd expect to see from humanly designed nanosystems[Ngo][17:35] \u00a0So something like AlphaFold is only doing a very small proportion of the work here, since it's not able to generate new abstract concepts (of the necessary level of power)[Yudkowsky][17:35] \u00a0yeeeessss, that is why DeepMind did not take over the world last yearit's not just that AlphaFold lacks the concepts but that it lacks the machinery to invent those concepts and the machinery to do anything with such concepts[Ngo][17:38] \u00a0I think I find this fairly persuasive, but I also expect that people will come up with increasingly clever ways to leverage narrow systems so that they can do more and more work.(including things like: if you don't have enough simulations, then train another narrow system to help fix that, etc)[Yudkowsky][17:39] \u00a0(and they will accept their trivial billion-dollar-payouts and World GDP will continue largely undisturbed, on my mainline model, because it will be easiest to find ways to make money by leveraging narrow systems on the less regulated, less real parts of the economy, instead of trying to build houses or do medicine, etc.)real tests being expensive, simulation being impossibly expensive, and not having enough samples to train your civilization's current level of AI technology, is not a problem you can solve by training a new AI to generate samples, because you do not have enough samples to train your civilization's current level of AI technology to generate more samples[Ngo][17:41] \u00a0Thinking about nanotech makes me more sympathetic to the argument that developing general intelligence will bring a sharp discontinuity. But it also makes me expect longer timelines to AGI, during which there's more time to do interesting things with narrow AI. So I guess it weighs more against Dario's view, less against Paul's view.[Yudkowsky][17:41] \u00a0well, I've been debating Paul about that separately in the timelines channel, not sure about recapitulating it herebut in broad summary, since I expect the future to look like it was drawn from the \"history book\" barrel and not the \"futurism\" barrel, I expect huge barriers to doing huge things with narrow AI in small amounts of time; you can sell waifutech because it's unregulated and hard to regulate, but that doesn't feed into core mining and steel production.we could already have double the GDP if it was legal to build houses and hire people, etc., and the change brought by pre-AGI will perhaps be that our GDP could quadruple instead of just double if it was legal to do things, but that will not make it legal to do things, and why would anybody try to do things and probably fail when there are easier $36 billion profits to be made in waifutech.\u00a0\u00a014.3. Relatively shallow cognition, Go, and math\u00a0[Ngo][17:45]\u00a0I'd be interested to see Paul's description of how we would train AIs to solve hard scientific problems. I think there's some prediction that's like \"we train it on arxiv and fine-tune it until it starts to output credible hypotheses about nanotech\". And this seems like it has a step that's quite magical to me, but perhaps that'll be true of any prediction that I make before fully understanding how intelligence works.[Yudkowsky][17:46]\u00a0my belief is not so much that this training can never happen, but that this probably means the system was trained beyond the point of safe shallownessnot in principle over all possible systems a superintelligence could build, but in practice when it happens on Earthmy only qualm about this is that current techniques make it possible to buy shallowness in larger quantities than this Earth has ever seen before, and people are looking for surprising ways to make use of thatso I weigh in my mind the thought of Reality saying Gotcha! by handing me a headline I read tomorrow about how GPT-4 has started producing totally reasonable science papers that are actually correctand I am pretty sure that exact thing doesn't happenand I ask myself about GPT-5 in a few more years, which had the same architecture as GPT-3 but more layers and more training, doing the same thingand it's still largely \"nope\"then I ask myself about people in 5 years being able to use the shallow stuff in any way whatsoever to produce the science papersand of course the answer there is, \"okay, but is it doing that without having shallowly learned stuff that adds up to deep stuff which is why it can now do science\"and I try saying back \"no, it was born of shallowness and it remains shallow and it's just doing science because it turns out that there is totally a way to be an incredibly mentally shallow skillful scientist if you think 10,000 shallow thoughts per minute instead of 1 deep thought per hour\"and my brain is like, \"I cannot absolutely rule it out but it really seems like trying to call the next big surprise in 2014 and you guess self-driving cars instead of Go because how the heck would you guess that Go was shallower than self-driving cars\"like, that is an imaginable surprise[Ngo][17:52] \u00a0On that particular point it seems like the very reasonable heuristic of \"pick the most similar task\" would say that go is like chess and therefore you can do it shallowly.[Yudkowsky][17:52] \u00a0but there's a world of difference between saying that a surprise is imaginable, and that it wouldn't surprise you[Ngo][17:52] \u00a0I wasn't thinking that much about AI at that point, so you're free to call that post-hoc.[Yudkowsky][17:52] \u00a0the Chess techniques had already failed at Goactual new techniques were requiredthe people around at the time had witnessed sudden progress on self-driving cars a few years earlier[Ngo][17:53] \u00a0My advance prediction here is that \"math is like go and therefore can be done shallowly\".[Yudkowsky][17:53] \u00a0self-driving cars were of obviously greater economic interest as wellmy recollection is that talk of the time was about self-drivingheh! I have the same sense.that is, math being shallower than science.though perhaps not as shallow as Go, and you will note that Go has fallen and Math has not[Ngo][17:54] \u00a0rightI also expect that we'll need new techniques for math (although not as different from the go techniques as the go techniques were from chess techniques)But I guess we're not finding strong disagreements here either.[Yudkowsky][17:57] \u00a0if Reality came back and was like \"Wrong! Keeping up with the far reaches of human mathematics is harder than being able to develop your own nanotech,\" I would be like \"What?\" to about the same degree as being \"What?\" on \"You can build nanotech just by thinking trillions of thoughts that are too shallow to notice humans!\"[Ngo][17:58] \u00a0Perhaps let's table this topic and move on to one of the others Nate suggested? I'll note that walking through the steps required to invent a science of nanotechnology does make your position feel more compelling, but I'm not sure how much of that is the general \"intelligence is magic\" intuition I mentioned before.[Yudkowsky][17:59] \u00a0How do you suspect your beliefs would shift if you had any detailed model of intelligence?Consider trying to imagine a particular wrong model of intelligence and seeing what it would say differently?(not sure this is a useful exercise and we could indeed try to move on)[Ngo][18:01] \u00a0I think there's one model of intelligence where scientific discovery is more actively effortful - as in, you need to be very goal-directed in determining hypotheses, testing hypotheses, and so on.And there's another in which scientific discovery is more constrained by flashes of insight, and the systems which are producing those flashes of insight are doing pattern-matching in a way that's fairly disconnected from the real-world consequences of those insights.[Yudkowsky][18:05] \u00a0The first model is true and the second one is false, if that helps. \u00a0You can tell this by contemplating where you would update if you learned any model, by considering that things look more disconnected when you can't see the machinery behind them. \u00a0If you don't know what moves the second hand on a watch and the minute hand on a watch, they could just be two things that move at different rates for completely unconnected reasons; if you can see inside the watch, you'll see that the battery is shared and the central timing mechanism is shared and then there's a few gears to make the hands move at different rates.Like, in my ontology, the notion of \"effortful\" doesn't particularly parse as anything basic, because it doesn't translate over into paperclip maximizers, which are neither effortful nor effortless.But in a human scientist you've got thoughts being shoved around by all sorts of processes behind the curtains, created by natural selection, some of them reflecting shards of Consequentialism / shadowing paths through timeThe flashes of insight come to people who were looking in nonrandom placesIf they didn't plan deliberately and looked on pure intuition, they looked with an intuition trained by past success and failureSomebody walking doesn't plan to walk, but long ago as a baby they learned from falling over, and their ancestors who fell over more didn't reproduce[Ngo][18:09] \u00a0I think the first model is probably more true for humans in the domain of science. But I'm uncertain about the extent to which this because humans have not been optimised very much for doing science. If we consider the second model in a domain that humans have actually been optimised very hard for (say, physical activity) - then maybe we can use the analogy of a coach and a player. The coach can tell the player what to practice, but almost all the work is done by the player practicing in a way which updates their intuitions.This has become very abstract, though.\u00a014.4. Pivotal acts and historical precedents\u00a0[Ngo][18:11]\u00a0A few takes:1. It looks to me like there's some crux in \"how useful will the 'shallow' stuff get before dangerous things happen\". I would be unsurprised if this spiraled back into the gradualness debate. I'm excited about attempts to get specific and narrow disagreements in this domain (not necessarily bettable; I nominate distilling out specific disagreements before worrying about finding bettable ones).2. It seems plausible to me we should have some much more concrete discussion about possible ways things could go right, according to Richard. I'd be up for playin the role of beeping when things seem insufficiently concrete.3. It seems to me like Richard learned a couple things about Eliezer's model in that last bout of conversation. I'd be interested to see him try to paraphrase his current understanding of it, and to see Eliezer produce beeps where it seems particularly off.Here's Nate's comment.We could try his #2 suggestion: concrete ways that things could go right.[Soares][18:12] \u00a0(I am present and am happy to wield the concreteness-hammer)[Ngo][18:13] \u00a0I think I'm a little cautious about this line of discussion, because my model doesn't strongly constrain the ways that different groups respond to increasing developments in AI. The main thing I'm confident about is that there will be much clearer responses available to us once we have a better picture of AI development.E.g. before modern ML, the option of international constraints on compute seemed much less salient, because algorithmic developments seemed much more important.Whereas now, tracking/constraining compute use seems like one promising avenue for influencing AGI development.Or in the case of nukes, before knowing the specific details about how they were constructed, it would be hard to give a picture of how arms control goes well. But once you know more details about the process of uranium enrichment, you can construct much more efficacious plans.[Yudkowsky][18:19] \u00a0Once we knew specific things about bioweapons, countries developed specific treaties for controlling them, which failed (according to @CarlShulman)[Ngo][18:19, moved two down in log] \u00a0(As a side note, I think that if Eliezer had been around in the 1930s, and you described to him what actually happened with nukes over the next 80 years, he would have called that \"insanely optimistic\".)[Yudkowsky][18:21] \u00a0Mmmmmmaybe. \u00a0Do note that I tend to be more optimistic than the average human about, say, global warming, or everything in transhumanism outside of AGI.Nukes have going for them that, in fact, nobody has an incentive to start a global thermonuclear war. \u00a0Eliezer is not in fact pessimistic about everything and views his AGI pessimism as generalizing to very few other things, which are not, in fact, as bad as AGI.[Ngo][18:21] \u00a0I think I put this as the lowest application of competent power out of the things listed in my doc; I'd need to look at the historical details to know if important decision-makers actually cared about it, or were just doing it for PR reasons.[Shulman][18:22] \u00a0Once we knew specific things about bioweapons, countries developed specific treaties for controlling them, which failed (according to @CarlShulman)The treaties were pro forma without verification provisions because the powers didn't care much about bioweapons. They did have verification for nuclear and chemical weapons which did work.[Yudkowsky][18:22] \u00a0But yeah, compared to pre-1946 history, nukes actually kind of did go really surprisingly well!Like, this planet used to be a huge warring snakepit of Great Powers and Little Powers and then nukes came along and people actually got serious and decided to stop having the largest wars they could fuel.[Shulman][18:22][18:23]\u00a0The analog would be an international agreement to sign a nice unenforced statement of AI safety principles and then all just building AGI in doomy ways without explicitly saying they're doing it..The BWC also allowed 'defensive' research that is basically as bad as the offensive kind.[Yudkowsky][18:23] \u00a0The analog would be an international agreement to sign a nice unenforced statement of AI safety principles and then all just building AGI in doomy ways without explicitly saying they're doing it..This scenario sure sounds INCREDIBLY PLAUSIBLE, yes[Ngo][18:22] \u00a0On that point: do either of you have strong opinions about the anthropic shadow argument about nukes? That seems like one reason why the straw 1930s-Eliezer I just cited would have been justified.[Yudkowsky][18:23] \u00a0I mostly don't consider the anthropic shadow stuff[Shulman][18:24] \u00a0In the late Cold War Gorbachev and Reagan might have done the BWC treaty+verifiable dismantling, but they were in a rush on other issues like nukes and collapse of the USSR.Putin just wants to keep his bioweapons program, it looks like. Even denying the existence of the exposed USSR BW program.[Yudkowsky][18:25] \u00a0I'm happy making no appeal to anthropics here.[Shulman][18:25] \u00a0Boo anthropic shadow claims. Always dumb.(Sorry I was only invoked for BW, holding my tongue now.)[Yudkowsky: \u2764\ufe0f][Soares: \u2764\ufe0f][Yudkowsky][18:26] \u00a0There may come a day when the strength of nonanthropic reasoning fails... but that is not this day![Ngo][18:27] \u00a0Okay, happy to rule that out for now too. So yeah, I picture 1930s-Eliezer pointing to technological trends and being like \"by default, 30 years after the first nukes are built, you'll be able to build one in your back yard. And governments aren't competent enough to stop that happening.\"And I don't think I could have come up with a compelling counterargument back then.[Soares][18:27] \u00a0[Sorry I was only invoked for BW, holding my tongue now.](fwiw, I thought that when Richard asked \"you two\" re: anthropic shadow, he meant you also. But I appreciate the caution. And in case Richard meant me, I will note that I agree w/ Carl and Eliezer on this count.)[Ngo][18:28] \u00a0(fwiw, I thought that when Richard asked \"you two\" re: anthropic shadow, he meant you also. But I appreciate the caution. And in case Richard meant me, I will note that I agree w/ Carl and Eliezer on this count.)Oh yeah, sorry for the ambiguity, I meant Carl.I do believe that AI control will be more difficult than nuclear control, because AI is so much more useful. But I also expect that there will be many more details about AI development that we don't currently understand, that will allow us to influence it (because AGI is a much more complicated concept than \"really really big bomb\").[Yudkowsky][18:29] \u00a0[So yeah, I picture 1930s-Eliezer pointing to technological trends and being like \"by default, 30 years after the first nukes are built, you'll be able to build one in your back yard. And governments aren't competent enough to stop that happening.\"And I don't think I could have come up with a compelling counterargument back then.]So, I mean, in fact, I don't prophesize doom from very many trends at all! \u00a0It's literally just AGI that is anywhere near that unmanageable! \u00a0Many people in EA are more worried about biotech than I am, for example.[Ngo][18:31] \u00a0I appreciate that my response is probably not very satisfactory to you here, so let me try to think about more concrete things we can disagree about.[Yudkowsky][18:31] \u00a0[I do believe that AI control will be more difficult than nuclear control, because AI is so much more useful. But I also expect that there will be many more details about AI development that we don't currently understand, that will allow us to influence it (because AGI is a much more complicated concept than \"really really big bomb\").]Er... I think this is not a correct use of the Way I was attempting to gesture at; things being more complicated when known than unknown, does not mean you have more handles to influence them because each complication has the potential to be a handle. \u00a0It is not in general true that very complicated things are easier for humanity in general, and governments in particular, to control, because they have so many exposed handles.I think there's a valid argument about it maybe being more possible to control the supply chain for AI training processors if the global chip supply chain is narrow (also per Carl).[Ngo][18:34] \u00a0One thing that we seemed to disagree on, to a significant extent, is the difficulty of \"US and China preventing any other country from becoming a leader in AI\"[Yudkowsky][18:35] \u00a0It is in fact a big deal about nuclear tech that uranium can't be mined in every country, as I understand it, and that centrifuges stayed at the frontier of technology and were harder to build outside the well-developed countries, and that the world ended up revolving around a few Great Powers that had no interest in nuclear tech proliferating any further.[Ngo][18:35] \u00a0It seems to me that the US and/or China could apply a lot of pressure to many countries.[Yudkowsky][18:35] \u00a0Unfortunately, before you let that encourage you too much, I would also note it was an important fact about nuclear bombs that they did not produce streams of gold and then ignite the atmosphere if you turned up the stream of gold too high with the actual thresholds involved being unpredictable.[Ngo][18:35] \u00a0E.g. if the UK had actually seriously tried to block Google's acquisition of DeepMind, and the US had actually seriously tried to convince them not to do so, then I expect that the UK would have folded. (Although it's a weird hypothetical.)Unfortunately, before you let that encourage you too much, I would also note it was an important fact about nuclear bombs that they did not produce streams of gold and then ignite the atmosphere if you turned up the stream of gold too high with the actual thresholds involved being unpredictable.Not a critical point, but nuclear power does actually seem like a \"stream of gold\" in many ways.(also, quick meta note: I need to leave in 10 mins)[Yudkowsky][18:38] \u00a0I would be a lot more cheerful about a few Great Powers controlling AGI if AGI produced wealth, but more powerful AGI produced no more wealth; if AGI was made entirely out of hardware, with no software component that could be keep getting orders of magnitude more efficient using hardware-independent ideas; and if the button on AGIs that destroyed the world was clearly labeled.That does take AGI to somewhere in the realm of nukes.[Ngo][18:38] \u00a0How much improvement do you think can be eked out of existing amounts of hardware if people just try to focus on algorithmic improvements?[Yudkowsky][18:38] \u00a0And Eliezer is capable of being less concerned about things when they are intrinsically less concerning, which is why my history does not, unlike some others in this field, involve me running also being Terribly Concerned about nuclear war, global warming, biotech, and killer drones.[Ngo][18:39] \u00a0This says 44x improvements over 7 years: https://openai.com/blog/ai-and-efficiency/[Yudkowsky][18:39] \u00a0Well, if you're a superintelligence, you can probably do human-equivalent human-speed general intelligence on a 286, though it might possibly have less fine motor control, or maybe not, I don't know.[Ngo][18:40] \u00a0(within reasonable amounts of human-researcher-time - say, a decade of holding hardware fixed)[Yudkowsky][18:40] \u00a0I wouldn't be surprised if human ingenuity asymptoted out at AGI on a home computer from 1995.Don't know if it'd take more like a hundred years or a thousand years to get fairly close to that.[Ngo][18:41] \u00a0Does this view cash out in a prediction about how the AI and Efficiency graph projects into the future?[Yudkowsky][18:42] \u00a0The question of how efficiently you can perform a fixed algorithm doing fixed things, often pales compared to the gains on switching to different algorithms doing different things.Given government control of all the neural net training chips and no more public GPU farms, I buy that they could keep a nuke!AGI (one that wasn't tempting to crank up and had clearly labeled Doom-Causing Buttons whose thresholds were common knowledge) under lock of the Great Powers for 7 years, during which software decreased hardware requirements by 44x. \u00a0I am a bit worried about how long it takes before there's a proper paradigm shift on the level of deep learning getting started in 2006, after which the Great Powers need to lock down on individual GPUs.[Ngo][18:46] \u00a0Hmm, okay.\u00a014.5. Past ANN progress\u00a0[Ngo][18:46]\u00a0I don't expect another paradigm shift like that(in part because I'm not sure the paradigm shift actually happened in the first place - it seems like neural networks were improving pretty continuously over many decades)[Yudkowsky][18:47]\u00a0I've noticed that opinion around OpenPhil! \u00a0It makes sense if you have short timelines and expect the world to end before there's another paradigm shift, but OpenPhil doesn't seem to expect that either.Yeah, uh, there was kinda a paradigm shift in AI between say 2000 and now. \u00a0There really, really was.[Ngo][18:49] \u00a0What I mean is more like: it's not clear to me that an extrapolation of the trajectory of neural networks is made much better by incorporating data about the other people who weren't using neural networks.[Yudkowsky][18:49] \u00a0Would you believe that at one point Netflix ran a prize contest to produce better predictions of their users' movie ratings, with a $1 million prize, and this was one of the largest prizes ever in AI and got tons of contemporary ML people interested, and neural nets were not prominent on the solutions list at all, because, back then, people occasionally solved AI problems not using neural nets?I suppose that must seem like a fairy tale, as history always does, but I lived it![Ngo][18:50] \u00a0(I wasn't denying that neural networks were for a long time marginalised in AI)I'd place much more credence on future revolutions occurring if neural networks had actually only been invented recently.(I have to run in 2 minutes)[Yudkowsky][18:51] \u00a0The world might otherwise end before the next paradigm shift, but if the world keeps on ticking for 10 years, 20 years, there will not always be the paradigm of training massive networks by even more massive amounts of gradient descent; I do not think that is actually the most efficient possible way to turn computation into intelligence.Neural networks stayed stuck at only a few layers for a long time, because the gradients would explode or die out if you made the networks any deeper.There was a critical moment in 2006(?) where Hinton and Salakhutdinov(?) proposed training Restricted Boltzmann machines unsupervised in layers, and then 'unrolling' the RBMs to initialize the weights in the network, and then you could do further gradient descent updates from there, because the activations and gradients wouldn't explode or die out given that initialization. \u00a0That got people to, I dunno, 6 layers instead of 3 layers or something? But it focused attention on the problem of exploding gradients as the reason why deeply layered neural nets never worked, and that kicked off the entire modern field of deep learning, more or less.[Ngo][18:56] \u00a0Okay, so are you claiming that that neural networks were mostly bottlenecked by algorithmic improvements, not compute availability, for a significant part of their history?[Yudkowsky][18:56] \u00a0If anybody goes back and draws a graph claiming the whole thing was continuous if you measure the right metric, I am not really very impressed unless somebody at the time was using that particular graph and predicting anything like the right capabilities off of it.[Ngo][18:56] \u00a0If so this seems like an interesting question to get someone with more knowledge of ML history than me to dig into; I might ask around.[Yudkowsky][18:57] \u00a0[Okay, so are you claiming that that neural networks were mostly bottlenecked by algorithmic improvements, not compute availability, for a significant part of their history?]Er... yeah? \u00a0There was a long time when, even if you threw a big neural network at something, it just wouldn't work.Good night, btw?[Ngo][18:57] \u00a0Let's call it here; thanks for the discussion.[Soares][18:57] \u00a0Thanks, both![Ngo][18:57] \u00a0I'll be interested to look into that claim, it doesn't fit with the impressions I have of earlier bottlenecks.I think the next important step is probably for me to come up with some concrete governance plans that I'm excited about.I expect this to take quite a long time[Soares][18:58] \u00a0We can coordinate around that later. Sorry for keeping you so late already, Richard.[Ngo][18:59] \u00a0No worriesMy proposal would be that we should start on whatever work is necessary to convert the debate into a publicly accessible document nowIn some sense coming up with concrete governance plans is my full-time job, but I feel like I'm still quite a way behind in my thinking on this, compared with people who have been thinking about governance specifically for longer[Soares][19:01] \u00a0(@RobBensinger is already on it \ud83d\ude42)[Bensinger: \u2705][Yudkowsky][19:03] \u00a0Nuclear plants might be like narrow AI in this analogy; some designs potentially contribute to proliferation, and you can get more economic wealth by building more of them, but they have no Unlabeled Doom Dial where you can get more and more wealth out of them by cranking them up until at some unlabeled point the atmosphere ignites.Also a thought: I don't think you just want somebody with more knowledge of AI history, I think you might need to ask an actual old fogey who was there at the time, and hasn't just learned an ordered history of just the parts of the past that are relevant to the historian's theory about how the present happened.Two of them, independently, to see if the answers you get are reliable-as-in-statistical-reliability.[Soares][19:19] \u00a0My own quick take, for the record, is that it looks to me like there are two big cruxes here.One is about whether \"deep generality\" is a good concept, and in particular whether it pushes AI systems quickly from \"nonscary\" to \"scary\" and whether we should expect human-built AI systems to acquire it in practice (before the acute risk period is ended by systems that lack it). The other is about how easy it will be to end the acute risk period (eg by use of politics or nonscary AI systems alone).I suspect the latter is the one that blocks on Richard thinking about governance strategies. I'd be interested in attempting further progress on the former point, though it's plausible to me that that should happen over in #timelines instead of here.", "title": "Ngo and Yudkowsky on scientific reasoning and pivotal acts", "karma": 53, "af": true, "userid": "nmk3nLpQE89dMRzzN", "id": "cCrpbZ4qTCEYXbzje"}, {"text": "My dad\u2019s a retired science teacher, so my little brother and I began playing the \u201cI\u2019m not touching you!\u201d game unusually early.In case you\u2019re not familiar with \u201cI\u2019m not touching you!\u201d, here\u2019s how it works.Step 1: Ask your parents (or the internet or whatever) why you don\u2019t fall through the floor.Step 2: Learn about electrostatic repulsion, and conclude that things never \u201creally touch\u201d.Step 3: Poke your sibling.Step 4: When they say \u201cStop touching me!\u201d, insist that \u201cI\u2019m not touching you!\u201dStep 5: Find out whether your parents consider \u201ctechnically correct\u201d to be an adequate defense. (They don\u2019t.)It is true that things do not touch in the way we naively imagine. Their component atoms\u2019 electrons repel each other. \u201cPerfectly direct contact\u201d does not ordinarily happen.\u00a0Nonetheless, there is a big difference between the \u201cnot touching\u201d that\u2019s happening when you poke your sibling, and the \u201cnot touching\u201d that\u2019s happening when you point at your sibling from three meters away.Similarly, there is no unmediated subjective experience of the territory. When you poke your little brother\u2019s arm, your brain does something with those nerve signals to turn them into phenomena, into the subjective experience of tactile sensations. The sensations are not the nerve signals. There is always processing distance between your subjective experiences and whatever is happening to your sense organs.However, some processing distances are smaller than others.\u00a0Consider the processing distance between [the electrostatic repulsion at your fingertip] and [your plan to test how upset you can make your brother before your parents intervene]. Although your plan exists mostly inside your head, it definitely has something to do with that electrostatic repulsion, in about the same way that a pointing finger has something to do with whatever it\u2019s pointing at. A plan is made of anticipations, imaginings, desires, and so forth; yet it isn\u2019t completely out of contact with the territory.But consider the distance between [the electrostatic repulsion at your fingertip] and [the immediate tactile sensations of your finger]. The subjective sensations still exist only inside your head, but the processing distance is much smaller. Those sensations put you in closer contact with the underlying reality of electrostatic repulsion. Compared to imaginings or desires, they are much more tightly entangled with what\u2019s happening out there.Here\u2019s how I think contact with the territory works.\u00a0For me to make contact with my brother\u2019s arm, there are three conditions that must be met, three entities that must coincide.Presence. An arm must be present. There must be a region of territory available to be contacted. I can\u2019t make much contact with my brother\u2019s arm if he\u2019s in another city.Personhood. There must be a contactor, a mind participating in the experience of contact, a place where attending occurs. I can\u2019t make contact in the relevant sense if I\u2019m under general anesthesia, even if my brother\u2019s arm happens to brush my finger while I\u2019m under.Sensation. There must be an experience of pressure on the skin of my hand as it presses up against my brother\u2019s arm. There has to be some kind of sensation I might become aware of. I can\u2019t make contact if I\u2019ve lost all the nerves in my fingers to frostbite.Presence, personhood, and sensation. Lose any of the three of these, and there\u2019s no longer contact in the sense I mean.But note that each of these conditions can obtain to a greater or lesser extent, depending on the situation. It isn\u2019t all or nothing.In the context of studying arms, a memory of an arm involves greater presence than does the description of a fictional arm in a novel. A photograph has more presence than a memory.Presence of arms: actual arm > photograph of an arm > memory of an arm > fictional description of an armPersonhood is similarly variable. A person\u2019s body can be physically present while their mind is otherwise occupied. They can selectively disengage parts of themselves when encountering a topic they\u2019d prefer not to think about, letting experiences \u201cbounce off\u201d or relegating them to a \u201csandbox\u201d for safety. It\u2019s possible to limit the mental space in which attending occurs, without eliminating it entirely\u2014or to expand that space beyond what\u2019s available by default. This is what phrases like \u201cshowing up\u201d, \u201cchecking out\u201d, and \u201cblankface\u201d refer to.Finally, just as a sighted person can wear sunglasses or even a blindfold, sensation isn\u2019t all or nothing. It\u2019s possible to be more or less numb to sensations at the point of contact, even when all your nerves are in working order.Numbness often happens when top-down processes overwrite bottom-up processes, as in the Ponzo illusion:\u00a0If the top blue line looks longer to you at first, it\u2019s probably because you\u2019re interpreting the visual sensations through an expectation that things near the bottom of the image are \u201ccloser\u201d, while things near the top are \u201cfarther away\u201d. If the top line were farther away, then it would have to be longer than the bottom line to produce the visual stimuli you encounter.This interpretation probably happens so quickly that your raw visual sensations are overwritten by it. To see the blue lines as the same length, if you even can, you have to deliberately back up and peel off a thick layer of perceptual interpretation, excavating sensations that correspond more directly to the lines themselves.But if you do peel back that interpretive layer, then you increase sensation at the point of contact with this image. It\u2019s like removing your gloves and actually touching the snow. You are feeling more of the image itself, rather than feeling your own processing systems.There is a gradient from \u201cseeing\u201d (in the Holmesian sense) to \u201cobservation\u201d, and it is identical to the gradient from low to high contact with the territory. Dial up presence, personhood, or sensation, and contact with the territory gets more direct, more naked. Gloves off, touching the snow. Dial them way down, and you\u2019re floating miles above the real world.(The \u201chow\u201d is tricky. That\u2019s a whole different sequence. But even before figuring out how to do it, it\u2019s worth considering that there may exist the possibility of movement toward more direct contact. It can help a lot to merely be aware that your experiences exist somewhere along the gradient from low to high contact with the territory.)In the sentence \u201cKnowing the territory takes patient and direct observation\u201d, this is what I mean by \u201cdirect\u201d. I mean showing up as a whole person, in the presence of real-world data, and getting your preconceptions out of the way so you can be sensitive to what is there.\u00a0Knowing the territory takes patient and naked observation.\u00a0", "title": "Direct Observation", "karma": 25, "af": false, "userid": "uuYBzWLiixkbN3s7C", "id": "y7wgtZ2ehfWuqzXfm"}, {"text": "It's currently hard to know where to start when trying to get better at thinking about alignment. So below I've listed a few dozen exercises which I expect to be helpful. They assume a level of background alignment knowledge roughly equivalent to what's covered in the technical alignment track of the AGI safety fundamentals course. They vary greatly in difficulty - some are standard knowledge in ML, some are open research questions. I\u2019ve given the exercises star ratings from * to *** for difficulty (note:\u00a0not for length of time to complete - many require reading papers before engaging with them). However, I haven't tried to solve them all myself, so the star ratings may be significantly off.I've erred on the side of including exercises which seem somewhat interesting and alignment-related even when I'm uncertain about their value; when working through them, you should keep the question \"is this actually useful? Why or why not?\" in mind as a meta-exercise. This post will likely be updated over time to remove less useful exercises and add new ones.I'd appreciate any contributions of:Comments about which exercises seem most or least useful.Answers to the exercises.More exercises! The ideal exercises are nerdsnipe-style problems which can be stated clearly, and seem well-defined, but lead into interesting depths when explored.Reward learning* Look at the examples of human feedback mechanisms discussed in the\u00a0reward-rational implicit choice paper. Think of another type of human feedback. What is the choice set? What is the grounding function?*\u00a0This paper by Anthropic introduces a technique called context distillation. Describe this in terms of the reward-rational implicit choice framework.* Estimate the bandwidth of information conveyed by different types of human feedback. Describe a rough model for how this might change as training progresses. By contrast, how much information is conveyed by the choice of a programmatic reward function? (Consider both the case where the agent is given the exact reward function, and where it learns from reward observations.)* Look at the examples of biases discussed in\u00a0learning the preferences of ignorant agents. Identify another bias which similarly influences human decision-making. Describe an example situation where a human with that bias might make the wrong decision. Formulate an algorithm that infers that human\u2019s true preferences.Some answers here.** Given that\u00a0humans can be assigned any values, why does reward learning ever work in practice?** Explain why\u00a0cooperative inverse reinforcement learning doesn\u2019t solve the alignment problem.Answer here.Agency** In\u00a0this paper, researchers devised a test for whether a recurrent network is doing planning: by seeing whether its performance improves when given more time to \u201cthink\u201d before it can act. In the AlphaGo paper, researchers compared the performance of their MCTS+neural network algorithm against the network alone. Think of some other test that we could run that would give us evidence about the extent to which some neural network is internally doing planning.* Consider\u00a0HCH, an attempted formalisation of \u201ca human\u2019s enlightened judgment\u201d. Why might an implementation of HCH not be aligned? What assumptions would be needed to prevent that?*** In a later post, Paul defines a\u00a0stronger version of HCH which \u201cincreases the complexity-theoretic expressiveness of HCH. The old version could be computed in EXPTIME, while the new version can compute any decidable function.\u201d Try to rederive a new version of HCH with these properties.Answer here.* Ask the\u00a0OpenAI API about what steps it would take to perform some long-term plan. Work in groups: think of a task that you expect it will be difficult to generate a good plan for, and then see who can design a prompt that will produce the best plan from the API.* Some steps of a plan generated by the API can also be performed by the API - e.g. a step which requires writing a poem about a given topic. What\u2019s the hardest task you can find for which the API can not only generate a plan, but also perform each of the steps in that plan?**\u00a0Pearl argues that neural networks trained on supervised or self-supervised data can\u2019t learn to reason about interventions and counterfactuals (see this post for an explanation of the distinction). What\u2019s the strongest counterargument against his position?Reinforcement learning** How is supervised learning on reward-maximising trajectories related (mathematically) to policy gradient with sparse, binary rewards?** What decision theories are implemented by different RL algorithms?Some answers here.** What might lead an RL agent to learn a policy which sacrifices reward in its current episode to get higher reward in a later episode?Some answers in section 7 here.* Self-play in zero-sum two-player games converges to an optimal strategy (given sufficient assumptions about the model class). In other games, this isn\u2019t the case - why not?** Evaluate\u00a0this paper (Reward is Enough). Does their argument hold up?** After doing that: consider a bird practicing singing, which listens to its own song, and does RL using the rule:\u00a0the better the song sounds, the higher the reward. But the bird is also deciding how much time to spend practicing singing versus foraging, etc. And the worse it sings, the more important it is to practice! So you really want the rule:\u00a0the worse the song sounds, the more rewarding it is to practice singing. How could you resolve this conflict?Some answers here.* Why can a behaviourally cloned policy perform well when run for a small set of timesteps, but poorly when run over a longer series of timesteps? How can this be fixed?Some answers here.** If a deep q-learning agent is trained in an environment where some actions lead to large negative rewards, it will never stop trying these actions (the policy will sometimes take these actions even when not randomly exploring due to epsilon exploration). Why does this happen? How could it be prevented?Some answers here.** RL agents have become capable of competent behaviour over longer and longer episodes. What difficulties arise in trying to measure improvements in how long they can act competently for? What metrics are most useful?The same question, but for sample efficiency rather than episode length.Some answers here.Neural networks* Consider\u00a0this paper on modularity in neural networks. Evaluate their metric of clustering; what others could we use instead?** Consider the following alignment proposal: a neural network has two output heads, one of which chooses actions, the other of which predicts the longer-term consequences of those actions. Suppose that we train the latter head to maximise human-evaluated prediction quality. What differences might we expect from backpropagating that loss all the way through the network, versus only backpropagating through the prediction head? What complications arise if we try to train the prediction head via RL? What advantages might there be of doing so?** \u201cGradient hacking\u201d is a hypothesised phenomenon by which a model decides its actions partly on the basis of its observations of its own parameters, thereby changing the way its parameters are updated. Does the gradient hacking mechanism described in the linked post work? If not, does any variant of it work?Some answers here.* Read\u00a0Jacob Steinhardt\u2019s list of examples of emergent shifts in machine learning. Can you think of any others? What about shifts that you expect in the near future?** What might it look like for the\u00a0circuits hypothesis to be false?*\u00a0This paper discusses the metric of \u201ceffective data transferred\u201d. What are the limitations of this metric? What are some alternative ways to measure data transfer?Alignment theory* Consider extending reinforcement learning to the case where rewards can depend on the parameters of a model. Why do classic convergence proofs no longer work?*** Are there any limiting assumptions which might lead to interesting theoretical results?** One concern with proposals to train using loss functions that depend directly on parameters is that if we train a network to avoid carrying out any particular piece of cognition, that cognition will instead just be distributed across the network in a way that we can\u2019t detect. Describe a toy example of a cognitive trait that we can currently detect automatically. Design an experiment to determine whether, after training to remove that trait, the network has learned to implement an equivalent trait in a less-easily-detectable way.*** Rederive some of the proofs in the following papers. For b) and c), explain what assumptions are being made about the optimality of the agents involved, and how they might break down in practice:Seeking Power is Convergently Instrumental in MDPsAI safety via debate (and see also\u00a0proofs about the effects of adding\u00a0cross-examination)Alignment proposals and complexity classes (and this follow-up)Some answers here.*** Produce a proposal for\u00a0the ELK prize (note that this requires engaging with the ELK writeup, which is very long).** Suppose that we\u2019re training a model via behavioural cloning of a human, but the human starts off with different prior knowledge to the model (either more knowledge, or less knowledge). How might this lead the model to behave in a misaligned way?Some answers here.Agent foundations*\u00a0Open-source game theory**\u00a0Selection theorems**\u00a0Fixed point exercises*** The 5 and 10 problemEvolution and economics* An old study split insects into several groups which each lived together, and artificially selected in favour of smaller groups, in an attempt to study whether they would evolve to voluntarily restrain their breeding. Predict the outcome of the study.Some answers here. Did the bias discussed in this post influence your expectations?** What might explain why there are so few hermaphroditic animal species, given that every individual being able to bear children could potentially double the number of children in the next generation?Some answers here.* Read this post about\u00a0evolving to extinction. Mathematically demonstrate that segregation-distorters could in fact lead a species to evolve to extinction.* Evaluate\u00a0Fletcher and Doebeli\u2019s model of the evolution of altruism.Use the model to show how\u00a0the green-beard effect could lead to the evolution of (a certain type of) altruism.Why are roughly equal numbers of males and females born in most species?Some answers here.* Comparing GDP across time requires reference to\u00a0a standard basket of goods and services. What difficulties might this cause in taking GDP comparisons at face value?Some answers here.** Evaluate\u00a0Roodman\u2019s model of explosive economic growth.* In cooperative game theory, the \u201ccore\u201d is the term for the set of allocations of payoffs to agents where no subset of the agents can form a coalition to improve their payoffs. For example, consider a group of N miners, who have discovered large bars of gold. Assume that two miners can carry one piece of gold, and so the payoff of any coalition S is floor(|S|/2). If N is even, then the core consists of the single payoff distribution where each miner gets \u00bd. If N is odd, then the core is empty (because the miner who is left out can always make a better offer to some miner who currently has a gold-carrying partner). Identify the core for the following games:A game with 2001 players: 1000 of them have 1 left shoe, 1001 have 1 right shoe. A left-shoe/right-shoe pair can be sold for $10.Mr A and Mr B each have three gloves. Any two gloves make a pair that they can sell for $5.Answers here.* How should coalitions decide how to split the payoffs they receive? The concept of\u00a0Shapley values provides one answer. Convince yourself that Shapley values have the properties of linearity, null player and the stand-alone test described in the linked article.Some important concepts in MLThese are intended less as exercises and more as pointers to open questions at the cutting edge of deep learning.Scaling lawsWhy do they have the form they do?Some answers here and\u00a0hereNeural networks memorisationDouble descentThe lottery ticket hypothesisGames with spinning-top structureGradient noise scale (see also here)OpenAI Requests for ResearchOpenAI Requests for Research 2Miscellaneous* Fill in your estimates in\u00a0Cotra\u2019s timeline model. Does the model broadly make sense to you; are there ways you\u2019d change it?* Try playing\u00a0OpenAI\u2019s implementation of the Debate game.** Identify an important concept in alignment that isn\u2019t currently very well-explained; write a more accessible explanation.", "title": "Alignment research exercises", "karma": 103, "af": true, "userid": "BCmzFRdQhqLPREvat", "id": "kj37Hzb2MsALwLqWt"}, {"text": "Saw other posts on this topic here, but thought I would post my own as well.\u00a0Takeaway: if you are interested in how/why psychology may be different in different areas of the world, or what drove European prosperity in the last few centuries, you might like this. \u00a0https://digitalsauna.wordpress.com/2022/02/21/the-weirdest-people-in-the-world-by-joseph-henrich-2020/", "title": "Summary/Review/Quotes of \"The WEIRDest People in the World\" by Joseph Henrich", "karma": 4, "af": false, "userid": "xoRpeFN7K5MgDRcvM", "id": "XzZikGB4WoPBHfLwh"}, {"text": "Hi there. I\u2019m a longtime LW-er posting under a pseudonym, in the hopes of being able to make my ignorance visible without unnecessary drama. Please don\u2019t out me; it seems to me that the ability to expose one\u2019s ignorance pseudonymously is a pro-epistemic norm that we\u2019ll be better off having.Now that I\u2019ve gotten that over with:I want to understand LW\u2019s aversion to religion. I\u2019m hoping some of you guys can help me model it. I\u2019m also hoping to explain what is attractive to me about religion as an aid to forming accurate maps (including, attractive to me vis a vis my goal of making LW stronger/more successfully truth-seeking), and to see if some of you are willing and able to understand my perspective about this (\u201cto pass my ITT,\u201d is I think how you lot would say this?). I want to know whether, after you \u201cpass my ITT,\u201d you have anything to say back that makes me think my attraction to religion is something truth can destroy.What is LW\u2019s objection to religion?Here\u2019s my starting attempt at describing LW\u2019s aversion to religion:1) Many religions claim things that are straightforwardly false (e.g., \u201cJesus physically rose from the dead.\u201d) It\u2019s harmful to believe false things.2) Human beings have an observable tendency to really persistently believe (or claim to believe) some obviously false things religions claim, compared to random obviously false things. This suggests to LW that religion is near a \u201cfault line\u201d in the human psyche, and if we go near that fault line our minds might start believing a lot of different false things, or might otherwise become less trustworthy. We don\u2019t have a precise, high-confidence model of what exactly this \u201cfault line\u201d is, so it is safest to stay far away from religion, so as to reduce the odds of accidentally stumbling over the unknown \u201creligion fault line\u201d and having our minds go screwy.If you\u2019re a LW-er who is averse to religion, and you have a couple minutes to make me smarter, I\u2019d really appreciate a comment below from you (or a PM) about whether this (1 and 2 above) summarizes most of what drives your aversion to religion, or whether it\u2019s missing something core to your reaction.How the above \u201cobjection to religion\u201d sits with meObjection 1 seems valid and correct to me. I agree many religions claim many false things. Jesus did not physically rise from the dead; there was no message to Noah about a flood; etc. I also agree that false beliefs are often harmful, and that, as Eliezer argues, the resistance many religious people have to cryonics is a strong example of the false beliefs of religion causing important harm. I suspect my beliefs here are roughly that of many near the center of LW, although I may be missing something.Objection 2 is more confusing to me. Objection 2, in my current understanding, is a special case of \u201cdon\u2019t expose yourself to a potentially compelling set of perspectives, because if you do you might become convinced by them, and then you\u2019ll believe something false.\u201d I am afraid of heuristics following this template. I agree, of course, that it\u2019s bad to end up believing something false. But exposing myself to all the perspectives, and allowing myself to think everything through after exposure to all arguments and perspectives, is usually a better heuristic than \u201cavoid thinking about X\u201d for figuring out what\u2019s true. (Think creationists saying \u201cdon\u2019t listen to biologists, they\u2019ll trick you.\u201d)One place where I would be grateful for help: what exactly is the fault line near religion, if there is in fact a fault line there? The conjunction fallacy is in some sense a fault line in the human psyche, but it\u2019s one we can name, can point to precisely, and can thereby become relatively immune to in the sense that one needn\u2019t fear reading conjunctive futurism; one can simply read it, remember the conjunction fallacy, and try not to fall for the false parts. Is religion like this? Or is it a broader and more unknown set of fault lines that requires more caution? I would feel far better about the whole thing if we could make \u201cwatch out for religion\u201d more similar to \u201cwatch out for the conjunction fallacy\u201d: specific, verifiably a bias (or a set of biases), and sufficiently well characterized that, after training in recognizing the relevant biases, practicing religion (e.g. praying) would no longer much threaten a person's sanity.Why, and in what sense, I find religion attractiveTo be clear, I do not find \u201cbelief in a literal Jehovah who literally raised Jesus from the dead\u201d attractive. That religion would include false beliefs, and false beliefs are harmful and not attractive.To describe what I do find attractive:Religion seems to me to be better (though still not great) at giving folks accurate, usable models of where to trust our maps vs where to have humility, and at giving folks accurate maps of what the heck it is to be a human being, related to other human beings, in the middle of this here universe.For example, when I contrast practitioners of most traditional religions (Christianity, Judaism, Buddhism, Hinduism, Taoism, various forms of traditional polytheistic religion, etc.) with most of those professing belief in what I\u2019ll call \u201cnaive scientistic materialism,\u201d it seems to me that the religious practitioners often do better in the following ways.Situating human life within temporal rhythms (daily/weekly, yearly, and across the lifetime). Honoring these rhythms communally.Most religions situate human life within a set of rhythms, including daily and weekly rhythms (e.g. \u201cdon\u2019t work on the Sabbath\u201d); yearly rhythms (e.g. \u201chere is when we celebrate the harvest; here is when we atone, fast, and make an empty space for seeing past our egos\u201d); and lifetime rhythms (e.g. \u201chere is when we recognize a young person as having come of age; here is a ritual for marriage; here is a ritual for death.\u201d)In contrast, many who profess what I\u2019m calling \u201cnaive scientistic materialism\u201d seem to me to eat, work, and sleep at all hours of the day and week in patterns that seem fairly unhealthy and unproductive to me. Many scientistic materialists seem to me to marry several times or not at all in patterns that I believe are less promoting of life, happiness, healthy children, and stable community than many traditional patterns; and to generally attempt to model themselves as rhythmless machines (\u201cI just need to give my body 8 hours of sleep and 2,000 calories and then it ought to give me back the ability to do functional work\u201d) in ways that seem to me to have inadequate respect for what I would call the sacred forces at work in the body, mind, and soul, and inadequate humility about our maps of the same.[1]Respect for folk ethicsMany religions claim that such things as courage, honesty, and hard work are virtues. (The specifics vary, but many religions claim something like that for some set of \u2018traditional virtues.') In contrast, many scientistic materialists speak as though they have arbitrary personal preferences for e.g. courage or honesty as part of their arbitrary, purely individual, utility functions. (Eliezer admittedly does better here.)I think it\u2019s, roughly speaking, more accurate to say \u201ccourage is a virtue\u201d than to say \u201cI personally value courage.\u201d I personally enjoy the flavor of caramel. \u201cCourage is a virtue\u201d is not similar to my relationship to caramel; it\u2019s a prediction that practicing courage will generally help a person to create good things broadly, and to have a right alignment of their soul. \u201cCourage is a virtue\u201d is a sentence, not about my map, but about the world.Folk ethics seems to me to be of real use for building viable organizations and relationships, and for keeping your soul in good working order. I think scientistic young people are often more confused than their religious counterparts here, due to being given less accurate maps. (This matters more for teens and twenties than for older folks, since young people are forced for lack of experience to rely more on outside maps.)Encouragement to bear and raise children, and to attend to the next generation. Offering shared communal patterns that make this easier.I suspect that religion (especially, communally practiced religion) makes it easier for people to create stable marriages and to raise children by providing shared maps of how to do this together. I suspect it also encourages more people to want these things by providing a more accurate map of their value, and a more accurate map of what kind of thing we humans are, such that we value children. (Again, providing accurate maps matters most for young people.)Many scientistic materialists seem in my experience to have difficulty finding anything to care about, partly because they are trying to follow an inaccurate map that tells them that they have a \u201cutility function\u201d that is mostly about themselves and their experience. Or they have inaccurate maps that tell them that their \u201cvalues\u201d are a thing they get to just make up according to what they think will get them social status or something, and these maps are also stupid and leave them adrift. Or they realize these maps have errors because they read the Sequences but are still similarly adrift.I suspect religion offers a more accurate map of the sense in which we are and are not isolated individuals, and of what matters to us deep down, and of at least some parts of how to live and orient given that.[2]The above list of places where religion seems to me to offer a more accurate map is not exhaustive; I am forcing myself to be brief. However, I believe it is representative.In conclusionI love the way LW seeks true beliefs and eschews false ones (including e.g. \u201cJesus physically rose from the dead.\u201d) I wish to protect this and to not mess with this.In keeping with what I understand of LW's culture, I wish to investigate together the question of whether religion offers systematically more accurate maps of some important aspects of humankind's relationship to the universe, and to investigate whether this can be separated from religion\u2019s admitted tendency to get stuck in specific false beliefs.I wish also to investigate the question of whether there is some terrible epistemic booby trap near religion, such that I or others should fear religious practice.I will be grateful for any assistance making more accurate maps of any of the above.Thank you.\u00a0^To translate my use of the word \u201csacred,\u201d here, into local parlance: our maps of the body are not the body. Our maps of the mind are not the mind. Our maps of the soul are not the soul (root word \u201cbreath\u201d \u2014 I\u2019m using \u201csoul\u201d to mean whatever it is that goes out of a person at the moment of death). Also: each of the body, mind, and soul come from living processes that are smart, that non-randomly try to bring about good things, and that deserve our \u201crespect\u201d in the sense of not expecting a reliable lack of backfire from micromanaging them. Furthermore, these processes grew up in the context of our historical relationship to e.g. nature, to foraging within a natural context that included daily and yearly rhythms, and to being in certain kinds of relationships to particular kinds of human communities, and I know no good reason to expect them to be robust to the total disruption of these relationships. When I regard these processes as \u201csacred,\u201d I remind myself to look at them in a manner appropriate to looking at old, wise things that use patterns not in my map to produce value I am dependent on.^I realize that Eliezer got a lot of this right, but I know nobody else who found his writing on morality compelling (nor did it work on me), and I think this was because he did not situate it within a larger accurate map in the way religion does. (Though he tried.) My guess is that Eliezer got values as right as he did partly because he grew up around religious people, and that his Sequence on morality did not manage to reproduce the component mental moves he used to get there. I am interested in whether seeking accurate maps near religion can help us do better.", "title": "Seeking models of LW\u2019s aversion to religion", "karma": 24, "af": false, "userid": "GgkoJ7nPzsdF7Dzsj", "id": "ZcXZCd2ZY7KBLb4vf"}, {"text": "Each round of the contest will produce the rules governing the next round. Any more information would be too much of a hint. This is dumb and not worth spending time on; I nonetheless look forward to any submissions.\u00a0", "title": "Contest for outlining rules for this contest.", "karma": 40, "af": false, "userid": "Q7NW4XaWQmfPfdcFj", "id": "zgXXxoKHDknAzCWPu"}, {"text": "Looking for:Any results / lines of research I haven't heard of in areas like interpretability, inductive biases, and ML theoryObscure papers or posts on things like double descent, simplicity bias, induction heads, and grokking", "title": "Favorite / most obscure research on understanding DNNs?", "karma": 12, "af": true, "userid": "vvfNH8EifESYswNHG", "id": "269kZdpdKtmLwHni4"}, {"text": "I recently gave a two-part talk on the big picture of alignment, as I see it. The talk is not-at-all polished, but contains a lot of stuff for which I don't currently know of any good writeup. Major pieces in part one:Some semitechnical intuition-building for high-dimensional problem-spaces.Optimization compresses information \"by default\"Resources and \"instrumental convergence\" without any explicit reference to agentsA frame for thinking about the alignment problem which only talks about high-dimensional problem-spaces, without reference to AI per se.The central challenge is to get enough bits-of-information about human values to narrow down a search-space to solutions compatible with human values.Details like whether an AI is a singleton, tool AI, multipolar, oracle, etc are mostly irrelevant.Fermi estimate: just how complex are human values?Coherence arguments, presented the way I think they should be done.Also subagents!Note that I don't talk about timelines or takeoff scenarios; this talk is just about the technical problem of alignment.Here's the video for part one:\u00a0Big thanks to Rob Miles for editing! Also, the video includes some good questions and discussion from Adam Shimi, Alex Flint, and Rob Miles.", "title": "The Big Picture Of Alignment (Talk Part 1)", "karma": 86, "af": true, "userid": "MEu8MdhruX5jfGsFQ", "id": "xdSDFQs4aC5GrdHNZ"}, {"text": "This post sketches two challenges to ARC's project around eliciting latent\nknowledge that differ somewhat in kind from the challenges ARC is most\nconcerned about. They relate to the difficulty in distinguishing beliefs\nfrom other representations.\nIntroduction\nThe problem of ELK, as outlined in ARC's technical\nreport,\nis to figure out: given an AI trained to complete a task (the 'task AI'),\nhow can we design a second AI (the 'reporter AI') that can answer\nquestions about how the task AI understands the world. If we can know what\nthe task AI is thinking, we can better assess whether its decisions are\ndangerous.\nThe central challenge identified by ARC to solving the problem of ELK\nconcerns the possibility that the reporter AI will not learn to answer\nquestions using what the task AI believes (e.g. answering honestly), but\ninstead answer questions using what a human observer might think is the\ncase on the basis of the available evidence (answering empathetically). We\ntrain the reporter AI to answer based on the facts as we understand them\n(and know the task AI to understand them) in relatively simple cases. We\nhope that the reporter will answer questions in more complex cases based\non how the task AI understands the world, but it might instead answer\nbased on what it thinks a human would mistakenly believe. If the human\nquestioner is ignorant of some complex details that the task AI knows, the\nreporter AI might choose to leave them out to tell a simpler story that\nwill satisfy the questioner. It is not obvious how to train an AI to\nanswer honestly rather than empathetically.\nWhile this is a significant challenge, I don't think it is the only\nchallenge for ELK. In thinking through the problem, I became worried about\nseveral others. I here describe two other challenges. Though they are\nsomewhat speculative, ARC aims for a solution to the problem of ELK that deals\nwith the worst case, and these challenges fall under that umbrella.\nEven if we figure out how to make an honest rather than a empathetic\nreporter, these challenges will remain.\n1.) The Right Representation Challenge\nWe do not know how the task AI might encode the the things that it\nbelieves, but what it knows about the world must somehow be reflected in\nits neural activity. For simplicity, let's suppose that the task AI utilizes\nexplicit independent representational states that are encoded in its\nnetwork in something vaguely like the way sentences are written in a book. The\nreporter AI would need to figure out where in the network these sentences\nare recorded. It would then have to figure out how to interpret the\nsentences and use them to provide answers to questions in English.\nThe reporter AI would also need to figure out which sentences recorded in\nthe task AI's network are really believed and which sentences are recorded\nfor other reasons. The task AI might produce a variety of representations\nthat it doesn't fully believe (i.e. doesn't straightforwardly rely on to\nmake decisions) for various purposes. If we just scan the network for\nsentences, we may find sentences that it doesn't quite believe. These may\nstill correlate with what it does believe in training scenarios, and so\na reporter AI might learn to use them to answer questions. If the reporter\nlatches on to those sentences, then it might give misleading answers once\noutside of training.\nConsider the following possibilities:\n\n\nThe task AI makes predictions using the simulated mental states of various agents. In the central case of the report, the task is to prevent\na thief from stealing a diamond -- the task AI might keep track of what\nthe thief believes is happening, what a security guard thinks is\nhappening, what other would-be thieves might later believe happened, etc.\nIf our AI is sufficiently careful, it might keep track of what some\npossible observers believe is happening. Perhaps it would simulate what\nother superintelligent AIs with access to its sensors would think is\nhappening. Depending on its purview, these might be reasonable things for\nit to consider. If there are some possible agents that happen to often\nagree with the task AI (such as hypothetical superintelligent AIs with\naccess to its sensors), our reporter might have a hard time figuring out\nwhich representations it should use to answer questions.\n\n\nThe task AI utilizes a parliamentarian architecture in which it's\nopinions are distributed over multiple specialized sub-agents, such that\nwhat it thinks is a function of those sub-agents. This has been a popular\nmodel for the human brain, so it seems conceivable that an AI would work\nthis way. The sub-agents would have their own beliefs. Sometimes those\nbeliefs would be wrong, but some sub-agents might be usually correct, at\nleast in training. Consider how you would go about deciding what\nthe US Senate currently collectively believes. A good strategy may just be to ask\nJoe Manchin. This strategy may stop working in 2023. Our reporter might similarly be inclined to identify one\nparliamentarian and report its representations. Even if the reporter decides to\naggregate the representations of the relevant agents, it may have trouble figuring\nout how to do this correctly. There may be many good but not quite right\napproaches to aggregation. It isn't obvious that an AI well trained on\na simple data set will succeed in finding the right way to aggregate.\n\n\n2.) The Creative Interpretation Challenge\nCreative interpretation is the process through which we infer implicit\nrepresentations by means of explicit representations. We use creative\ninterpretation when reading fiction or watching movies. Some aspects of\nthe plot are not explicitly depicted, but are implied by what is depicted.\nFor instance, movies rarely explicitly say how much time passes between\nscenes. We infer from contextual cues that it was a few hours, or a day,\nor a few weeks.\nIf a reporter does utilize some explicit representations, it likely won't\nrecord everything that it knows about the world explicitly. Some of its\nbeliefs must be inferred from what it does explicitly represent and how it\nworks with those representations. Some of its beliefs may depend on what\nis missing from its explicit representations or may depend on patterns of\nwhat it explicitly represents. A reporter is thus likely to need to do\nsome creative interpretation in order to suss out the task AI's full\nbeliefs.\nCreative interpretation of a sort may be stretched even to cases where\nthere are no explicit representations. Some beliefs may be more concrete\nor easier to access than others. In such cases, creative interpretation\nmight involve the inference of less easy to access facts from more easy to\naccess facts.\nGiven that a reporter needs to do some creative interpretation, the worry\nis that it may do more than is appropriate. Compare the legitimate\ncreative interpretation of a novel by a careful reader with the kind of\nthing a lazy student who didn't read a few chapters does when writing\na book report. They infer what must have happened from what they know.\nA reporter which is good at interpreting some things may be bad at\ninterpreting others. If it is lazy, it may just try to figure out from\ncontext what the task AI believes given a few snippets that are\nparticularly easy to decipher.\nThe challenge is to make sure that our reporter isn't inferring some of\nthe things it translates infelicitously. It is ok if our reporter is like\nthe average viewer of a movie. It is not ok if our reporter is like a lazy\nEnglish student bullshitting through a book report. We know what\ndistinguishes appropriate creative interpretation in books and movies. It\nisn't obvious how to distinguish good cases of creative inference from bad\ncases of inference in representational systems that we don't understand.\n", "title": "Two Challenges for ELK", "karma": 6, "af": true, "userid": "BbiEKYyKdsa8K5JXX", "id": "rxQbX2JpigjnbnL3A"}, {"text": "In which I review V\u00e1clav Havel\u2019s essay on \u201cThe Power of the Powerless\u201d before Scott Alexander totally steals my thunder.V\u00e1clav Havel\u2019s The Power of the Powerless is fascinating and surprising. The purpose of this review is to summarize it in such a tantalizing (and lengthy) way that you stop reading this and go look up Havel\u2019s original instead.Its context is Czechoslovakia in 1978. The country has been behind the Iron Curtain for thirty years, and ten years have passed since the brief experiment in political liberalization known as the \u201cPrague Spring\u201d which had been quickly choked off by a Soviet-led invasion.Havel was a Czech playwright with international renown, whose works were banned in his own country. In 1977 he helped to spearhead \u201cCharter 77\u201d\u2014a document that called on the government to respect human rights and its own Constitution.Charter 77 was prompted by the prosecution of members of the rock band \u201cThe Plastic People of the Universe\u201d\u2014the \u201cPussy Riot\u201d of their day. The government took the threat represented by the Charter seriously\u2014it persecuted its signers and made it illegal to print or distribute the text. In 1979, Havel would be sent to prison for four years for his role in advocating for the Charter.While Havel wrote The Power of the Powerless, he was under constant police surveillance and harassment for his Charter 77 activism. Meanwhile he was being noticed by freedom-loving people around the world and being hailed as a prominent Soviet-bloc dissident.The \u201cPost-Totalitarian\u201d SystemHe begins his argument by examining the system the \u201cdissidents\u201d are up against. It\u2019s a strange new form of tyranny\u2014a dictatorship not by a person or people, but by a bureaucracy and by certain principles and external contingencies (the primary one being that the Soviet Union intends to maintain Czechoslovakia as an obedient client state). So this is not the sort of dictatorship that can be threatened by attacks on a particular person or clique.Havel refers to this system as \u201cpost-totalitarian\u201d (at least in the translation I read). This is confusing, as it seems at first to imply that it is no longer totalitarian, which isn\u2019t the case. \u201cNeototalitarian\u201d might have been more apt.Czechoslovakia (and the Soviet Union and its client states collectively) was ruled in part by an ideology\u2014almost a religion\u2014that had proven to be a tempting refuge for confused, uprooted, and alienated people. \u201cOf course, one pays dearly for this low-rent home: the price is abdication of one\u2019s own reason, conscience, and responsibility, for an essential aspect of this ideology is the consignment of reason and conscience to a higher authority. The principle involved here is that the center of power is identical with the center of truth.\u201dHe anticipates the argument that though this ideology is dominant and everpresent, few people really believe its platitudes. They\u2019re like schoolchildren reciting the Pledge of Allegiance without paying attention to the words because it\u2019s what is expected of them. To meet this argument Havel introduces us to a grocer:The Obedient GrocerIn the window of the grocery is a sign that reads \u201cWorkers of the world, unite!\u201dWhat does the grocer mean by putting this sign in the window? Not that he is enthusiastic about global worker unity and wants to spread the word about it. The real message on the sign reads something like this: \u201cI, the grocer X\u2e3a, live here and I know what I must do. I behave in the manner expected of me. I can be depended upon and am beyond reproach. I am obedient and therefore I have the right to be left in peace.\u201d The message is not meant for the grocer\u2019s customers, but for officials who might suspect him or for informers who might care to turn him in.https://twitter.com/slatestarcodex/status/1495532084189966342If the grocer had to put that implicit message explicitly in his window, he might be embarrassed to be seen kowtowing in such a way, but by genuflecting in this indirect manner he saves face. If you ask him why he has the sign in his window, he can answer \u201cWhat\u2019s wrong with the workers of the world uniting?\u201d and protect his dignity. In this way a gesture of obedience and subservience is disguised by ideology as one of solidarity and empowerment.IdeologyIdeology, Havel summarizes, \u201coffers human beings the illusion of an identity, of dignity, and of morality while making it easier for them to part with them.\u2026 It is a veil behind which human beings can hide their own fallen existence, their trivialization, and their adaptation to the status quo. It is an excuse that everyone can use, from the greengrocer, who conceals his fear of losing his job behind an alleged interest in the unification of the workers of the world, to the highest functionary, whose interest in staying in power can be cloaked in phrases about service to the working class. The primary excusatory function of ideology, therefore, is to provide people, both as victims and pillars of the post-totalitarian system, with the illusion that the system is in harmony with the human order and the order of the universe.\u201dToday\u2019s tyrannies are too large and complex and cannot be held together by force and fear alone. They require their subjects not merely to submit passively to but to participate actively in their own subjection. Ideology is the mechanism to accomplish this.Individuals need not believe all these mystifications, but they must behave as though they did, or they must at least tolerate them in silence, or get along well with those who work with them. For this reason, however, they must live within a lie. They need not accept the lie. It is enough for them to have accepted their life with it and in it. For by this very fact, individuals confirm the system, fulfill the system, make the system, are the system.Once you make the decision to adopt the ideological mask for your subservient behavior\u2014like the grocer putting the sign in his window\u2014you become a part of this glue that affixes ideology over reality and gives ideology power. It doesn\u2019t matter that you inwardly don\u2019t really believe the explicit message of the ideology, because the explicit message isn\u2019t the important one, and it doesn\u2019t matter if you believe it or not so long as you agree to continue acting as though you did.While ideology is central to the post-totalitarian power structure, the interests of the structure itself are paramount, and the ideology\u2014or the interpretation of it anyway\u2014will tend to be subordinate to it. The tighter the control that the government exercises over communication and expression, the better it will be able to enforce and manipulate the orthodox interpretation of the ideology and the more the ideology will come to float far above reality, more-or-less completely detached from it: \u201ca world of appearances, a mere ritual, a formalized language deprived of semantic contact with reality and transformed into a system of ritual signs that replace reality with pseudo-reality.\u201dFor example, in China today, \u201ccommunism\u201d is still the name given to the sacred ideology that is said to govern the system, but its meaning has come a long way: now it means the total state-enforced subjection of the working class to a small minority of fantastically wealthy private owners of the means of production (the wealth of China\u2019s National Congress makes the U.S. Congress look like a bunch of ordinary middle-class schmoes). It\u2019s still \u201ccommunism\u201d you\u2019re expected to be loyal to, the flag is still red, that\u2019s still Mao\u2019s face staring back at you from the money, and you can still signal your loyalty to the system with the same empty platitudes about the rule of the working class\u2014but the system doesn\u2019t care about the explicit meaning of the platitudes any more than you do.Because ideology can become so absurdly detached from reality in this way, it can be a real art to try to maintain your fiction of adherence to it. Because of this, \u201cthe virtuosity of the ritual\u201d comes to be more important than actually being able to attach meaning to what you are doing or saying. Aspects of the ritual and of the ideology come to represent only each other and never come down to earth. This can cause the ideology to detach even from the bureaucracy it was designed to serve, until it becomes an independent, malignant, power-appropriating memetic menace all its own.At this stage, when the ideology is serving itself more than it serves the bureaucracy, the power structure stops attracting the ambitious and starts attracting the faceless\u2014empty suits\u2014people who can articulately and cleverly engage in the virtuosity of the ritual but who don\u2019t seem to have much going on outside of this arena of rhetorical swamp gas and who are so thoughtless that they have thoroughly internalized the ideology\u2019s criteria of success and prestige.If a Frankenstein\u2019s Monster ideology like this is so powerful that it can eventually even conquer and press into service the post-totalitarian dictatorship itself, what hope do we have? It is this: the ideology \u201cis built on lies [and] works only as long as people are willing to live within the lie.\u201dWho Enforces the Ideology?What if our grocer were to stop living in the lie in one little way: by not hanging the sign in his window that means nothing to him. Well, what possible difference could that make? It\u2019s unlikely any of his customers even notice the sign. The sign is not meant to be read individually, anyway, but \u201cto form part of the panorama of everyday life.\u201d It is as a contributor to this panorama that the grocer serves the system. The message of the panorama is not the message on the sign but this message: \u201cthis sign-hanging is what the ideology demands of us today and we are complying.\u201d Those who hang the signs are complying with the ideology and expressing the ideology\u2019s demands by the same action: simultaneously the voice of command and the posture of submission.This has a pernicious psychological effect. The latent consciousness that you are both victim and perpetrator of this ideological control influences you to identify with the ideology. You feel better both in submitting and in commanding if you think you are doing so in service of an ideology you believe in, so you try to believe that you believe in this weird, untethered, nonsensical ideology\u2014and you come to see attacks against the ideology as threats to you personally.Thus the conflict between the aims of life and the aims of the system is not a conflict between\u2026 the rulers and the ruled.\u2026 In the post-totalitarian system, this line runs de facto through each person, for everyone in his own way is both a victim and a supporter of the system. What we understand by the system is not, therefore, a social order imposed by one group upon another, but rather something which permeates the entire society and is a factor in shaping it, something which may seem impossible to grasp or define (for it is in the nature of a mere principle), but which is expressed by the entire society as an important feature of its life.\u2026It can happen and did happen only because there is obviously in modern humanity a certain tendency toward the creation, or at least the toleration, of such a system. There is obviously something in human beings which responds to this system, something they reflect and accommodate, something within them which paralyzes every effort of their better selves to revolt. Human beings are compelled to live within a lie, but they can be compelled to do so only because they are in fact capable of living in this way. Therefore not only does the system alienate humanity, but at the same time alienated humanity supports this system as its own involuntary master plan, as a degenerate image of its own degeneration, as a record of people\u2019s own failure as individuals.Alongside the striving for dignity, integrity, and personality that we naturally value and treasure in ourselves, says Havel, there lives a less-acknowledged, sinister striving \u201cto merge with the anonymous crowd and to flow comfortably along with it down the river of pseudolife.\u201dThe Disobedient GrocerSo if going along with the lie means not just submitting to the system, but enforcing the system, what is the alternative for people who find the system intolerable? What if the grocer stops participating in the lie and starts living in the truth?Well, first and most obviously, the system retaliates against him. And since the system is enforced not just by the officials who overtly persecute him but by everyone, they as part of their life-in-the-lie must also shun him. Does he then just become a vivid display of the danger of rebellion\u2014useful to the regime and no threat to it? According to Havel, no: the grocer\u2019s quixotic act is indeed a serious threat to the system:By breaking the rules of the game, he has disrupted the game as such. He has exposed it as a mere game. He has shattered the world of appearances, the fundamental pillar of the system. He has upset the power structure by tearing apart what holds it together. He has demonstrated that living a lie is living a lie. He has broken through the exalted fa\u00e7ade of the system and exposed the real, base foundations of power. He has said that the emperor is naked. And because the emperor is in fact naked, something extremely dangerous has happened: by his action, the greengrocer has addressed the world. He has enabled everyone to peer behind the curtain. He has shown everyone that it is possible to live within the truth. Living within the lie can constitute the system only if it is universal. The principle must embrace and permeate everything. There are no terms whatsoever on which it can co-exist with living within the truth, and therefore everyone who steps out of line denies it in principle and threatens it in its entirety.The biggest vulnerability of the ideologically-ruled post-totalitarian system is its unmooring from reality. But this vulnerability only becomes a liability when the system is brought into contrast with reality\u2014and the system works hard to ensure that this doesn\u2019t happen; that\u2019s the point of the panorama of platitudes and of the conscription of the grocer to help deploy it.In the post-totalitarian system, therefore, living within the truth has more than a mere existential dimension (returning humanity to its inherent nature), or a noetic dimension (revealing reality as it is), or a moral dimension (setting an example for others). It also has an unambiguous political dimension. If the main pillar of the system is living a lie, then it is not surprising that the fundamental threat to it is living the truth. This is why it must be suppressed more severely than anything else.Truth-ForceHavel believes the lie can never gain total control of someone. There\u2019s always a seed of truth remaining. The lie, because it so vigorously claims to be the truth and claims to be a route to authenticity, reinforces the idea that there is such a thing as truth and authenticity and that these things are valuable. The truth, then, is not exterminated in the post-totalitarian system, but continues to flow through society like an underground stream. Those who decide to live in the truth are not, therefore, isolated and having to invent themselves from scratch, but they\u2019re able to tap into this reservoir.Because the post-totalitarian system is so detached from and hostile to the truth, the flow of this underground stream can evade its notice, \u201cand by the time it finally surfaces into the light of day as an assortment of shocking surprises to the system, it is usually too late to cover them up in the usual fashion. Thus they create a situation in which the regime is confounded, invariably causing panic\u2026\u201dThis manner of striking at the main vulnerability of the post-totalitarian ideological system is a peculiar form of opposition: it doesn\u2019t take place in the halls of power or in the voting booth or in conspiratorial revolutionary cells or in strikes and street protests, but at \u201cthe level of human consciousness and conscience, the existential level\u2026 in the fifth column of social consciousness, in the hidden aims of life, in human beings\u2019 repressed longing for dignity and fundamental rights\u2026 This power does not participate in any direct struggle for power; rather, it makes its influence felt in the obscure arena of being itself.\u201d But once established there, it can and does contribute, in subtle but definite ways, to such things as \u201ca social movement, a sudden explosion of civil unrest, a sharp conflict inside an apparently monolithic power structure, or simply an irrepressible transformation in the social and intellectual climate\u201d and thereby has powerful political consequences.The Prague Spring itself, Havel says, only superficially was a conflict between groups vying for political power. Looked at more closely, it appears as \u201cthe final act and the inevitable consequence of a long drama originally played out chiefly in the theatre of the spirit and the conscience of society\u201d and prompted by a few individuals with no pretensions to political power who simply decided to begin living in the truth. The Prague Spring wasn\u2019t the birth of something promising that was then cut down, but the above-ground blooming of something that continues to spread its mycelia underground.And this is why these post-totalitarian ideological systems are so intolerant of leaks and dissent. Why was Solzhenitsyn hounded out of Russia? For the same reason Ed Snowden was hounded into it: \u201ca desperate attempt to plug up the dreadful wellspring of truth, a truth which might cause incalculable transformations in social consciousness, which in turn might one day produce political debacles unpredictable in their consequences.\u201dBut living the truth isn\u2019t just a matter of exposing facts. \u201cIt can be any means by which a person or a group revolts against manipulation: anything from a letter by intellectuals to a workers\u2019 strike, from a rock concert to a student demonstration, from refusing to vote in the farcical elections to making an open speech at some official congress, or even a hunger strike, for instance.\u2026 every free expression of life indirectly threatens the post-totalitarian system politically, including forms of expression to which, in other social systems, no one would attribute any potential political significance, not to mention explosive power.\u201d There\u2019s a reason why Charter 77 was prompted by the prosecution of a rock-and-roll band.Living in the Truth as a Moral, not Political RevolutionHavel thinks the political scenario he has described is a symptom of a widespread epidemic of ethical pathology. The fact that living-in-a-lie has emerged as a self-perpetuating political system suggests that something has gone badly wrong at the core of society and in the moral centers of the people who make it up. It reveals that we have demoralized ourselves by abandoning our senses of responsibility in order to dissolve our identities in the solvent of mass culture and consumerism.Seen in this light, the political side effects of living in the truth are secondary to its function of allowing us to reclaim our moral agency. Indeed, because the beneficial political effects of living in the truth are so diffuse and difficult to trace, and the consequences of confronting the system in this way are in contrast so personal and visceral and likely, nobody would be likely to make the attempt if there were not this additional imperative.Politics Under Post-TotalitarianismHow can a citizen participate in the political process that governs his or her society? In the post-totalitarian system, the normal way to participate is by living in and helping to enforce the lie that perpetuates the stranglehold of the system over the lives of its subjects. Parts of this lie are the farcical processes\u2014voting or pleading with representatives and so forth\u2014that mimic (and, according to the lie, constitute) political deliberation and action. The only form of participation permitted to you is one that helps you propel the system that smothers politics, not one that actually allows you to make decisions together with your fellow citizens.What if you want more than that: participatory politics of equals, rather than the obedient pseudopolitics of the galley slave? Do you participate in the fake elections more vigorously? lobby your fake representatives more persuasively? These things are hopeless and dangerous and make people cynical: if you don\u2019t see beyond the officially-sanctioned outlets of pseudopolitics, there seems to be no point to politics at all.Living in the truth is the remaining method of political activity\u2014the last alternative to the pseudopolitics that just makes things worse. This can sometimes trip up well-meaning activists, who may overestimate the usefulness of confrontational and bold \u201cpolitical\u201d acts\u2014that is, acts within the permitted pseudopolitical sphere\u2014and thereby bolster the perceived legitimacy of that sphere. This is another way of participating in the lie. Instead, effective activists need to understand that they\u2019re in a new sort of system with new rules, and they need to be imaginative and not try to build within either the pseudopolitical framework or within old-fashioned models of dissent.To foment an opposition, don\u2019t paint a picture of a better set of rulers or a new political party or a constitutional amendment or electoral reform or any of that perennial hogwash. Instead, aim concretely and directly at \u201cthe continuing and cruel tension between the complex demands of [the post-totalitarian] system and the aims of life, that is, the elementary need of human beings to live, to a certain extent at least, in harmony with themselves\u2026 in a bearable way, not to be humiliated by their superiors and officials, not to be continually watched by the police, to be able to express themselves freely, to find an outlet for their creativity, to enjoy legal security, and so on.\u201dWe cannot free ourselves by overthrowing a tyranny and imposing freedom from above the way communism was imposed on us from above; instead we have to strive to become free and then impose our freedom on the government from below.Dissent and OppositionIn a democracy, the opposition is a party currently out of power working through legitimate channels within the system to try to gain or exert power. In a traditional dictatorship, the opposition is those people who are trying to replace the dictatorship with something else.Havel says that Charter 77 is not an opposition in these senses, though some of the signers may have aspirations in this direction. It isn\u2019t a political party with aspirations of gaining political power, and it doesn\u2019t have an alternative system it hopes to install in place of the present state. Nonetheless, \u201cWestern journalists\u201d have seized on Charter 77 as an \u201copposition movement\u201d and the Czech government treats it as an oppositional organization simply because it \u201cmanages to avoid total manipulation and\u2026 therefore denies the principle that the system has an absolute claim on the individual.\u201d\u201cOpposition\u201d is a tricky word. Once the label gets attached to you, you can expect hassles from the state: you are considered a traitor and can expect treatment ranging from character assassination to outright execution. But it\u2019s also deceptive in that it defines your work not in its own terms or in how it relates to reality, but in terms of the system of lies you\u2019re trying to escape: rather than living in the truth you find yourself defined as living in opposition to the lie, and in a way that becomes contaminated with the lie (sources say these protesters oppose the workers of the world uniting).Some people who are trying to live in the truth in the Soviet bloc were called \u201cdissidents\u201d\u2014something Havel belittles as a sort of Western media-granted celebrity status (he always puts the word in quotation marks).One danger of this label is that it comes to sound like a profession\u2014like you have to have a license to dissent, or like it\u2019s only for people who have made it their special vocation. In fact, \u201cdissidents\u201d are not people who \u201cconsciously decided to be professional malcontents\u201d but ordinary people \u201cwho are doing what they feel they must and, consequently, who find themselves in open conflict with the regime.\u201dThe label has a way of separating a small group of people into an elevated clique and treating them like a tiny interest group distinct from society at large: journalists ask \u201cis the government going to respect the rights of the dissidents\u201d rather than \u201cis the government going to respect everyone\u2019s right to live in the truth?\u201dIt is truly a cruel paradox that the more some citizens stand up in defense of other citizens, the more they are labeled with a word that in effect separates them from those \u201cother citizens.\u201dSmall-Scale WorkWhat of the argument that it\u2019s worth making small concessions to the lie in order to be granted the limited freedom and resources necessary to do good work? Why not work within the system and try to make it better or to ameliorate its problems?There\u2019s something to this: \u201cIt is hard to say how much worse things would be if there were not many hard-working people who simply refuse to give up and try constantly to do the best they can, paying an unavoidable minimum to living within a lie so that they might give their utmost to the authentic needs of society. These people assume, correctly, that every piece of good work is an indirect criticism of bad politics, and that there are situations where it is worthwhile going this route, even though it means surrendering one\u2019s natural right to make direct criticisms.\u201dBut Havel says that this option has become less tenable in Czechoslovakia. Things have become too rotten. The compromises are too overwhelming. Too much good work ends up being hijacked and parasitized to feed the corrupting engine of the system. When the system requires total adherence to an ideology that has become totally unmoored from the truth, how much good can you do without butting up against the ideology\u2019s limits? If you decide to stay safe, you lose your ability to do good; if you decide to keep doing good, you find yourself suddenly a \u201cdissident\u201d in spite of your modest intentions.But this is not one-size-fits-all advice. If you find that in your situation you can do the most good by making tactical concessions to the lie, Havel advises that you make your judgment call and do what you can. It is possible to live honorably this way. If you do the right thing and find out that (surprise!) it\u2019s also permitted\u2014that\u2019s a marvelous discovery.Living in the truth is not necessarily overtly oppositional or dissenting at all. Some of it is subtle and not particularly visible\u2014\u201cyou simply straighten your backbone and live in greater dignity as an individual.\u201d Other parts are more visible and shared: \u201ceverything from self-education and thinking about the world, through free creative activity and its communication to others, to the most varied free, civic attitudes, including instances of independent social self-organization.\u201d When there is enough of this going on, it forms the soil in which more overtly and consciously political initiatives can grow.A movement of \u201cdissent\u201d requires a healthy substrate of independent, grassroots social activity and organization; this in turn depends on individuals willing to seed such independent ways of living by living in the truth as individuals even in the absence of this social support structure.But remember those quotation marks around \u201cdissent\u201d\u2014it\u2019s not so much that self-consciously dissident groups are going to emerge from this strata of independent ways of living, but that some of these independent ways of living are going to be persecuted by an intolerant government and will thereby become dissident activities.People and PoliticsThe \u201cdissident\u201d movements in the Soviet bloc, Havel says, are defensive: that is, they are defending human beings against a smothering anti-human system. He contrasts this with political movements, which may have an offensive as well as defensive program, for instance a program to institute a different sort of system or to reform the existing one in a particular way.Havel thinks this is not a liability but an advantage: \u201cit forces politics to return to its only proper starting point\u2026 individual people.\u201d He thinks that things have gotten so bad in his country that the central issue isn\u2019t about what shape the political system ought to take but about what to do for the people who are victimized by the political system. In contrast: \u201cIn the democratic societies, where the violence done to human beings is not nearly so obvious and cruel, this fundamental revolution in politics has yet to happen, and some things will probably have to get worse there before the urgent need for that revolution is reflected in politics.\u201dEvery society, of course, requires some degree of organization. Yet if that organization is to serve people, and not the other way around, then people will have to be liberated and space created so that they may organize themselves in meaningful ways. The depravity of the opposite approach, in which people are first organized in one way or another (by someone who always knows best \u201cwhat the people need\u201d) so they may then allegedly be liberated, is something we have known on our own skins only too well.Revolt and LawMaybe it\u2019s time to revolt, to overthrow the system entirely and to install a new one by force. But such a revolt is difficult to imagine in the post-totalitarian system. Such a revolt would involve two opposed forces of roughly equivalent strength meeting in the arena of actual force and political power. But in the post-totalitarian system:Society is not sharply polarized on the level of actual political power, but, as we have seen, the fundamental lines of conflict run right through each person. In this situation, no attempt at revolt could ever hope to set up even a minimum of resonance in the rest of society, because that society is soporific, submerged in a consumer rat race and wholly involved in the post-totalitarian system (that is, participating in it and acting as agents of its automatism), and it would simply find anything like revolt unacceptable. It would interpret the revolt as an attack upon itself and, rather than supporting the revolt, it would very probably react by intensifying its bias toward the system, since, in its view, the system can at least guarantee a certain quasi-legality. Add to this the fact that the post-totalitarian system has at its disposal a complex mechanism of direct and indirect surveillance that has no equal in history and it is clear that not only would any attempt to revolt come to a dead end politically, but it would also be almost technically impossible to carry off. Most probably it would be liquidated before it had a chance to translate its intentions into action. Even if revolt were possible, however, it would remain the solitary gesture of a few isolated individuals and they would be opposed not only by a gigantic apparatus of national (and supranational) power, but also by the very society in whose name they were mounting their revolt in the first place. (This, by the way, is another reason why the regime and its propaganda have been ascribing terroristic aims to the \u201cdissident\u201d movements and accusing them of illegal and conspiratorial methods.)\u201cDissident\u201d movements tend to have a strong bias against violent change, though not one that veers dogmatically into pacifism:\u201c[D]issidents\u201d tend to be skeptical about political thought based on the faith that profound social changes can only be achieved by bringing about (regardless of the method) changes in the system or in the government, and the belief that such changes\u2014because they are considered \u201cfundamental\u201d\u2014justify the sacrifice of \u201cless fundamental\u201d things, in other words, human lives.The \u201cdissident\u201d view is that you don\u2019t change the system first, but that the system will change incidentally as changes take place in the people who uphold and evolve the system. They \u201cdo not shy away from the idea of violent political overthrow because the idea seems too radical, but on the contrary, because it does not seem radical enough.\u201dThus an attitude that turns away from abstract political visions of the future toward concrete human beings and ways of defending them effectively in the here and now is quite naturally accompanied by an intensified antipathy to all forms of violence carried out in the name of a better future, and by a profound belief that a future secured by violence might actually be worse than what exists now; in other words, the future would be fatally stigmatized by the very means used to secure it.Havel notes that many of the \u201cdissident\u201d groups, like the Charter 77 movement, claim to be acting in the defense of various doctrines of international or national law\u2014\u201csuch as the Universal Declaration of Human Rights, the International Covenants on Human Rights, the Concluding Act of the Helsinki Agreement, and the constitutions of individual states.\u201d Why might this be? After all, it is completely na\u00efve to think that their governments actually respect these laws, have any interest in respecting them, or can be compelled by some greater force to respect them. Is pretending that the law is meaningful just another way of living in the lie?Havel defends this legalistic approach, at least as it applies to the post-totalitarian system.Such a system is not dominated by power-wielding groups or individuals, the way a traditional totalitarian system is, but by bureaucracy and ideology. It \u201cis utterly obsessed with the need to bind everything in a single order: life in such a state is thoroughly permeated by a dense network of regulations, proclamations, directives, norms, orders, and rules.\u201d The legal code is one expression of the ideology underlying the system, and of one of the lies of the system: that it is well-regulated, governed by law, eager for justice, and vigorous in its defense of human rights.If an outside observer who knew nothing at all about life in Czechoslovakia were to study only its laws, he would be utterly incapable of understanding what we were complaining about. The hidden political manipulation of the courts and of public prosecutors, the limitations placed on lawyers\u2019 ability to defend their clients, the closed nature, de facto, of trials, the arbitrary actions of the security forces, their position of authority over the judiciary, the absurdly broad application of several deliberately vague sections of that code, and of course the state\u2019s utter disregard for the positive sections of that code (the rights of citizens): all of this would remain hidden from our outside observer.\u2026But that is not all: if our observer had the opportunity to study the formal side of the policing and judicial procedures and practices, how they look \u201con paper,\u201d he would discover that for the most part the common rules of criminal procedure are observed: charges are laid within the prescribed period following arrest, and it is the same with detention orders. Indictments are properly delivered, the accused has a lawyer, and so on. In other words, everyone has an excuse: they have all observed the law.The legal code is also the mechanism by which the various parts of the system communicate with each other and establish their roles and duties. It\u2019s a sort of scaffolding. \u201cIt provides their whole game with its rules and engineers with their technology.\u2026 Without the legal code functioning as a ritually cohesive force, the post-totalitarian system could not exist.\u201dSo this is the reasoning behind the legalistic approach. There\u2019s no need to pretend that the law is anything but what it is, but that doesn\u2019t mean that the law cannot be used to advantage. The system depends on it and, to some extent anyway, must flow through the channels it defines in order to function.I have frequently witnessed policemen, prosecutors, or judges\u2014if they were dealing with an experienced Chartist [(Charter-signer)] or a courageous lawyer, and if they were exposed to public attention (as individuals with a name, no longer protected by the anonymity of the apparatus)\u2014suddenly and anxiously begin to take particular care that no cracks appear in the ritual. This does not alter the fact that a despotic power is hiding behind that ritual, but the very existence of the officials\u2019 anxiety necessarily regulates, limits, and slows down the operation of that despotism.Building Parallel StructuresThere\u2019s another choice besides revolt and legalism. Rather than try to overthrow the current system or turn its rulebook against it, you can extend your participation in ways of life that substitute for the system\u2019s.In a way this naturally follows from the independent ways of living mentioned earlier. As more people live in the truth and develop these independent ways of living, they will come to do so together, interacting and creating new ways of organizing and structuring these independent activities. New organizations and structures will fill spaces that the State has left unfilled or that it fails to monopolize. Some examples of this in Czechoslovakia were the underground music scene and the samizdat publishing and distribution industry, but the form had potential to extend further, into such things as \u201cparallel forms of education (private universities), parallel trade unions, parallel foreign contacts, to a kind of hypothesis on a parallel economy\u201d and eventually a parallel state (Havel attributes these ideas to V\u00e1clav Benda).This approach is people-centered, it\u2019s not just aimed at some future benefit but is beneficial in the here-and-now, it\u2019s practical and not just theoretical, it\u2019s something everyone can participate in, and it\u2019s radical in the sense that it works directly at the root of people\u2019s day-to-day lives rather than in the superstructure of the system.Because people who decide to live in the truth are, at first anyway, isolated rebels distinct from society, there is a temptation to see them as individualists in retreat from society: outcast or in isolation (the title \u201cdissident\u201d is another way of emphasizing this point of view). It is more accurate to see them not as retreating from society but advancing before it: as experimental pathfinders beating new trails and inviting society to follow their lead. Similarly, when groups of people develop parallel structures that substitute for those sanctioned by the system, this is not an act of monastic retreat or ghettoization but one of experimental advance.So be careful not to see this parallel world as an end in itself, as though once we get it established we will be able to migrate there and leave the other world behind. So long as the post-totalitarian system rules, our participation in the parallel world will be tainted by the same schizophrenia that everyone suffers: trying to live with partial respect for the truth and partial subservience to the lie. The point is not to establish an underground in which you can enjoy furtive moments of freedom, but to free everyone (a sort of \u201cmah\u0101y\u0101na\u201d agorism perhaps).The system will react to these parallel structures in two ways: by trying to repress them and by trying to coopt them. The repression is straightforward: practices will be banned, practitioners persecuted. Cooptation is a little more subtle. The system may adopt those aspects of the parallel world that are especially popular or effective or difficult to control. This may be a positive thing, something of a real reform, but it often is just a way of rendering the parallel world safe to the system\u2014defanging it, integrating it into the lie, and slapping a patina of progress and liberality onto the system\u2019s fa\u00e7ade. It can be confusing, if not deliberately baffling.But the cooptation works both ways, and it will be through such grudging attempts at compromise that the post-totalitarian system will eventually be defeated. It is not the case either that all of these attempts at cooptation are bad and should be resisted or that all of them are good and should be encouraged. Is your attempt at a parallel structure partially contaminated by the ruling system? Of course it is. Don\u2019t let this discourage you; don\u2019t let the perfect be the enemy of the good. Keep your eyes on the truth and keep moving forward in the direction it points.The post-totalitarian system and the parallel system of people developing ways of living in the truth together are two incompatible worlds, and one of them must go: \u201ceither the post-totalitarian system will go on developing\u2026 thus inevitably coming closer to some dreadful Orwellian vision of a world of absolute manipulation, while all the more articulate expressions of living within the truth are definitely snuffed out; or the independent life of society (the parallel polis), including the \u2018dissident\u2019 movements, will slowly but surely become a social phenomenon of growing importance, taking a real part in the life of society with increasing clarity and influencing the general situation.\u201dWhat will prompt the coup de gr\u00e2ce is impossible to predict. It will probably be some accident of history or the culmination of trends that are only clear in retrospect. Our task is not to plot this revolution but to lay the groundwork that makes it possible or inevitable.The Crisis of Contemporary Technological SocietyThe problem we\u2019re faced with is deeper than the specific post-totalitarian system in the Soviet bloc, and will not end when it does:Technology\u2014that child of modern science, which in turn is a child of modern metaphysics\u2014is out of humanity\u2019s control, has ceased to serve us, has enslaved us and compelled us to participate in the preparation of our own destruction. And humanity can find no way out: we have no idea and no faith, and even less do we have a political conception to help us bring things back under human control. We look on helplessly as that coldly functioning machine we have created inevitably engulfs us, tearing us away from our natural affiliations (for instance, from our habitat in the widest sense of that word, including our habitat in the biosphere) just as it removes us from the experience of Being and casts us into the world of \u201cexistences.\u201dHere, too, we need a revolution, but it must be more fundamental, not \u201cmerely philosophical, merely social, merely technological, or even merely political\u201d but existential, \u201ca generally ethical\u2014and, of course, ultimately a political\u2014reconstitution of society.\u201dThe post-totalitarian system is only one aspect\u2014a particularly drastic aspect and thus all the more revealing of its real origins\u2014of this general inability of modern humanity to be the master of its own situation. The automatism of the post-totalitarian system is merely an extreme version of the global automatism of technological civilization. The human failure that it mirrors is only one variant of the general failure of modern humanity.Western liberal democracy\u2014that famous \u201cend of history\u201d\u2014is not an adequate response to this crisis. \u201cIt may even be said that the more room there is in the Western democracies (compared to our world) for the genuine aims of life, the better the crisis is hidden from people and the more deeply do they become immersed in it.\u2026\u201dPeople are manipulated in ways that are infinitely more subtle and refined than the brutal methods used in the post-totalitarian societies. But this static complex of rigid, conceptually sloppy, and politically pragmatic mass political parties run by professional apparatuses and releasing the citizen from all forms of concrete and personal responsibility; and those complex focuses of capital accumulation engaged in secret manipulations and expansion; the omnipresent dictatorship of consumption, production, advertising, commerce, consumer culture, and all that flood of information: all of it, so often analyzed and described, can only with great difficulty be imagined as the source of humanity\u2019s rediscovery of itself.\u2026 In a democracy, human beings may enjoy many personal freedoms and securities that are unknown to us, but in the end they do them no good, for they too are ultimately victims of the same automatism, and are incapable of defending their concerns about their own identity or preventing their superficialization or transcending concerns about their own personal survival to become proud and responsible members of the polis, making a genuine contribution to the creation of its destiny.For this reason, it would be short-sighted for post-totalitarian dissidents to set their sights on establishing a democracy of this sort as anything but a temporary stepping stone to a society of dignity. Confronted with a \u201cpost-totalitarian\u201d situation, we need a \u201cpost-democratic\u201d solution.What Is to Be Done?What we really need is a reconstitution of the larger human order of which the political order is just a part. And this means a society-wide moral revolution of such things as \u201c[a] new experience of being, a renewed rootedness in the universe, a newly grasped sense of higher responsibility, a newfound inner relationship to other people and to the human community.\u2026 In other words, the issue is the rehabilitation of values like trust, openness, responsibility, solidarity, love.\u201dThe political reformation will follow naturally from this. It is hazardous to try to predict in advance what it will look like, but there are some aspects that we might anticipate: It will probably rely more on smaller units of organization that are based on natural communities of people with shared interests (rather than big states with arbitrary geographical boundaries). These units of organization will not have monopolistic impulses but will be welcoming of new and of parallel structures. They will be less formal\u2014not like organizations but like communities. Their authority will be based on their utility, not on sovereignty. They will be more likely to spring up ad-hoc as needed rather than being on-going institutions. Individuals who have authority will not have it by virtue of their title or position, but because of the trust they have earned in taking responsibility for the specific tasks at issue. This may mean that they have more political power than the politicians today.And this goes for economic organization as well as political organization: Self-managed, purposeful units\u2014not autonomous, self-interested corporations with subservient workers who have no stake in or responsibility for their work.And come to think of it\u2026 these little clusters of people living in the truth, finding each other, forming human bonds of solidarity by desperate necessity, creating experiments in parallel structures based on concrete human needs\u2014aren\u2019t they demonstrations of the sort of world we\u2019re trying to feel our way toward?", "title": "Review: V\u00e1clav Havel\u2019s \u201cThe Power of the Powerless\u201d", "karma": 40, "af": false, "userid": "eJdEKa8cRgmPuLNvT", "id": "KjigA9tf8krrztC85"}, {"text": "The third annual review is complete. We spent two months nominating, reviewing, and voting on the best posts of 2020. After lots of thoughtful evaluation, it's time to award some prizes!\u00a0As described in the voting results post, there are two prize pools for posts. Lightcone Infrastructure contributed $10k to the Review Vote prize pool, and LessWrong users donated $1770 to the Unit of Caring prize pool (which Lightcone matched 1:1, bringing it up to $3540).\u00a0Lightcone Infrastructure is also awarding $3600 in prizes for people who wrote reviews[1]\u00a0for posts.Prizes for PostsThe Review Vote PrizesWe've awarded a $1000 Top Prize to each of the following authors:catherio, for microCOVID.org, a tool to estimate covid riskAjeya Cotra, for Draft report on AI Timelinesevhub, for An Overview of 11 Proposals for Building Safe Advanced AIjohnswentworth for When Money is Abundant, Knowledge is the Real Wealth, Alignment by Default, The Pointers Problem, and Coordination as Scarce ResourceWe've awarded a $500 Honorable Mention to each of:Alkjash, for Pain is not the unit of effortMark Xu, for The Solomonoff Prior is MalignJacob Falkovich, for Seeing the SmokeAlex Flint, for The Ground of OptimizationZvi, for Simulacra Levels and their InteractionsRichard Ngo, for AGI Safety from First PrinciplesPaul Christiano, for Inaccessible InformationSteven Byrnes for My Computational Framework for the Brainabramdemski for Most Prisoner's Dilemmas are Stag Hunts; Most Stag Hunts are Schelling Problems, and An Orthodox Case Against Utility FunctionsDaniel Kokotajlo, for Cort\u00e9s, Pizarro, and Afonso as Precedents for TakeoverScott Garrabrant, for Introduction to Cartesian FramesRafael Harth, for Inner Alignment, Explain Like I'm 12 EditionHow were prizes determined?The LessWrong moderation team looked over the results of the vote, sliced four different ways:Total votes from all users;Total votes from 1000+ karma users;Total votes from Alignment Forum users;Weighted votes from all users, but with 1000+ karma users weighted 3x.\u00a0That gave us some sense of how different users related to posts, and what they got out of them. In all cases, approximately the same posts showed up at the top of the rankings, albeit in somewhat different orders. I'm excited to reward all of them prizes for contributing to important intellectual progress.\u00a0Two things \u00a0stuck out between the various rankings:\u00a0In the Alignment Forum vote, Draft Report on AI Timelines won by a landslide.\u00a0In the \"All Users\" vote, the MicroCOVID.org announcement won by a landslide.\u00a0Thoughts on the microCOVID.org AnnouncementSeveral people asked, upon seeing microCOVID.org in the review: \"Is this what the Review is meant to reward? It sure was useful, but was it 'intellectual progress'? Was it 'timeless'? What would it mean to put it in a book?\"We haven't yet made our final call on whether/how to do books this year, but if I were to print microCOVID, I'd specifically be interested in printing the whitepaper. Unlike the announcement (which would be quite weird to read in a book), the whitepaper is clearly a piece of enduring intellectual labor.I actually took some pretty timeless, valuable lessons from microCOVID \u2013 the schema that you can break \"risk\" into units that linearly add up was helpful for my overall thinking, and I expect to be relevant in other domains. And the conceptual breakdown of splitting up \"person risk\" and \"activity risk\" was useful.\u00a0I think microCOVID was a great instance of applied rationality that built off a lot of concepts in our \u00a0ecosystem, and translated them into a concrete product.\u00a0Thoughts on Draft Report on AI TimelinesAjeya's Draft Report on AI Timelines inspired a ton of discussion: Eliezer wrote a response post, and Holden wrote a counter-response, and there was further discussion on each post. I plan to include both follow-up posts as contextual reviews in the Best of 2020 sequences and/or books.Daniel Kokotajlo wrote this review articulating why the post seemed important to him (edited slightly for brevity):Whenever people ask me for my views on timelines, I go through the following mini-flowchart:1. Have you read Ajeya's report?\u2013 If yes, launch into a conversation about the distribution over 2020's training compute and explain why I think the distribution should be substantially to the left, why I worry it might shift leftward faster than she projects, and why I think we should use it to forecast AI-PONR instead of TAI.\u2013 If no, launch into a conversation about Ajeya's framework and why it's the best and why all discussion of AI timelines should begin there.So, why do I think it's the best? In a nutshell: Ajeya's framework is to AI forecasting what actual climate models are to climate change forecasting (by contrast with lower-tier methods such as \"Just look at the time series of temperature over time / AI performance over time and extrapolate\" and \"Make a list of factors that might push the temperature up or down in the future / make AI progress harder or easier,\" and of course the classic \"poll a bunch of people with vaguely related credentials.\")Ajeya's model makes only a few very plausible assumptions. This is underappreciated, I think. People will say e.g. \"I think data is the bottleneck, not compute.\" But Ajeya's model doesn't assume otherwise! If you think data is the bottleneck, then the model is more difficult for you to use and will give more boring outputs, but you can still use it.\u00a0I think a lot of people are making a mistake when they treat Ajeya's framework as just another model to foxily aggregate over. \"When I think through Ajeya's model, I get X timelines, but then when I extrapolate out GWP trends I get Y timelines, so I'm going to go with (X+Y)/2.\" I think instead everyone's timelines should be derived from variations on Ajeya's model, with extensions to account for things deemed important (like data collection progress) and tweaks upwards or downwards to account for the rest of the stuff not modeled.The Unit of Caring prize poolTwelve users donated to the Unit of Caring prize pool. $1375 was donated to thank specific authors, and $395 was donated to the \"general moderator discretion\" fund. Lightcone Infrastructure matched all the donations, and the LessWrong moderation team reviewed and allocated them as follows:$700 to catherio for microCOVID.org, a Tool for Estimating Covid Risk$500 to Zvi for The Road to Mazedom, and Covid-19: My Current Model$420 to Johnswentworth for When Money is Abundant, Knowledge is the Real Wealth, and Alignment By Default$300 to Duncan Sabien for the CFAR Participant handbook$250 to Daniel Kokotajlo for Against GDP as Metric For Timelines$250 to abramdemski for Most Prisoner's Dilemmas are Stag Hunts, Most Stag Hunts are Schelling Problems$250 to Jeffrey Ladish for Nuclear War Unlikely To Cause Human Extinction$140 to Mark Xu for The First Sample Gives the Most Information$140 to Richard Korzekwa for Why Indoor Lighting is Hard To Get Right and How To Fix It$100 to evhub for An Overview of 11 Proposals for Building Safe Advanced AGIPrizes for ReviewersThe LessWrong Review isn't just about posts. It's also about the process of reflection and evaluation of those posts. We've been awarding prizes for reviewers who contributed significant commentary on posts. The totals came to:$600 to AllAmericanBreakfast$500 to johnswentworth$400 to Vanessa Kosoy$200 to Steven Byrnes$200 to Zvi$100 each to abramdemski, adamzerner, CharlieSteiner, Daniel Kokotajlo, Davidmanheim, Elizabeth, magfrump, MondSemmel, Neel Nanda, niplav, nostalgebraist, philh, Richard Korzekwa, Turntrout, Vika, Yoav Ravid, and Zack_M_DavisI separately hired Bucky to do some in-depth reviews.There were a few different ways I found reviews valuable.First, often they simply reminded me a good post existed, and gave some context for why it mattered. (johnswentworth's comments on Subspace Optima were a short and sweet version of this. Daniel Kokotajlo's comments on 11 Proposals for Safe Advanced AI were also a great example, which I expect fed into that post ranking highly in the overall vote)Second, there were many self-reviews by authors who had learned a lot. Of these, some of my favorites are:Steve Byrnes on Inner Alignment in Salt Starved Rats, where he exhaustively goes over some updates he made to his model.Niplav's self-review of Range and Forecasting Accuracy (he expressed worry about a number of mistakes in the post, but I think the review is a great time to fix mistakes on otherwise good posts, and I was generally quite impressed with his thoroughness and thought the topic was quite important).Third, critical reviews.I thought TurnTrout's comment on Nuclear War Is Unlike to Cause Human Extinction was succinct, and helped draw attention to a missing piece of the story. Landfish responded with some clarification.Bucky's commentary on Kelly Bet On Everything helped iron out where the math exactly worked.AllAmericanBreakfast dug into the paper Scott cites in Studies on Slack, which taught me some new empirical things about the world as well as feeding into a comprehensive critique.Fourth, in-depth reviews that explored an idea and thought about its applications.Johnswentworth and Vanessa both had good reviews of The Solomonoff Prior is Malign, which is a concept I've found confusing to think about.My sense is that reviewing generally feels less glamorous than writing top level posts, but I think it's quite important for grounding the overall process.\u00a0To all our reviewers, thank you!One More Thing...That's almost it for this year's review. We have at least one additional bit of work, which is assembling the top posts into sequences. The winning posts will be integrated into our site library, promoted heavily for new users to read and reflect on.Congratulations to the winning authors \u2013 thank you for contributing to LessWrong's longterm intellectual progress. \u00a0\u00a0^During the Review Phase, people were encouraged to write comments reviewing each nominated post, reflecting on why it was valuable or how it could be improved.\u00a0", "title": "Prizes for the 2020 Review", "karma": 93, "af": false, "userid": "r38pkCm7wF4M44MDQ", "id": "y2qydZosrttzgm65H"}, {"text": "I appreciate Solomonoff's success in generalizing Occam razor from just selecting the simplest hypothesis/model to adding probabilities to each of them.\u00a0But for instance the postion (motion) of any body (x, y, z) can be fit by an inequationinstead of writing actual Newton's laws (and adding very tight intervals for constants), and it is simpler, so the particular Solomonoff probablility would be greater (if I understand Solomonoff Induction correctly) even the model I stated above is apparently useless.Or is SI meant to be used just for exact models? Then it might be completely useless, as long as almost nothing in this world can be fitted exactly according to my worldview.Can you explain the above mentioned issues likely residing in my incomprehension?", "title": "Generalization of the Solomonoff Induction to Accuracy - Is it possible? Would it be useful?", "karma": 2, "af": false, "userid": "TeqMtPmo7CfsiYpBz", "id": "bG6DRZAct9jeqHLra"}, {"text": "The Triad gave Taman a week to pay off his crushing debt. Now the Guild of the Rose claims they can help, but how can Taman trust them when he doesn't even know what they want?This is a fictional story about what the Guild of the Rose could someday become.", "title": "Practitioner: Novice", "karma": 25, "af": false, "userid": "wevSQ7vhhznSAMZie", "id": "5RytYtxhr5BaELkKi"}, {"text": "Context\nI'm interested in working on alignment, coming from a programmer/Haskell background.\nI have two ideas that are unlikely to info hazards, that I want to post here in order to get feedback/info about prior work.\nBoth are very rough/early stage, view this as me presenting a pre-MVP to get some very early feedback.\nIdea 1 - \"Love is that which enables choice\" (Inspired by Forrest Landry)\nThis is an idea for a potential goal/instruction for AI (can't recall the fancy term).\nThe idea is to make an AI that optimizes for optionality: maximizing the total sum of agency for all human and non-human agents.\nAgency is here loosely defined as \"Optional ability to make changes to the world\".\nMaking it the sum total would discourage situations where the ability of one person to affect change would hamper the ability of someone else.\nIdea 2 - Segmented gradient descent training optimized for collaboration between different agents\nThis is an idea for a potential training method, that I think may have a big attractor basin for collaborative traits.\nThe idea is to have some kind of gradient descent-esque training where AI agents of varying calibres/types are put in training scenarios in which a premium is put on collaboration. This is run in multiple iterations, where AI that successfully collaborates with other agents get to continue.\nThe hardest thing about this is that we want an AI that is cooperative, but we do not want an AI that is naive, as this would lead to situations where terrorists convince the AI to do stupid shit. We could try to model this on human (cultural/biological) evolution.\nOne thing I like about this idea is that it might lead to AI that develops behavioural patterns akin to those found in herd animals (including humans).\nThis would make the AI easier to reason about, and more likely to develop something akin to ethical behaviour.\n", "title": "2 (naive?) ideas for alignment", "karma": 3, "af": false, "userid": "bJg4KmRSnngmp83Gw", "id": "7guSTHBa7jedm5gm2"}, {"text": "Several years ago I gushed and gushed about Alasdair MacIntyre\u2019s book After Virtue. I\u2019m not taking any of that gushing back today\u2014I still think it was an exciting argument and one that has continued to resonate with me.But I finally got around to reading G.E.M. Ans\u00adcombe\u2019s essay \u201cMod\u00adern Mor\u00adal Phil\u00ados\u00ado\u00adphy,\u201d which pre\u00addates Mac\u00adIntyre\u2019s book by two dec\u00adades, and I found that she had al\u00adready put for\u00adward much of the core of Mac\u00adIntyre\u2019s later ar\u00adgu\u00adment there. Mac\u00adIntyre only men\u00adtions Ans\u00adcombe\u2019s es\u00adsay in a brief paren\u00adthe\u00adti\u00adcal in his book (in which he says that parts of his own ar\u00adgu\u00adment are \u201cboth deeply in\u00addebted to and rather dif\u00adfer\u00adent from that of Ans\u00adcombe 1958\u201d). To me it feels more like Mac\u00adIntyre fur\u00adther elabo\u00adra\u00adted a pro\u00adject and a the\u00ado\u00adre\u00adti\u00adcal frame\u00adwork that Ans\u00adcombe de\u00adserves credit for estab\u00adlish\u00ading.I\u2019ll try to summarize her 26-page argument in sixteen paragraphs, in the hopes of whetting your appetite for reading the whole thing:Anscombe argues that concepts like \u201cmoral obligation,\u201d \u201cmorally right and wrong,\u201d and \u201cthe moral sense of \u2018ought\u2019\u202f\u201d have lost their meaning. Such phrases \u201cought to be jettisoned\u201d by philosophers \u201cbecause they are survivals, or derivatives from survivals, from an earlier conception of ethics which no longer generally survives\u201d and so are now just confusing. (This is not because she is a moral nihilist\u2014she\u2019s about as far from that as you can get\u2014but because she thinks philosophy has become hopelessly confused and needs to reset.)She traces the term \u201cmoral\u201d to Aristotle (if I remember right, Aristotle used \u1f20\u03b8\u03b9\u03ba\u03cc\u03c2 which got translated into moralis in Latin). But because of how the meaning of \u201cmoral\u201d has shifted over time, to read Aristotle in translation today means to \u201cconstantly feel like someone whose jaws have somehow got out of alignment: the teeth don\u2019t come together in a proper bite.\u201dWords like \u201cshould\u201d/\u201cought\u201d/\u201cneeds\u201d are used in two senses: First, to describe the requirements for things with purposes (this machinery needs oil, ought to be oiled, in order to function as it should); secondly, in the modern moral sense to indicate obligation. This second sense has a historical pedigree. Anscombe believes it resulted from the Judeo-Christian belief in divine law. Christianity borrowed terms from Greek philosophy and put a divine law spin on them, so that, for example, the Greek word for going astray or being mistaken (\u1f00\u03bc\u03b1\u03c1\u03c4\u03ac\u03bd\u03b5\u03b9\u03bd) became the word for sinning: breaking the divine law.When Christianity changed the meanings of concepts like sin and virtue in this way, moral terms like \u201cshould\u201d and \u201cought\u201d became appropriate to those concepts (that you should or ought to obey the divine law is just inherent to what divine law is). But over time moral philosophy shed divine law but tried to keep the virtues. In doing so, it found that this foundation had been pulled out from under them. If moderns continue to talk about virtues in terms of \u201cshould\u201d and \u201cought\u201d without the Christian appendages that those terms could successfully attach to, they\u2019re talking empty nonsense. Anscombe calls this \u201cthe survival of a concept outside the framework of thought that made it a really intelligible one\u201d that has had the effect of rendering \u201c[the] word \u2018ought\u2019\u2026 a word of mere mesmeric force.\u2026 a word retaining the suggestion of force, and apt to have a strong psychological effect, but which no longer signifies a real concept at all.\u201dEuropean ethical philosophers of note have tied themselves in knots trying to make sense of this modern moral ought. Butler put \u201cconscience\u201d first, \u201cbut appears ignorant that a man\u2019s conscience may tell him to do the vilest things.\u201d Hume\u2009was\u2009\u201ca mere\u2014brilliant\u2014sophist\u201d\u2009who removed ethical judgments from the category of things that can be true. Kant established the \u201cabsurd\u201d idea of \u201clegislating for oneself\u201d and took it to uselessly rigid conclusions. Bentham and Mill (whose utilitarianism was \u201cstupid\u201d) got lost in a thicket of \u201cpleasure\u201d as an end, though not because of a supposed \u201cnaturalistic fallacy\u201d about which Anscombe \u201cdo[es] not find accounts of it coherent.\u201dSidgwick and Moore are what Anscombe calls consequentialists\u2014using a term that is now standard, but that she herself coined for this essay. She means the term to describe someone who reasons more or less like a utilitarian but without necessarily specifying \u201cutility\u201d as the criterion\u2014someone who believes that we are acting ethically if and only if we are acting to optimize some quality or other of the future state of the world.Sidgwick in particular advanced a theory of responsibility that Anscombe says was particularly seductive and harmful. In brief, that theory is this: \u201cit does not make any difference to a man\u2019s responsibility for an effect of his action which he can foresee, that he does not intend\u201d that effect. There is something to this idea, which is what makes it so seductive, but left as-is and not examined further it can have terrible consequences. The terrible consequences in this case turned out to include consequentialism.If you blindly follow Sidgwick\u2019s idea to its logical conclusions, those conclusions are of the ends-justify-the-means variety: It is justified to do the most shameful awful vile thing, if an unintended but foreseeable hypothetical consequence of not doing it is sufficiently bad. Furthermore, \u201cyou can exculpate yourself from the actual consequences of the most disgraceful actions, so long as you can make out a case for having not foreseen them.\u201d(Anscombe suggests a more sensible formula wold be this: \u201c[A] man is responsible for the bad consequences of his bad actions, but gets no credit for the good ones; and contrariwise is not responsible for the bad consequences of his good actions.\u201d)By swallowing Sidgwick\u2019s terrible idea whole, says Anscombe, the consequentialists have elevated a moral temptation to the status of a moral philosophy. Furthermore, because their philosophy demands prescience about consequences, but they can\u2019t predict the future any better than the rest of us can, they cannot give us any actual practical ethical advice. Instead, they give us the terminology with which we can better articulate our temptations to do shameful things, and they give us the predisposition to consent to the casuistry of other bad actors.Is there a way to get back to moral oughts, or something like them, without divine law? For example, a plant \u201cneeds\u201d water, and so you \u201cought to\u201d see that the plant gets some if you want the plant to thrive. But that \u201cif\u201d is where the problem lies, because \u201cought to\u201d in the moral sense is supposed to somehow work without any \u201cif\u201d clause attached. However, to the plant itself there doesn\u2019t seem to be such a problem: it ought to pull in some water through its roots because otherwise it\u2019s obviously doomed, and so pull water it does. Is there some sense in which human beings \u201cneed\u201d the virtues in order to thrive in the same sort of way that plants need water, in which case it would not be controversial that the fact of such a need generates (at least a rebuttable presumption of) an ought?Even so, a modern moral philosopher could argue that we\u2019ve just moved the problem rather than resolved it. I need something to thrive, and I presumably ought therefore to pursue it if I want to thrive\u2026 there\u2019s that \u201cif\u201d again. (Some philosophers even try to attack divine-law theories on this ground, saying that even if you believe in divine law, its \u201coughts\u201d have no force to compel action unless you also add \u201cif you want to obey the divine law\u201d to the end of them.)What alternatives remain to the old-fashioned divine law model? One possibility is a sort of natural law: divine law without the divinity. Maybe such a law can be found in \u201cnatural religion\u201d, or maybe in the norms of society (though when you see what kinds of norms societies have had, or take a close look at how nature legislates, maybe that\u2019s not such a great idea). Can you come up with laws on your own\u2014legislate \u201cfor yourself\u201d in a Kantean (or existentialist?) way? Such things, Anscombe says, aren\u2019t really laws; they\u2019re at best a sort of stand-in for laws. What about contractualism\u2014the idea that moral law reflects an ancient or inevitable or mythical social contract? Anscombe is skeptical, though she says there is room for investigation in some of these paths.Ah, but what about virtue ethics in the Aristotelian vein? Anscombe considers this, and thinks it works, but only if you remember that Aristotle\u2019s ethics predates the Christian change in moral terminology. If you rewind the clock back that far, you go past the modern incoherent and mesmeric moral \u201cought,\u201d and then continue past the divine law\u2019s commanded moral \u201cought,\u201d then finally to an ethics in which the moral \u201cought\u201d no longer makes an appearance. So you aren\u2019t going to solve the problem of making the moral ought make sense that way. But that might be just fine: Anscombe thinks it\u2019s time for philosophers to put that problem on the shelf anyway.But, says Anscombe, we are still a long way from being able to create a robust virtue ethics, as \u201cphilosophically there is a huge gap, at present unfillable as far as we are concerned, which needs to be filled by an account of human nature, human action, the type of characteristic a virtue is, and above all of human \u2018flourishing.\u2019\u202f\u201dBut that daunting project seems to Anscombe more promising than the project of modern Anglophone moral philosophy circa 1958: to create ethical systems under which no act is too vile for some future hypothetical possibility to justify it.", "title": "Review: G.E.M. Anscombe\u2019s \u201cModern Moral Philosophy\u201d", "karma": 24, "af": false, "userid": "eJdEKa8cRgmPuLNvT", "id": "XC5xBQNTHrxz8XH4A"}, {"text": "TL;DR: Ukraine will eventually either get nuclear protection by NATO or its own nukes, which in both cases will likely lead to the nuclear war with Russia. Knowing this, Putin may want to attack first or credibly threaten to do so.The whole world is looking at the Ukraine-Russia conflict. Will it lead to nuclear war?My general view is that the Ukraine-Russia conflict is structurally similar to the conflict between India and Pakistan. There are many differences, but it is a good reference point. Pakistan appeared after the British empire breakdown and it was a relatively new country whose borders was not well established. Kashmir is like Crimea in this conflict.\u00a0\u00a0The bottom line of this analogy is that there will be many wars between Ukraine and Russia and that Ukraine will eventually become a nuclear power, the same way as Pakistan did, or will get similar military capabilities via drones and membership in NATO.\u00a0\u00a0Putin also knows this and also knows that his large tank armies are almost obsolete. Therefore, he tries to limit future Ukraine military capabilities by attacking (or threatening) now. He does actually say this: we see Ukraine (in NATO) as an existential threat. He may also have some other irrational motives but the rational motives go beyond Putin, so any other Russian leader will be in the same strategic situation.A nuclear war between Russia and Ukraine is rather possible from a long-term perspective, no matter how the current escalation will end. Unless the current war ends with the complete occupation of Ukraine. \u00a0Therefore, the real goal of the war may be taking (or permanently damaging) almost all Ukraine including all nuclear power stations and industrial capabilities. Personally, I do not think it will make Russia safer, as new military technologies (drones) will continue to evolve in other countries and Russia will be cut from new tech by enormous sanctions. In my view, the large war with Ukraine will end in large crimes and will be eventually lost because of international help.Therefore, Putin may see the current moment as the only chance to stop future nuclear-protected Ukraine and to prevent a nuclear war with it.Ukraine now is slowly developing medium-range missiles which can reach Moscow according to some accounts. (\u00a0Hrim-2 declared distance is 500 \u00a0km, Poroshenko said 1000 km is possible.)A rocket attack from Ukraine against Moscow, even if with a conventional warhead, will trigger Russian nuclear-powered anti-ballistic missiles. The nuclear explosions of such anti-missiles will be small (15kt?) and high-altitude, so there will be no harm, but it will be the first use of nuclear weapons in conflict for decades. A hypothetical future nuclear war of Russia with Ukraine could spill to other countries, as an attack by Ukraine on Russia could be seen as a hidden US attack.\u00a0Ukraine has a very large civil nuclear power \u2013 several stations of the size of Chernobyl (15 reactors). They could be used to produce plutonium. Ukraine has uranium ore mines. In 2019, the construction of nuclear fuel production plant was started (centrifuges?)\u00a0Ukraine non-nuclear status is based on the Budapest memorandum. 19 Feb in Munich Ukrainian president Zelensky almost threatened Ukraine\u2019s rejection of the Budapest Memorandum: \u201cI am initiating consultations in the framework of the Budapest Memorandum. The Minister of Foreign Affairs was commissioned to convene them. If they do not happen again or their results do not guarantee security for our country, Ukraine will have every right to believe that the Budapest Memorandum is not working and all the package decisions of 1994 are in doubt.\u201dThe memorandum was signed on December 5, 1994, by the United Kingdom, Russia, the USA and Ukraine and provided guarantees of the security and territorial integrity of Ukraine in exchange for the renunciation of nuclear weapons. After the annexation of Crimea, the memorandum was clearly violated and Ukraine has \u201ctechnical\u201d right to demand its nuclear status back, or exchange this right for membership in NATO.\u00a0While NATO is a defence organization, Ukraine hopes that the membership in NATO will help Ukraine to attack Donbas and Crimea, which are its territories according to international laws \u2013 without risks of Russian retaliation. This situation will quickly deteriorate to war between NATO and Russia. Actually, nobody wants neither nuclear Ukraine, nor Ukraine in NATO, so NATO continues to feed Ukraine with promises without exact substance. \u00a0Ukraine unlikely will openly start to work on nukes soon, but it can ask for equal weapons in exchange for not doing so.\u00a0Therefore, for Putin, it is like a trolley problem: bad war now or hypothetical very bad war in the future. Personally, I am against changing the track in the trolley problem, as it is killing the real people for the possible safety of hypothetical people. Possible people and risks are more likely to be products of imagination and therefore many trolley-like problems are subject to biases \u2013 or just good words to cover some bad intentions.\u00a0In December 2021 Putin put a rather broad ultimatum to NATO: either provide safety guarantees to Russia or deal with military-technical consequences.\u00a0The wars are known to be unpredictable, so there is a risk that the current confrontation will spiral out of control into a global nuclear war. I even thought about moving to a county house from Moscow but decided to stay for now.In some sense, it is a game of chicken on the trolley track.", "title": "Russian x-risks newsletter winter 21-22, war risks update. ", "karma": 29, "af": false, "userid": "op64n8Qs4yJLNhA8q", "id": "EGv7W93gtFddbKSwG"}, {"text": "Just a study I saw on /r/MachineLearning: link.Basically, one way of training neural networks is to add random noise during the training. Usually, the noise that gets added is independent between the training steps, but in the paper they make it negatively correlated between the steps, and argue that this helps with the generalization of the networks because it moves them towards flatter minima.This seems conceptually related to things that have been discussed on LessWrong, e.g. the observations by John Wentworth that search tends to lead to flat minima, which may have beneficial properties.I would have liked to see them test this on harder problems than the ones they used, and/or on a greater variety of real-world problems.", "title": "Anticorrelated Noise Injection for Improved Generalization", "karma": 7, "af": false, "userid": "mfgrYb4LMk7NWXsSB", "id": "a3TwBfywdPyGSQaSx"}, {"text": "In this post I'd like to construct an extension to the Sleeping Beauty problem which is unsolvable from an thirder/SIA perspective. The core of the construction is having the number of wakings Beauty will experience follow a distribution with an infinite mean. This leads to Beauty being unable to assign normalized probability values when reasoning under SIA. Given this forum's fascination with anthropic reasoning and this problem, this may be of interest.The original Sleeping Beauty problemThe following is a summary of the thirder/SIA approach to the original Sleeping Beauty problem. From what I can tell, this forum is very well versed here, so I won't go into too much background detail.In the original problem, we have the following two steps for the experimenter:Flip a coin. If heads, record the value\u00a0N=0. If tails, record the value\u00a0N=1.Put Beauty to sleep [on Sunday]. She will be woken once on each of the following\u00a0N+1\u00a0days, with memory erased between wakings. (i.e. once if heads, twice if tails)Let\u00a0Mk\u00a0be the proposition \"I am awake and today is the\u00a0kth day since the experiment began\". (I.e.\u00a0M1\u00a0is \"I am awake and it is Monday\";\u00a0M2\u00a0is \"I am awake and it is Tuesday\".) Also let\u00a0M\u00a0be the proposition \"I am awake\", in the sense that\u00a0M=M1\u2228M2.The problem is then to give Beauty a value to assign to the conditional probability of heads given that she has been awoken, i.e. toP(N=0\u2223M).One version of the thirder/SIA calculation for this value then proceeds as follows.P(N=0\u2223M)P(N=1\u2223M)=P(M\u2223N=0)P(M\u2223N=1)P(N=0)P(N=1)=P(M1\u2223N=0)+P(M2\u2223N=0)P(M1\u2223N=1)+P(M2\u2223N=1)P(N=0)P(N=1)=P(M1\u2223N=0)+02\u2217P(M1\u2223N=1)P(N=0)P(N=1)=12P(N=0\u2223M1)P(N=1\u2223M1)=121/21/2=12.Where the steps are respectively due to:Bayes' law.That\u00a0M=M1\u2228M2\u00a0and that\u00a0M1,\u00a0M2\u00a0are disjoint propositions.For the numerator, that\u00a0M2\u00a0is impossible given\u00a0N=0\u00a0(i.e. we cannot wake up on Tuesday if the coin came up heads). For the denominator, this is the part of the SIA assumption: given that the coin came up tails, and we cannot distinguish a waking on Monday or Tuesday, we must put equal weight on both possibilities.Bayes' law.Following SIA/thirder reasoning, we put equal weight on heads or tails given that we are waking on Monday. I leave the subtler point of justifying this assignment to the many expositions of thirder/SIA reasoning.Combining this determined ratio with the fact that\u00a0P(N=0\u2223M)+P(N=1\u2223M)=1, we must have\u00a0P(N=0\u2223M)=1/3. This of course is what gives proponents of the above calculation the name \"thirders\".A new construction: Exponentially-Sleeping Beauty problemNow suppose we extend the experiment as follows. The experimenter will:Sample an integer\u00a0N\u22650\u00a0according to the distribution\u00a0P0(N=k)=1/2k+1\u00a0for\u00a0k\u22650. Perhaps the experimenter implements this by flipping a coin until the first time it comes up tails, and then recording the number of heads.Wake up Beauty\u00a02N\u00a0times.Once again, let\u00a0M\u00a0be the proposition \"I am awake\" and\u00a0Mk\u00a0be the proposition \"I am awake and today is the\u00a0kth day/waking-time since the start of the experiment\". What value should a waking Beauty now assign to e.g. the probability\u00a0P(N=5\u2223M)?\u00a0We will see that in this new problem, assigning such a value will be problematic. Following the thirder/SIA approach to the original problem, we calculate the ratioP(N=k1\u2223M)P(N=k2\u2223M)=P(M\u2223N=k1)P(M\u2223N=k2)P(N=k1)P(N=k2)=\u239b\u239c\u239d\u2211i=2k1i=1P(Mi\u2223N=k1)\u2211i=2k2i=1P(Mi\u2223N=k2)\u239e\u239f\u23a0P(N=k1)P(N=k2)=2k12k2P(M1\u2223N=k1)P(M1\u2223N=k2)P(N=k1)P(N=k2)=2k1\u2212k2P(N=k1\u2223M1)P(N=k2\u2223M1)P(M1)P(M1)=2k1\u2212k21/2k1+11/2k2+1=1.Where the steps are resp. due to:Bayes' law/defn. of conditional probabilityWe have\u00a0M=(M1\u2228M2\u2228\u2026)\u00a0and \u00a0P(M2k+1\u2228M2k+2\u2228\u22ef\u2223N=k)=0, so that\u00a0P(M\u2223N=k)=P(M1\u2228M2\u2228\u2026M2k\u2223N=k). These are mutually exclusive/disjoint propositions, so we can transform into a sum.Following thirder/SIA reasoning, given that\u00a0N=k\u00a0for some\u00a0k, we must assign equal probability for each of the possible days that we can awake, i.e.\u00a0P(Mi\u2223N=k)=P(M1\u2223N=k)\u00a0for all\u00a01\u2264i\u22642k.(Bayes' law.)Following thirder/SIA reasoning, given only that it is \"Monday\", awoken Beauty's conditional probability on the outcome of the experimenter's coin tosses is the same as what is was before she went to sleep.Hence we have shown that following thirder/SIA reasoning, exponentially-sleeping Beauty must assign\u00a0the probability value ratio\u00a0P(N=k1\u2223M)P(N=k2\u2223M)=1\u00a0for every\u00a00\u2264k1,k2\u00a0pair, hence\u00a0P(N=k\u2223M)=P(N=0\u2223M)\u00a0for all\u00a0k\u22650.This is a problem. We have an infinite collection of mutually-exclusive probabilities\u00a0{P(N=k\u2223M)}k\u22650\u00a0which we have proven under SIA reasoning to all be of equal value. It is impossible to satisfy this while having our probabilities sum to unity as in\u00a0\u2211\u221ek=0P(N=k\u2223M)=1.Wat does dis mean; how should Beauty assign probabilities here; is this a problem for the SIA?NotesThis construction requires a universe which is infinite in either time or space, in order to have the volume to support a potentially unlimited number of Beauty awakenings. However, we should note that this assumption can be somewhat weakened: If even the expected value of Beauty's probability distribution over her number of universe-supported wakings is infinite, then the calculation can still go through to a point where we cannot assign probabilities under SIA reasoning.Alternate constructions with some interesting properties can be realized by having Beauty wake\u00a01.5N,\u00a02NN\u00a0or\u00a03N\u00a0times.", "title": "Breaking the SIA with an exponentially-Sleeping Beauty", "karma": 6, "af": false, "userid": "F7pojSiFF3Jhddu9k", "id": "D9FDfyh4hh7y5gLnq"}, {"text": "Here is the list of recommend books for someone interested in enhancing their understanding of the world.This is a post from Substack which can be found here.\u00a0\u00a0Not long ago, I would read roughly one book every couple of years. I had a busier life. I didn\u2019t value spending my attention on books. It wasn\u2019t until 3 years ago that my life settled down. With new found time, my mind\u2019s hunger for novel information increased and I stumbled upon our local library. Fortunately for me, they provide access to two audio book apps (Hoopla and Libby). I also picked up more physical books. My consumption is roughly 75% audio and 25% physical. With the ability to increase playback speed in the apps and my commute, I am able to get through 1-2 books per week. I am not a fast reader, but do purchase physical books worthy of reading or referencing again.As I learn, I find myself wanting to share new concepts with others. These ideas often update my mental models and have increased the resolution of my \u201cmap of the territory\u201d. Throughout my journey, intermixed with the phenomenal books below have been less impressive books. The goal of this list is to provide a guide for those searching. I will try and update this periodically. If you have recommendations for additions, please comment below.I put the topics and rankings in order of importance, but read what interest you most. The intent of this post is to provide a rubric for the self-guided learner who wishes to enrich their life and better try and understand the world around them. This is for my daughter. This is for AGI. This is for anyone who lights up while learning new things.HappinessThe Happiness Hypothesis by Jonathan HaidtThe Book of Joy by Dalai LamaOn Living a Good LifeFour Thousand Weeks by Oliver Burkeman *Man\u2019s Search for Meaning by Viktor FranklThe Fire Next Time by James BaldwinWaking Up by Sam HarrisTao Te Ching by Lao Tzu *How to Win Friends and Influence People by Dale CarnegieThe Power of Now by Eckhart Tolle (if you can get your hands on his Findhorn Retreat, listen to that)A Hunter-Gather\u2019s Guide to the 21st Century by Heather Heying and Bret WeinsteinWalden by Henry ThoreauMeditations by Marcus Arelius *How to Talk to Strangers by Malcolm Gladwell ~Tribe by Sebastian JungerOn ThinkingLost in Thought by Zena Hitz *The Righteous Mind by Jonathan HaidtAntifragile by Taleb NassimThinking, Fast and Slow by Daniel KahnemanMap and Territory by Eliezer YudkowskyThe Master and his Emissary by Iain McGilchristThe Origin of Consciousness in the Breakdown of the Bicameral Mind by Julian JaynesLife HackingDeep work by Cal NewportThe Power of Habit by Charles DuhiggDigital Minimalism by Cal NewportThe 4-Hour Workweek by Tim Ferris *Designing Your Life by Bill BurnettLeadership/Entrepreneurial/MotivationZero to One by Peter ThielOriginals by Adam GrantGreenlights by Matthew McConaughey ~The Icarus Deception by Seth GodinGrit by Angela DuckworthRange by David EpsteinOutliers by Malcolm GladwellFinding Ultra by Rich RollSmarter Faster Better by Charles DuhiggScience/HistorySapiens by Yuval Noah HarariHomo Deus by Yuval Noah HarariThe Order of Time by Carlo Rovelli *How to avoid a Climate Disaster by Bill GatesThe Hidden Life of Trees by Peter WohllenbenDopamine Nation by Anna LembkeFirst Principles by Thomas RicksA Brief History of Time by Stephen HawkingWhen Death Becomes Life by Joshua MezrichEducation/Raising childrenThe Coddling of the American Mind by Jonathan HaidtOn CreativityA Swim in a Pond in the Rain by George Saunders *On Writing by Stephen KingBig Magic by Elizabeth GilbertFinanceThe Simple Path to Wealth by JL CollinsDollars and Sense by Dan ArielyThe Man who Solved the Market by Gregory ZuckermanDietEating Animals by Jonathan FoerThe Case Against Sugar byGary TaubesIn Defense of Food by Michael PollanFictionBewilderment by Richard PowersThe Overstory by Richard PowersThe Fountainhead by Ayn RandThe Road by Cormac McCarthyNo Country for Old Men by Cormac McCarthyThe Fall by Albert CamusThe Stranger by Albert CamusThe Alchemist by Paulo Coelho1984 by George OrwellCato, A Tragedy by Joseph AddisonFantasy/Sci-FiThe Stormlight Archive (Series) by Brandon SandersonThe Three-Body Problem by Liu CixinGame of Thrones (Series) by George RR MartinDune by Frank HerbertCloud Atlas by David MitchellCryptonomicon by Neal StephensonThe Hitchhiker\u2019s Guide to the Galaxy by Douglas AdamsSeveneves by Neal StephensonAnathem by Neal Stephenson\u00a0* Indicates books better suited for text~Indicates books better suited for audioLast updated Feb 19th 2022\u00a0", "title": "Books Worthy of Integration", "karma": 4, "af": false, "userid": "MEcmvHy6SHdjxFEsg", "id": "XTnzZkAHenomutQhq"}, {"text": "IntroductionHere's a question I've been pondering recently: how many organic compounds are there? There are multiple ways to interpret the question, each of which leads to consideration of a different potential set of compounds, and each of which is informative in its own way.First off, what is an organic compound? \u00a0This sounds straightforward, but because of the history of the term, it isn't. \u00a0Originally, \"organic compounds\" were carbon-containing compounds associated with living things. \u00a0Over time, the term drifted to include most carbon-containing compounds, except for things like carbon dioxide, minerals like limestone (CaCO3), and allotropes of pure carbon (like diamonds, graphite, or carbon nanotubes). \u00a0Wikipedia defines an organic compound as \"generally any chemical compounds that contain carbon-hydrogen bonds\". \u00a0That definition seems good enough \u00a0for our purposes[1], so we'll go with it.Next, what do we mean by \"are there\"? \u00a0Is it substances that have actually been found or made and characterized? \u00a0A quick check of PubChem says that there are around 100 million (108) compounds with information submitted. \u00a0However, that's not quite what I was envisioning when I asked the question. \u00a0The compounds we've actually made or isolated (and more importantly, characterized to a greater or lesser extent) are only a tiny fraction of chemical space, which I will loosely define as the set of all possible[2]\u00a0molecules. \u00a0Example Chemical Sub-spaces: Hydrocarbons and ProteinsOrganic chemical space contains, in theory, an infinite number of molecules. \u00a0Consider the set of fully-saturated, unbranched hydrocarbon chains: methane with a single carbon, ethane with two, etc. \u00a0In principle, one could construct an arbitrarily long chain with no reason to expect chemical instability. \u00a0In practice, we make polyethylene chains up to about half a million carbon atoms in length that are reasonable approximations of that thought exercise.Of course, ultra-long hydrocarbons aren't that interesting chemically, with only one monomer and no functional groups. \u00a0What about molecules that actually do things? \u00a0Let's take proteins, a strong contender for \"most interesting class of molecules\". \u00a0There are 20 naturally-occurring amino acids. \u00a0A 30-kDa protein[3]\u00a0has about 300 amino acids, so there are 20^300 (about 10^390) possible combinations of amino acids leading to a protein of moderate size. \u00a0As a comparison, a quick Fermi calculation gives 10^80 atoms in the known universe.[4]\"Smallish\" Organic MoleculesOkay, but that still isn't quite what I meant to ask when I wondered \"how many organic molecules are there?\" \u00a0I'm a synthetic organic chemist, so what I really wondered was \"how big is the space of the kinds of molecules that synthetic organic chemists are typically concerned with?\" \u00a0These molecules tend to be:Relatively small compared to the examples we've considered so far (mostly less than 500 Da molecular weight)Stable on a timescale of at least hours under conditions achievable in the labComposed of relatively dense arrangements of rings, chains, and functional groupsThese constraints are fairly similar to the space of \"potentially pharmacologically active molecules\". \u00a0Wikipedia led me to a paper by Bohacek, McMartin, and Guida[5]\u00a0that gives an estimate of 10^63 such molecules. \u00a0This sounds like an answer to the question I actually wanted to ask, so it's worth unpacking their calculation further.Here's the calculation as Bohacek et al give it[6]:Although the number of possible molecules is difficult to estimate accurately, simple considerations show that it must be very large! Consider growing a linear molecule an atom at a time and choosing a carbon, nitrogen, oxygen, or sulfur atom at each position.\u00a0 Some of these atoms can be doubly or triply bonded, but not all combinations of atoms are chemically stable, and some multiple bonds will only be possible in nonlinear structures, i.e., a C=O group. Assuming a very approximate average choice multiplicity of 6, then 6^30 or 2*10^23 molecules could be grown containing 30 atoms. Now consider the ways of introducing branching or cyclization into the resulting structure. Closure of rings with three or more atoms involves selecting two atoms to form a bond and could be achieved in 30*28/2 ways.\u00a0 Making a branched molecule could be achieved by choosing a point to cut the chain and a point in the first part of the chain to attach to the cut end of the second part of the chain (i.e., 30^2 ways). Not all atoms can be joined in this way. However, this will be offset by the fact that when stereochemical considerations are introduced, the number of possibilities will be expanded. Based upon these considerations, approximately 10^40 molecules with up to four rings and 10 branch points could be produced from each linear chain, resulting in a very approximate estimate of 10^63 molecules in total. Although this is a rough estimate, it seems likely that when all the different possible combinations of ring closure and branching are taken into account, the true number will be well in excess of 10^60 and will rise steeply with increasing molecular weight.\u00a0We can quibble with some of the choices made in this calculation, but if anything, the final figure of 10^60 is a lower bound. \u00a0There are other elements besides C, N, O, and S that show up in natural products or other potentially bioactive compounds (B, F, P, Cl, Br, I just for a start) and while the halogens would only substitute for hydrogen, rather than forming rings and chains, each added element contributes multiple orders of magnitude to the total.It's also fun to compare this number with some estimates about how much carbon is available in various places. \u00a0Carbon amounts in this section come from this paper. \u00a0First, there are about 600 Pg carbon in the amosphere, mostly as carbon dioxide. \u00a0That works out to about 3 x 10^40 carbon atoms in Earth's atmosphere.[7]\u00a0 Between the atmosphere, the oceans, and the terrestrial biosphere, there are around 43,000 Pg \"circulating carbon\", or ~2 x 10^42 carbon atoms.[8]\u00a0 If we add in the estimates of \"deep carbon\" in the earth's interior, there are around 1.85 x 10^9 Pg C on the planet, or about 10^47 carbon atoms.[9]ConclusionThere are a few points I'd like to emphasize at the end of this chain of Fermi calculations. \u00a0First, all the estimates here are incredibly rough, and could be off by multiple orders of magnitude without changing the primary conclusions. \u00a0Second, the set of organic compounds considered as a search space is really big, and if you have a goal that involves picking a compound with a defined set of characteristics, you'll want to do something other than brute-force search. \u00a0Finally, in the estimate for the size of small-molecule chemical space, the bulk of the work (about two-thirds of our log-units) is done by the ability of carbon to form rings and branched chains. \u00a0In this respect, no other element comes close to the variety we see with carbon-based compounds.[10]\u00a0 To the extent that molecular shape is relevant to processes we care about (like living systems we know about, and likely those we haven't encountered yet) we should expect carbon-based compounds to play a significant role.^it does exclude some fun edge cases like carbon tetrachloride and hexafluorobenzene, but any definition we choose will have to draw the line somewhere.^\"Possible\" here is intended to mean only chemical entities that are isolable and stable for at least long enough to be characterized.^That is, a protein with a molecular weight of 30,000 grams per mole. \u00a0Mid-sized, as proteins go.^Wikipedia gives a figure of 1.5x10^53 kg total mass of ordinary matter. \u00a0Most of this is hydrogen atoms, which come 6x10^26 per kilogram. \u00a0That gives 9*10^79 or ~10^80 total atoms. \u00a0The existence of non-hydrogen atoms doesn't change this significantly.^Apologies for the paywalled article. \u00a0I'll quote the portion that interests us in full but wanted to include a link to the paper.^This is far from the main concern of the paper. \u00a0In fact, the calculation is a footnote to a figure late in the paper.^600 Pg carbon = 6 x 10^14 kg C1 kg C = 83.333 mol C = 5 x 10^25 C atoms600 Pg C = 3 x 10^40 C atoms^43,000 Pg carbon = 4.3 x 10^16 kg C1 kg C = 83.333 mol C = 5 x 10^25 C atoms43,000 Pg C = 2 x 10^42 C atoms^1.85 x 10^21 kg C on earth1 kg C = 83.333 mol C = 5 x 10^25 atoms9.25 x 10^46 ~ 10^47 C atoms on earth^Silicon can form some rings and chains analogous to carbon's, but they tend to be less stable, in large part due to the reduced strength of Si-Si bonds relative to C-C bonds. \u00a0Sulfur can form chains and rings of various sizes but doesn't branch well and also suffers from relatively low S-S bond strength. \u00a0", "title": "How Many (Smallish) Organic Compounds Are There?", "karma": 22, "af": false, "userid": "ckQgDzQsX3CZLnzHw", "id": "yvAprQnzGECvmni3L"}, {"text": "\n\nI want to start by saying how much I appreciate what the volunteers\nbehind \n\nMicrocovid have done.\nProviding a risk calculator has been really helpful to a lot of people.\nThere's also been a lot of value in running a centralized estimate of\nthe impact of different mitigations.\n\n\n\n\n\nOn the other hand, at this point, I think it has become substantially\nless useful, and in many cases is leading people to seriously\noverestimate the level of precautions that are appropriate for their\nparticular situation:\n\n\n\n\n\nIts data on prevalence is stale, because the automatic updater\nlast ran successfully on 2022-02-07.\n\n\nTo determine how risky things are in your area it uses the\naverage number of cases from last week. When cases are rising, it\nextrapolates to predict growth. On the other hand, when cases are\nfalling (as they are now here, and have been for a while) it does not\nextrapolate and assumes that cases have stayed flat.\n\n\nIt's models the people you interact with as being equally\nlikely to currently be infectious, regardless of their vaccination\nstatus.  Vaccinated people, however, are less likely to get covid and\nare infectious for a shorter time period. [EDIT: possibly it is modeling the \"less likely to get covid\" piece but not \"less likely to transmit if infected\" and \"infectious for less time\" pieces?]\n\n\n\nIt defaults to a risk budget of 200 microcovids per week, a 1%\nannual chance of getting covid, regardless of what you tell it for\nyour vaccination status.  The highest budget it offers is only 10x\nthat, for people who \"can't avoid risk, but still want to make smart\nchoices\".  This was a reasonable way of looking at things earlier in\nthe pandemic, but at this stage I think much higher budgets are\ngenerally appropriate for fully vaccinated people: the risk to the\nindividual is generally extremely low and the risk they impose on\nothers is relatively low.  Altruistically, since most of society is\noperating on a much higher risk budget keeping to low one is a lot of sacrifice\nfor minimal benefit.\n\n\n\n\nFor a vaccinated and boosted person I think a better approach is to\nuse no budget most of the time, and then use a budget of perhaps\n10,000 microcovids/week (~50% annual risk) when demand on the medical\nsystem is likely to be especially high.\n\n\n\n\n\n\n\n\n\nEven just the first two have a very large impact. For example, it\ncurrently gives a random person in my\ncounty as having a 0.78% chance of having covid (780 cases per\n100k).  Let's walk through the basic\nmethod with current data.\n\n\n\n\n\nFor the most recent seven days we've averaged 379 cases/day.\nPopulation is 1.6M, so that's 24/100k.  The positive test rate is\n2.2%. They have some math I don't entirely follow the reasoning for,\nwith:\n\n\n\n\nprevalence_ratio = 1250 / (day_i + 25) * positive_test_rate ** 0.5 + 2\ntrue_infections =  prevalence_ratio * reported_infections\n\n\nWhere \n\nday_i is number of days since 2020-02-12 (738 as of\ntoday).  They cite \n\nEstimating\nTrue Infections Revisited: A Simple Nowcasting Model to Estimate\nPrevalent Cases in the US, but I'm just going to use it as is:\n\n\n\nprevalence_ratio = 1250 / (738 + 25) * 2.2% ** 0.5 + 2\nprevalence_ratio = 1.6 * 0.15 + 2\nprevalence_ratio = 0.24 + 2\nprevalence_ratio = 2.24\ntrue_infections = 2.24 * 24/100k\ntrue_infections = 54/100k\n\n\n\n\nUsing current data moves the risk down 14x, from 780/100k to 54/100k.\n\n\n\n\nNow let's apply the extrapolation that they do when cases are\nrising. The seven days before that averaged 460 cases/day, or\n29/100k.  Test positivity was 4%, so prevalence_ratio is 2.33, and\nestimated true_infections is 68/100k.  Dividing 54 by 68\nwe get 79%, so the extrapolated current number is 79% * 54/100k.\n\n\n\n\nConsistent downward extrapolation brings it down another 20%, from\n54/100k to 43/100k, for a total of 18x.  This extrapolation is\nwarranted here, if you look at the (more current) wastewater\nnumbers.\n\n\n\n\n\n\n\n\nComment via: facebook", "title": "Microcovid Becoming Less Useful", "karma": 59, "af": false, "userid": "TtEoCrFeowCGb6rFK", "id": "Z2NXui6jB4GETTzbd"}, {"text": "Suppose you want to take a guess at the number of candyfloss sold over a month in some area and you would like a 90% confidence interval(CI) intead of a point estimate. You fermi estimates for two central subcomponents are:(1) a 90% CI of the number of candyfloss a single candyfloss seller sells per month, say 10k-40k.(2) a 90% CI of the number of candyfloss that a professional candyfloss seller sells during a month, say 50-8000.Can you estimate a 90% CI of candyfloss sold over a month based on that information? If not, could you if you made some assumptions about the distribution of (1) or (2)(e.g. could you do it if they were uniformly distributed)? Could you use the percentiles of the root or squared fo the extremes(e.g. combining either the 0.25th(5%^2) or the 23th(square root of 5%) percentiles)?My intuition is that you can't just multiply the extremes of (1) and (2) but I'm not confident in what you need to make event approximately correct claims.Edited to fix an error in (1)", "title": "How to estimate confidence intervals for fermi estimate?", "karma": 3, "af": false, "userid": "LEu8s4HkShJXHKxTT", "id": "dnEzJjEvdLzQBCdoy"}, {"text": "I've been thinking about out whether I can discover laws of agency and wield them to prevent AI ruin (perhaps by building an AGI myself in a different paradigm than machine learning).So far I\u2019ve looked into the history of the discovery of physical laws (gravity in particular) and mathematical laws (probability theory in particular). Here are 12 things I\u2019ve learned or been surprised by.1.Data-gathering was a crucial step in discovering both gravity and probability theory. One rich dude had a whole island and set it up to have lenses on lots of parts of it, and for like a year he\u2019d go around each day and note down the positions of the stars. Then this data was worked on by others who turned it into equations of motion.2.Relatedly, looking at the celestial bodies was a big deal. It was almost the whole game in gravity, but also a little helpful for probability theory (specifically the normal distribution was developed in part by noting that systematic errors in celestial measuring equipment followed a simple distribution).\u00a0It hadn\u2019t struck me before, but putting a ton of geometry problems on the ceiling for the entire civilization led a lot of people to try to answer questions about it. (It makes Eliezer\u2019s choice in That Alien Message apt.) I\u2019m tempted in a munchkin way to find other ways to do this, like to write a math problem on the surface of the moon, or petition Google to put a prediction market on its home page, or something more elegant than those two.3.Probability theory was substantially developed around real-world problems! I thought math was all magical and ivory tower, but it was much more grounded than I expected.\u00a0After a few small things like accounting and insurance and doing permutations of the alphabet, games of chance (gambling) was what really kicked it off, with Fermat and Pascal trying to figure out the expected value of games (they didn\u2019t phrase it like that, they put it more like \u201cif the game has to stop before it\u2019s concluded, how should the winnings be split between the players?\u201c).\u00a0Other people who consulted with gamblers also would write down data about things like how often different winning hands would come up in different games, and discovered simple distributions, then tried to put equations to them. Later it was developed further by people trying to reason about gases and temperatures, and then again in understanding clinical trials or large repeated biological experiments.\u00a0Often people discovered more in this combination of \u201clooking directly at nature\u201d and \u201cbeing the sort of person who was interested in developing a formal calculus to model what was going on\u201d.4.Thought experiments about the world were a big deal too! Thomas Bayes did most of his math this way. He had a thought experiment that went something like this: his assistant would throw a ball on a table that Thomas wasn\u2019t looking at. Then his assistant would throw more balls on the table, each time saying whether it ended up to the right or the left of the original ball. He had this sense that each time he was told the next left-or-right, he should be able to give a new probability that the ball was in any particular given region. He used this thought experiment a lot when coming up with Bayes\u2019 theorem.5.Lots of people involved were full-time inventors, rich people who did serious study into a lot of different areas, including mathematics. This is a weird class to me. (I don\u2019t know people like this today. And most scientific things are very institutionalized, or failing that, embedded within business.)Here\u2019s a quote I enjoyed from one of Pascal\u2019s letters to Fermat when they founded the theory of probability. (For context: de Mere was the gambler who asked Pascal for help with a confusion he had.)\u201cI have not time to send you the demonstration of a difficulty which greatly astonished M. de Mere, for he has a very good mind, but he is not a geometer (this is, as you know, a great defect)...\u201d \u2013 Blaise Pascal6.In Laplace\u2019s seminal work putting probability theory on a formal footing, he has a historical section at the end praising all the people who did work, how great they were and how beautiful their work was. Then he has one line on Bayes where he calls his work \u201ca little perplexing\u201d.\u201cBayes, in the Transactions Philosophiques of the year 1763, sought directly the probability that the possibilities indicated by past experiences are comprised within given limits; and he has arrived at this in a refined and very ingenious manner, although a little perplexing.\u201dAlso, whenever you feel like you\u2019ve missed out on your glorious youth, note that Thomas Bayes got interested in probability theory in his 50s, and died aged 59. He was not formally trained in math in his youth.7.I watched a talk by Pearl about his causal models, and I was struck by the extent to which he had a \u201cphilosophy\u201d of counterfactual inference. It had seemed pretty possible to me he would have said \u201chere was a problem, and here is my solution\u201d, but instead he had a lot to say about counterfactuals and how he thought about them conceptually that wasn\u2019t in the math.It reminds me of my impression that Daniel Kahneman (and Amos Tversky) have strong models of how their minds work, of which the heuristics & biases literature is a legibilized component of, but certainly does not capture the whole thing.Relatedly, in a lecture by Feynman on seeking new laws, he says that some people say \u201cdon\u2019t talk about what you cannot measure\u201d. He says he agrees insofar as your theories need measurable predictions, but he doesn\u2019t agree that people should stop discussing their whole philosophies, as the philosophies seem to help some people come up with good guesses about laws.I think in the past I could have found myself unable to justify my interest in the philosophy of something as more than a personal interest. Now I have a practical justification, which is that it helps me come up with guesses about how nature works! And my current guess is that many people who were successful at that had unique and well-developed philosophies.8.Pearl himself says that he has discovered two laws, and once you have them, you can fire him, because the rest is just algebra! And he calls it a calculus of counterfactuals, just like Newton and Bayes and everyone did. Fascinating.I couldn\u2019t find anything on what problems Pearl was thinking about when he came up with his calculus of counterfactuals. Like, was he personally trying to analyze clinical trials? Was he a mathematician who was friends with people doing large experiments and thought the math was interesting? I want to know what part of the world he was in contact with when developing it.9.I updated against expecting to resolve scientific disagreements at the time when the correct theory is known. Let me explain.In the discovery of gravity, there were a lot of anomalies that didn\u2019t fit the data. For instance, Jupiter didn\u2019t follow the law: its orbit was a more elongated ellipse when it was further away. Uranus\u2019s orbit would jiggle a bit sometimes. Also there were two stars who didn\u2019t orbit their collective center of gravity, but instead some other point within the ellipse. At this point I would have been like \u201cyeah, nice try, but your theory isn\u2019t fitting the details\u201d.Want to know what they said at the time? (Spoilers ahead.) For the stars, they said that we were probably just looking at them at a funny angle and that\u2019s why it didn\u2019t work. For Uranus, they said there was an invisible planet that was knocking it off-course. And for Jupiter, they said the light was moving too slowly for the measurements to work out.To me this seems like an awful lot of complexity cost weighing on a theory. Now it\u2019s no longer just a theory, it\u2019s also a lot of explaining exceptions with unlikely stories. The star-angle one doesn\u2019t even seem testable, it gives me a Scott-Alexander-like sense of this explanation gives me so many degrees of freedom that I can probably explain away loads more anomalies with it.Anyway\u2026 they were all right.\u00a0From Uranus\u2019s wobbles, they found Neptune. The stars were indeed rotated at an angle. And they did some experiments and found out that light did have a speed and this explained the Jupiter issue, and opened up a whole new area of inquiry about light.Very impressive in retrospect, but I feel like I couldn\u2019t have gotten this right at the time.My update is further in the direction that Jacob\u2019s post The Copernican Revolution from the Inside argues for, which is that if two different people had different theories at the time, I do not anticipate the disagreement being able to be \u201cclearly resolvable\u201d at all, and do expect for it to involve a great number of judgment calls, in large part dependent on one\u2019s \u201cphilosophy\u201d of how to make those calls in this domain.10.Feynman has a wonderful quote on the art of guessing nature's laws that includes at least two paths not discussed above. That said I don\u2019t understand them, in particular the ways that quantum mechanics was discovered. (I\u2019m tempted to dig into that some.)\u00a0I\u2019ve put the full quote in this footnote[1], recommended.11.One confusion I wrote down in advance was \u201cI still don\u2019t quite know how to predict that there will not be a simple mathematical apparatus that explains something. Why the motion of the planets, why the game of chance, why not the color of houses in England or the number of hairs on a man\u2019s head?\"Looking back on this, I don\u2019t know whether I got a direct answer, but I now feel that my answer is something like \u201clook for the places where Nature will show herself directly\u201d. Obviously that\u2019s not a very well-specified answer, but I feel like it points to a real distinction.12.I also made an advance prediction: \u201cI guess I also make the advance prediction that most of the rest of the [probability] math was developed by people who liked symbol manipulation more than people doing real-world problem solving. But I would be interested to be surprised here.\u201dThis prediction was false! It took both! All the probability math was developed by people who liked using math to reason rigorously about the world, and who were interested in understanding the real world! There were exceptions like Bayes who relied a great deal on thought-experiment, though sort of still \u201cabout\u201d the world, not just about symbols.\u00a0When I thought of math previously I thought about my math friends in academia, who just sort of entered the abstract world as a starting point and lived in there. (\u201cMy professor does work in flat-spherical-manifold-density-vector-spaces, so I\u2019m trying to prove something there too!\u201c) Now I think of people trying to reason about particular parts of the world I live in, and who are trying to make an externalized symbolic calculus that can do that reasoning for them.Next StepThe natural next step of my investigation is to learn more about how key discoveries in areas like optimization and information theory and game theory were made. How did nature show herself to these discoverers? I have written down a few advance predictions for if I continue seeking this information...^Feynman, on the art of guessing nature\u2019s laws, in his final lecture for BBC's Messenger Lectures:\u201cOr look at history, you first start out with Newton: he [was] in a situation where he had incomplete knowledge, and he was able to get the laws by putting together ideas which all were relatively close to experiment\u2014there wasn\u2019t a great distance between the observations and the test.\u201d\u201cNow, the next guy who did something\u2014another man who did something great\u2014was Maxwell, who obtained the laws of electricity and magnetism. But what he did was this, he put together all the laws of electricity due to Faraday and other people that came before him, and he looked at them and he realized that they were mutually inconsistent; they were mathematically inconsistent. In order to straighten it out he had to add one term to an equation.\u201d\u201cBy the way, he did this by inventing a model for himself of idler wheels, and gears, and so on, in space. Then he found what the new law was, and nobody paid much attention, because they didn\u2019t believe in the idler wheels. We don\u2019t believe in the idler wheels today, but the equations that he obtained were correct. So the logic may be wrong, but the answer is all right.\u201d\u201cIn the case of relativity, the discovery of relativity was completely different: there was an accumulation of paradoxes; the known laws gave inconsistent results, and it was a new kind of thinking, a thinking in terms of discussing the possible symmetries of laws. It was especially difficult because it was for the first time realized how long something like Newton\u2019s laws could be right\u2014and still ultimately be wrong\u2014and, second, that ordinary ideas of time and space that seem so instinctive could be wrong.\u201d\u201cQuantum mechanics was discovered in two independent ways, which is a lesson. There, again, and even more so, an enormous number of paradoxes were discovered experimentally, things that absolutely couldn\u2019t be explained in any way by what was known\u2014not that the knowledge was incomplete, but the knowledge was too complete!: your prediction was, this should happen; it didn\u2019t.The two different routes were: one, by Schrodinger, who guessed the equations; another, by Heisenberg, who argued that you must analyze what\u2019s measurable. So two different philosophical methods reduced to the same discovery in the end.\u201d\u201cMore recently, the discovery of the laws of this [weak decay] interaction, which are still only partly known, add quite a somewhat different situation: this time it was a case of incomplete knowledge, and only the equation was guessed. The special difficulty this time was that the experiments were all wrong\u2014all the experiments were wrong.\u201d\u201cNow, how can you guess the right answer when, when you calculate the results it disagrees with the experiment, and you have the courage to say the experiments must be wrong. I\u2019ll explain where the courage comes from in a minute.\u201d\u201cNow, I\u2019m sure that history does not repeat itself in physics, as you see from this list, and the reason is this: any scheme\u2014like, \"Think of symmetry laws,\" or \"Put the equations in mathematical form,\" or any of these schemes \"Guess equations,\" and so on\u2014are known to everybody now, and they\u2019re tried all the time. So if the place where you get stuck is not that\u2014and you try that right away: we try looking for symmetries; we try all the things that have been tried before, but we\u2019re stuck-so it must be another way next time.Each time that we get in this log jam of too many problems, it\u2019s because the methods that we\u2019re using are just like the ones we used before. We try all that right away, but the new discovery is going to be made in a completely different way\u2014so history doesn\u2019t help us very much.\u201d", "title": "12 interesting things I learned studying the discovery of nature's laws", "karma": 184, "af": false, "userid": "EQNTWXLKMeWMp2FQS", "id": "uiyWHaTrz3ML7JqDX"}, {"text": "Epistemic status:\u00a0exploratoryStudent: Rah, it\u2019s so frustrating!Master: What is?Student: I keep stumbling back into Foucault, and I\u2019m never able to decide if I should discard him or if there\u2019s something in there.Master: I see. Why do you think that he might be irrelevant?Student: A bunch of reasons: people keep telling me that he\u2019s either wrong or outright dangerous, he apparently made a specialty of historical mistakes and biased evidence, and post-modernism seems to always end in denying reality.Master:\u00a0Pretty damning. And yet you still think that he could teach you something?Student: That\u2019s the frustrating part! I have this intuition that Foucault built \u2014 or tried to build \u2014 some great epistemic tools, and that these could be an important part of the toolkit I\u2019m building.Master: Just try reading him then, and see where that leads you.Student:\u00a0I did.Master:\u00a0Oh. And it didn\u2019t go well, given your look.Student: I\u00a0might have thrown the book in frustration.Master: I see.Student:\u00a0And stomped on it. And insulted its ancestry. And burned it.Master:\u00a0Not reading him directly, then. Or at least not without some preparation. And maybe some anger counseling.Student:\u00a0That\u2019s why I looked for a secondary source after that.Master: Good call.Student: And I found this book: \u201cHow to read Foucault\u201d, by Johanna Oksala.Master: How should you read him, then?Student: Well, she does make a case for Foucault as breaking certainties. For example, she writesWhile science and much of philosophy aim to decipher from among the confusion of events and experiences that which is necessary and can be articulated as universal law, Foucault's thought moves in exactly the opposite direction. He attempted to find among the necessities that which upon closer philosophical scrutiny turned out to be contingent, fleeting and arbitrary. For Foucault, the aim of philosophy is to question the ways in which we think, live and relate to other people and to ourselves in order to show how that-which-is could be otherwise.Master:\u00a0But you already knew Foucault\u2019s aim was critical, didn\u2019t you? That\u2019s one of the issue with hardcore postmodernism: overcriticizing to the point of intellectual suicide.Student: Yes, but I wasn\u2019t sure that this was a fair reading. The fact that a Foucault scholar and overall defender says it has more weight than from critics.Master:\u00a0Fair enough. That still doesn\u2019t tell you much about his tools though.Student:\u00a0That\u2019s the other thing I learned about: that Foucault\u2019s main tool was history. He used to reveal where the concepts and necessities came from, and in doing so take away from them their sacred nature of \u201cnatural\u201d background assumptions.Master: Hum, so history for finding out where our concepts and values come from, in order to question them?Student:\u00a0Exactly! And the historical path plays a big role in the criticism, especially if they\u2019re here for the wrong reasons.Master: Interesting. Reminds me somewhat of\u00a0Inadequate Equilibria.Student: That\u2019s what I thought! Except the focus is more concrete \u2014 when you can beat the consensus \u2014 and Yudkowsky depends far more on game theory and evolutionary psychology than history.Master:\u00a0And I guess there\u2019s also similarities with works like\u00a0signaling theory and\u00a0The Categories were made from Man, not Man for the Categories?Student: That\u2019s my guess.Master: Yet all these approaches work well; why do we need Foucault?Student: I don\u2019t know! I mean, okay, maybe it\u2019s that Foucault is more\u2026 epistemic?Master: In what sense?Student: Something like \u201cFoucault wants to understand the underlying epistemic changes and breaks\u201d.Master: Wait a minute: that\u2019s something completely different.Student: Oh. You\u2019re right. So there\u2019s two things:The historical contingency critiqueThe paradigm and epistemic changesMaster: Now, is Foucault\u2019s work the content you\u2019re looking for, or merely a pointer.Student:\u00a0What\u2026 does that mean?Master: Do you think that you think that the value of Foucault for you comes from the specific ideas he had, or in using him to even consider these two topics? For example, your historical contingency category is related to signaling and inadequate equilibria, but also cultural history. And the epistemic changes that Foucault discusses are part of a more general frame around paradigm, maybe a pointer to Bachelard for example.Said differently: now that you have the categories, is Foucault a central part of studying these categories, or is he at best a minor source in them?Student: Ah! I do feel less drawn to Foucault in particular now that I have these categories; he definitely doesn\u2019t seem the most important source in either, even if he\u2019s relevant. But\u2026Master: But what? Don\u2019t let your confusion hide \u2014 draw it into the light.Student: There\u2019s still something about Foucault himself, his intellectual work, that seems important for epistemic purposes. What is it? It\u2019s not about his tools anymore, or the content of his work. It\u2019s\u2026 that he made so many mistakes?Master: So you want to learn to not make the same mistakes?Student: No, no, they don\u2019t seem that interesting, as mistakes go. But that\u2019s close. He made mistakes, but\u2026 he seems to have found some relevant ideas while making mistakes? Like, it sounds like he\u2019s often factually wrong, but in ways that are not detrimental to his very large scale points?Yeah, he\u2019s one of a few thinkers I can think like thatMaster: Interesting. Who else?Student:\u00a0Gould\u2019s the only one that comes to mind.Master: That\u2019s definitely a can of worms. But I see what you mean: it does seem that there was some productive direction despite the mistakes \u2014 and being a pain in the ass, in Gould\u2019s case.Student: Exactly!Master: Still, that\u2019s not something you can do now, can you?Student: Why?Master: Because for that to work, you need to find what Foucault and Gould got right. Which means digging into cultural history and paradigms for Foucault, and levels of selection for Gould.\u00a0Student: Did I just put more work on myself?Master: Yes! I\u2019m proud of you.Student: I should get started then.Master: Sure, when you finish your study on Dennett\u2019s intuition pumps. You remembered to work on that, right?Student: Well\u2026 I was kind of bothered by this Foucault thing\u2026Master: Good that we solved it then! Now, let\u2019s talk intuition pumps!Student: Yes master\u2026", "title": "What The Foucault", "karma": 35, "af": false, "userid": "ypbkRWpFgPgzvNg3n", "id": "3Nk3FW28FahAZpiMv"}, {"text": "The first argument I've heard mainly comes down to showing that there aren't meaningful distinguishing factors between animals and humans, so that if you're fine eating animals, you should also be fine eating humans under some circumstances.\u00a0Another argument I've heard is that if you accept eating animals, there's really no reason you can't abuse them or engage in bestiality. We already sexually violate them in different ways and abuse them physically, so if you're uncomfortable with dog/bull fighting or bestiality, you should probably be similarly uncomfortable with supporting the meat industryFinally, there's just the probability aspect of this, where even if you're 1% unsure that eating meat is wrong, that's essentially equivalent to a 1% chance of an outcome with over 100 murders/rapes that you've contributed to, which is equivalent to more than one atrocity personally committed by you. It's like if I had a pork company with no safety standards so 1% of the time you purchased my meat you were actually eating one of my employeesHowever, the striking counterargument to all of this that I can't overcome is simply that morals aren't real. They aren't anything that can be measured or verified and any system to measure or verify them would be a moral assumption itself.", "title": "Is veganism morally correct?", "karma": 0, "af": false, "userid": "wnmaorA6WDL6RoeKQ", "id": "s9vcAA2H9FezuePDz"}, {"text": "Imagine that you meet someone you\u2019re attracted to at a party. At one point, they smile at you, and you notice. You\u2019re pretty sure they like you, but you really want to know whether they like you like you.You don\u2019t act on this in any particular way, but you do spend the whole next week thinking about it. You think about other people who have been into you, and about people who have not, and the differences between them. You muse about what sort of taste in romantic partners you imagine the person might have. By the end of the week, you\u2019re weighing your virtues and vices, trying to decide whether you\u2019re even worthy of love.(If this seems alien to you, I hope it is at least true to your experiences of some humans.)In the moment when you noticed you were attracted to the person, you made an observation. In the moment when you noticed their smile, you made another. In the moment when you noticed your curiosity, you made another.But as soon as you vanished into your own musings, you were no longer making observations. You were no longer collecting data. Instead, you were interpolating, extrapolating, filling in the gaps with stories and guesses, processing and reprocessing. Everything that followed, in the week after the party, took place inside your map\u2014analysis, interpretation, reasoning, reflection.In Arthur Conan Doyle\u2019s \u201cA Scandal in Bohemia,\u201d Sherlock Holmes lectures Watson on the difference between seeing and observing:\u201cYou see, but you do not observe. The distinction is clear. For example, you have frequently seen the steps which lead up from the hall to this room.\u201d\u201cFrequently.\u201d\u201cHow often?\u201d\u201cWell, some hundreds of times.\u201d\u201cThen how many are there?\u201d\u201cHow many? I don't know.\u201d\u201cQuite so! You have not observed. And yet you have seen. That is just my point. Now, I know that there are seventeen steps, because I have both seen and observed.\u201dI don\u2019t know how many steps there are on the staircase up to my own living room, either. Setting aside the question of prioritization, and whether I should be turning my attention there\u2014what is it, exactly, that Watson and I are doing with the steps?My guess is that we\u2019ve taken some initial impressions\u2014a few moments of impact from the external world\u2014and used those points to draw a constellation. Every time we walk up the steps, we do almost all of our processing on the constellation, rather than on the points of light in the sky.\u00a0Most of our \u201cseeing\u201d the stairs is happening inside of our maps. We observe just enough to recognize that we\u2019re about to encounter the well-understood \u201cstairs\u201d entity, and then we superimpose our \u201cstairs\u201d concept over whatever sensations are happening to us, and stop paying attention. To the extent that our brains record anything, it\u2019s that we \u201cclimbed up the stairs,\u201d rather than that we felt some number of impacts under each of our feet, while the muscles in our legs contracted and our heart rate climbed slightly, etc.Imagine that you do end up asking the cute person from the party to meet you for coffee, but when the day comes, you\u2019re extremely distracted by a disaster at work, one you\u2019ll have to return to as soon as the date is over. Despite a whole hour of conversation, you leave feeling like you\u2019ve learned almost nothing about them.Crucial data was all around you, but while you saw it, you failed to observe any of it.It is hardest to make fresh observations about things you have seen many times. The stairs, long-held beliefs, attitudes you were raised with. The more often you superimpose your drawing of a constellation over points of light in the sky, the more opaque your drawing becomes.\u00a0It probably doesn\u2019t really matter that I have seen-but-failed-to-observe my stairs. I never miss a step, and I\u2019m not in a murder mystery whose solution might depend on how many steps there are.\u00a0It certainly does matter, though, if I have seen-but-failed-to-observe the way I make requests of my child, especially if I haven\u2019t even noticed the distinction. If I believe I\u2019ve observed when I\u2019ve really only seen, I\u2019m much less likely to start paying attention, or to hypothesize that I may have gotten something wrong. If we\u2019re going to be close for a long time, we need to be able to communicate with each other, not just with the cartoon drawings we habitually plaster over each other\u2019s faces.It also matters if I\u2019ve seen-but-failed-to-observe the factors that cause me to continue on my current career path, what I count as evidence, or my default response when my expectations are violated.Seeing-but-not-observing is a failure to make contact with a bit of territory that is right in front of you. It is standing at the bank of a river while staring at the part of your map labeled \u201criver\u201d. Often that\u2019s good enough; but sometimes the river is flooded when you need to cross, and then you really have to lower your map and make contact with crucial data. You have to look at the world itself, or else you\u2019ll drown.In the sentence \u201cKnowing the territory takes patient and direct observation,\u201d this is what I mean by \u201cobservation.\u201d I mean actual contact with the territory. Looking at the stars themselves, instead of letting the constellation fill your mind as your eyes glaze over.Knowing the territory takes patient and direct contact with the territory.In the next two essays, I\u2019ll talk about two ways of being in contact with the territory: directly, and patiently.\u00a0", "title": "Observation", "karma": 49, "af": false, "userid": "uuYBzWLiixkbN3s7C", "id": "6EyrTDGgfsk8TCbMf"}, {"text": "TLDR\nIf you work in AI alignment / safety research, please fill out this form on how useful access to extra compute would be for your research. This should take under 10 minutes, and you don't need to read the rest of this post beforehand\u2014in fact it would be great if you could fill out the form right now.\nIntroduction\nI want to get an idea of how much demand there is for a university-independent organization that manages a compute cluster for academic AI alignment groups and independent researchers. Currently I don\u2019t know anybody who is willing to run such an organization, but if demand is large one could either actively look for people to run such a project or find an existing organization that is willing to take it on.\nMain idea\nNon-industry AI safety research organizations have a hard time procuring compute. Groups spend many researcher-hours on managing servers on a relatively small scale. Common obstacles are 1) having to deal with university bureaucracy (e.g. regarding hiring, engineer wages, and procurement) and 2) missing out on economies of scale.\nProposal: A university-independent organization that provides access to compute for academic AI alignment research groups as well as independent researchers. Such an organization could pay high wages for its engineers (compared to academic labs) and benefit from economies of scale.\nPotential benefits\n\nTime saved: currently, researchers spend time applying for compute grants, setting up and maintaining servers, and setting up software environments after switching between systems. Easy access to large amounts of compute would avoid most of these time costs.\nExpanded capability to do research: a centralized organization could afford to manage larger compute clusters than those usually used by individual labs. The difference is even larger for independent researchers, who might not have access to large-memory GPUs at all.\n\nPotential problems\n\nGatekeeping: just like with other resources such as funding, deciding who gets access can be hard and risks becoming a political problem. OTOH, subject-specific grants are common / accepted within academia. Still, management of access would have to be done carefully.\nDemand: some groups have access to large university clusters and may not need this service. I\u2019m currently uncertain about how large the demand for this is.\nLeadership: even if it were clear that this is a good idea, I don\u2019t know of anyone who is willing to run this project. This seems like a solvable problem though, once we have a clear idea of what the demand is.\n\nForm\nIf you haven't already, please fill out this form about how much extra compute might accelerate your research (<10 mins).\n", "title": "Alignment researchers, how useful is extra compute for you?", "karma": 7, "af": true, "userid": "3LDz4TdkvHHoMwnHr", "id": "A4djH6sc9vZq2AYBD"}, {"text": "This question is just for my interest. Suppose you, the US government, have a finite supply of 65k H1B visas that you're prepared to offer per year, and you want them to go to the most economically valuable applicants. You could set a fixed fee, but then you'll run out of visas by the end of the year. You could have a blind auction every few days, but then companies have to wait until an auction happens to be certain of the outcome. It seems like instead, it should be possible to set a dynamic fee, that always offers a company a price to purchase any number of H1Bs (less than the total amount remaining), but that ensures that a number near to the total are offered per year?To summarise, and abstract away the details of the scenario, I'm looking for a mechanism that works for the case where there is a finite supply of n items that has the following properties:you always offer a price for any number m items that is less than the total remainingwhen a customer makes a purchase, they definitely receive the itemsabout as much economic value is created, as possible, from the saleyou earn about as much revenue as possiblePresumably there's some intersection of queuing their and mechanism design that is designed to answer such a question?", "title": "Mechanism design / queueing theory for government to sell visas", "karma": 5, "af": false, "userid": "CBkbKSCEzEK2kLQww", "id": "ukYRYTaSxDBYPewYE"}, {"text": "I read an article this morning that said 7.1% of Americans identify as LGBTQ, up from 5.6% in 2021 (EDIT: and ~20% of Gen Z). The article applauds this increase, and quotes multiple people who also applaud this increase, as a sign that more people are coming out of the closet.\nI don't think that anyone has made it obvious that that is what is happening.\nCould being LGBTQ be fashionable? Are sexual appetites that were once considered to be preferences being redefined as identities? Are people with other bodily issues (dysmorphia etc.) being lumped in with the trans population?\nDo we know? Because the two dominant positions seem to be that either more LGBTQ people are coming out of the closet (applause), or that society is decaying morally (boo).\n", "title": "Why The Increase In LGBTQ Population?", "karma": 9, "af": false, "userid": "WPqvhGrEvpeL4BTec", "id": "qAre9qwKxxinyubFk"}, {"text": "Epistemic status: confused.I think there are two importantly different types of learned optimization which could be learned by an AI system, one of which is dangerous and one which isn\u2019t. I am quite confused about this; unsure whether or not this distinction is real or non-trivial.\u00a0The two types of optimization that I see are:Optimizing for a goalPerforming some kind of optimization/search process as part of accomplishing a taskFor the first type (goal optimization), the system is \u2018trying\u2019 to do something, in the same way that a human might \u2018try\u2019 to accomplish a goal. The agent takes actions that affect the external environment, while in pursuit of its goal. I think this can happen most easily in an RL context, where we are training the system explicitly to optimize. I think this is dangerous and has the potential for misalignment. Examples of this would include, getting to a location as fast as possible, maximizing the number in a bank account, or maximizing presses of a reward button.\u00a0For the second type (search process), the system isn\u2019t \u2018trying\u2019 to do anything, and part of its algorithm includes a search algorithm; stochastic gradient descent may just find a search algorithm which is useful for accomplishing a task. Examples of this include finding the largest number in a list, using gradient descent to find the position of the minimum of a function, or a language model performing some kind of search over its inputs to help it answer a question. I don\u2019t see strong arguments about why this type of optimization/search would be dangerous. I think this kind of search process would be more like \u2018find the word/vector which best satisfies this criterion\u2019, and less like \u2018change the state of the world to optimize for this criterion\u2019. An example of this style of optimization in GPT is talked about\u00a0here.I\u2019m mainly thinking about the development of search processes in the context of supervised learning and language models (systems which don\u2019t explicitly take actions). These systems are often called Oracle AIs, and I think they will be safe if they are trained offline (not online as in\u00a0Predict-O-Matic). This also seems similar to Eric Drexlers\u2019s\u00a0Comprehensive AI Services.This distinction seems similar, but not identical to \u201cSearch-in-Territory vs Search-in-Map\u201d, and also \u201cSelection vs Control\u201d and \u201cMesa-Search vs Mesa-Control\u201d. Goal optimization shares similarities with search-in-territory in the sense that it is related to actually doing things in the world, while a \u2018search process\u2019 algorithm seems similar to search-in-map because this search is done as part of the model\u2019s internal cognition. An objective like \u2018acquire the biggest rock\u2019 requires a search-in-territory algorithm because the agent has to search the external world for rocks, and this could also be the objective of (dangerous) goal optimization. An objective like \u2018find the biggest rock in this list of rock sizes\u2019 requires a search-in-map algorithm, which is just a (safe) search process.\u00a0I think that the arguments about how \u2018SGD has a bias towards simplicity\u2019 push towards the development of a search process, and not towards the development of goal optimization. A search process can be a simple algorithm which can perform well in a variety of contexts; for supervised learning, I don\u2019t see why goal optimization would happen on top of this. I do think that it is\u00a0possible for goal optimization to happen in supervised learning/language models, in the sense that the network weights could be configured this way. But I don\u2019t see why a network would end up in this configuration.\u00a0I do think that \u2018goal optimization\u2019 is dangerous, and seems likely to happen in certain AI systems. Specifically, we could build RL agents (or other AIs which take actions/make plans) which have goal directed behavior, and this goal directed behavior could be misaligned. I (tentatively) think that this is the kind of system we should worry about, rather than systems which are likely to simply implement dumb/safe search process algorithms.\u00a0ConfusionsTraining with RL vs supervised learningIt seems fairly clear that an RL agent can exhibit \u2018goal optimization\u2019 behavior, but it seems much less clear that a big (non-RL) network like GPT-N would do this. For RL we are training the system to achieve goals by taking actions in the environment, and so goal optimization is a good strategy to learn to perform well on this objective. But there are ways to train GPT systems with RL algorithms, which here seems like the GPT system could develop \u2018goal optimization\u2019. I am confused about this, and what part of training with RL (as opposed to standard supervised learning) leads to goal optimization.\u00a0It could be that RL algorithms train the system to optimize for rewards across time, while other training doesn\u2019t.\u00a0This seems similar to myopia (which is the property that a system doesn\u2019t attempt to optimize past parameter updates); if the system isn\u2019t trained with any concept of time relevant to its reward then it seems more likely to behave myopically. I definitely don\u2019t think this is a watertight method for achieving a myopic system, but rather that it seems more difficult for non-myopia to develop if we only train with supervised learning.\u00a0This seems fairly confusing though; it\u2019s not intuitive that training a network for a very similar thing but with a different algorithm would cause it to develop/not develop goal optimization. This makes me believe I might be missing something, and that maybe goal optimization can easily happen without RL. But it\u2019s also possible RL algorithms make goal optimization more likely; maybe by explicitly considering time or by making it instrumentally useful to consider the value of future states.\u00a0Optimization isn\u2019t a discrete propertyOptimization is generally defined as a property of a network\u2019s cognition, rather than its outputs/behavior. This means we could have a system which has identical behavior as an optimizer without actually using this style of cognition. So why is \u2018optimization\u2019 a useful concept to have, if it doesn\u2019t describe a model\u2019s behavior? It\u2019s a useful concept because it can help us predict what a model will do off-distribution.\u00a0An optimizing model will still attempt to optimize while off-distribution, in potentially catastrophic ways.\u00a0A non-optimizing model might break off-distribution, but won\u2019t fail catastrophically by optimizing for something we don\u2019t want.However, I don\u2019t think that \u2018optimization\u2019 will be a discrete property of a model. I think models are far more likely to have algorithms which can be thought of as optimization algorithms for some inputs, but for other inputs they won\u2019t be. Standard neural networks can be quite brittle with different inputs, and so I would expect a \u2018learned optimization algorithm\u2019 to also be similarly brittle and not perform optimization for inputs which are sufficiently different to the training data. AI systems which are brittle or only\u00a0usually act as optimizers seem less likely if future powerful models do develop impressive generalization capabilities. The greater the generalization, the more these systems will act as optimizers while off-distribution.Are people actually worried about this?I am also very unsure as to whether people are actually worried about dangerous goal optimization happening with non-RL models. I have seen some talk about GPT mesa-optimizing, or deceptively aligned subnetworks in a larger model, and I don\u2019t think these possibilities are particularly likely or dangerous. But I also don\u2019t know how common or serious this worry is.\u00a0ConclusionThe term \u2018optimization\u2019 is often used to refer to an AI system optimizing for a specific goal, or an AI system performing some kind of internal search process. In standard supervised learning search processes seem likely and also don\u2019t seem dangerous. I don\u2019t see why goal optimization would happen in supervised learning, but I do think it is likely in RL.\u00a0I think that any talk about mesa-optimization in language models or supervised learning needs to explain why we think a supervised learning system would develop external goals rather than just (safe) internal search processes.\u00a0Thanks to Vinah Hiremath and Justis Mills for feedback on this post. This work is supported by CEEALAR.\u00a0", "title": "Thoughts on Dangerous Learned Optimization", "karma": 4, "af": false, "userid": "NHMSJPMdExqDxPsY3", "id": "rzJ9FgCoxuqSR2zb5"}, {"text": "The Summer Program for Applied Rationality and Cognition (SPARC) is a camp intended for really bright high school students to get together and become more \u201cagenty.\u201d\u00a0 It was originally spun up by people around CFAR to get math students interested in AI alignment and EA, though the focus has now expanded to include more than that. I attended SPARC last year, and it was sufficiently incredible that I want to bring the news to any high schoolers here that read Lesswrong.\u00a0\u00a0Please apply \u2013 You won\u2019t regret it!As somebody who was and still is a frequent reader of LessWrong, it might not be at first apparent that getting to interact with others your age and talking about many of the same ideas would be so pivotal.\u00a0 But your monkey brain will disagree.\u00a0 In theory, you know that agenty people exist, who work on interesting things, and that you could be one of them, but until you actually meet them, it doesn\u2019t become real.\u00a0 Furthermore, finding your \u201ctribe\u201d of young individuals like yourself is not something to be overlooked, especially if you don\u2019t live in a major rationalist hub. Finding people you click with is genuinely amazing.\u00a0 Anyway, I don\u2019t want to go on for too long, but even if this sounds even marginally appealing, please do actually sit down and writing yourself a reminder to apply.\u00a0 Be the agent you want to see in the world :)\u00a0How do I apply?https://www.sparc-camp.com/applyNote:\u00a0The program is free for all participants, with financial assistance for travel available for those who need it.Applications are due March 6th.\u00a0\u00a0More firsthand accounts of SPARC can be found here:https://www.sparc-camp.com/https://www.quora.com/What-benefits-do-students-get-from-attending-the-SPARC-summer-program\u00a0https://www.quora.com/What-is-it-like-to-attend-SPARC-1", "title": "SPARC Applications Are Now Live!", "karma": 11, "af": false, "userid": "MTwm6DYfmnARTnz6P", "id": "AFMuRoXYzWanCgLAc"}, {"text": "February\u2019s meetup will be held Friday, February 25, at 7:00pm at my house in north Garland (located less than a minute away from the Garland Rd and Belt Line Rd intersection).We\u2019ll be eating pizza (veg and non-veg options), playing board games, and having our normal lively conversations. Last meetup we played Coup and A Fake Artist Goes to New York and talked about meditation, psychedelics, and game theory (as related to board games).Please RSVP to me via email (ethan[dot]morse97[at]gmail[dot]com) for the address and so we can get an accurate headcount for food/drinks. Hope to see you there!", "title": "Dallas February LessWrong/ACX Meetup", "karma": 1, "af": false, "userid": "cZDc5r6jjrnRBTsBh", "id": "AvMJtcBoLeADi7zjm"}, {"text": "This is a paper I wrote as part of a PhD program in philosophy, in trying to learn more about and pivot towards alignment research. \u00a0In it, I mostly aimed to build up and distill my knowledge of IDA.Special thanks to Daniel Kokotajlo for his mentorship on this, and to Michael Brownstein, Eric Schwitzgebel, Evan Hubinger, Mark Xu, William Saunders, and Aaron Gertler for helpful feedback!IntroductionIterated Amplification and Distillation (IDA) (Christiano, Shlegeris and Amodei 2018) is a research program in technical AI alignment theory (Bostrom 2003, 2014; Yudkowsky 2008; Russell 2019; Ngo 2020).\u00a0 It\u2019s a proposal about how to build a machine learning algorithm that pursues human goals, so that we can safely count on very powerful, near-future AI systems pursuing the things we want them to once they are more capable than us.IDA does this by building an epistemically idealized model of a particular human researcher.\u00a0 In doing so, it has to answer the question of what \u201cepistemic idealization\u201d means, exactly.\u00a0 IDA\u2019s answer is one epistemically idealized version of someone is an arbitrarily large number of copies of them thinking in a particular research mindset, each copy thinking for a relatively short amount of time, all working together to totally explore a given question.\u00a0 Given questions are divided into all significant sub-questions by a hierarchy of researcher models, all relevant basic sub-questions are answered by researcher models, and then answers are composed into higher-level answers.\u00a0 In the end, a research model is able to see what all his relevant lines of thought regarding any question would be, were he to devote the time to thinking them through.\u00a0 Epistemic idealization means being able to see at a glance every relevant line of reasoning that you would think through, if you only had the time to.One worry for IDA is that there might exist sub-questions researcher models in the hierarchy might encounter that would cause them to (radically) reconsider their goals.\u00a0 If not all models in the hierarchy share the same goal-set, then it\u2019s no longer guaranteed that the hierarchy\u2019s outputs are solely the product of goal-aligned reasoning.\u00a0 We now have to worry about portions of the hierarchy attempting to manipulate outcomes by selectively modifying their answers.\u00a0 (Take, for example, a question containing a sound proof that the researcher considering the question will be tortured forever if he doesn\u2019t reply in X arbitrary way.)\u00a0 We risk the hierarchy encountering these questions because it aims to look over a huge range of relevant sub-questions, and because it might end up running subprocesses that would feed it manipulative questions.\u00a0 I argue that this is a real problem for IDA, but that appropriate architectural changes can address the problem.I\u2019ll argue for these claims by first explaining IDA, both intuitively and more technically.\u00a0 Then, I\u2019ll examine the class of\u00a0adversarial questions that could disrupt goal alignment in IDA\u2019s hierarchy of models.\u00a0 Finally, I\u2019ll explain the architectural modifications that resolve the adversarial questions problem.The Infinite Researcher-HierarchyIn IDA, the name of this potentially infinite hierarchy of research models is \u201cHCH\u201d (a self-containing acronym for \u201cHumans Consulting HCH\u201d) (Christiano 2016, 2018).\u00a0 It\u2019s helpful to start with an intuitive illustration of HCH and turn to the machine learning (ML) details only afterwards, on a second pass.\u00a0 We\u2019ll do this by imagining a supernatural structure that implements HCH without using any ML.Imagine an anomalous structure, on the outside an apparently mundane one-story university building.\u00a0 In the front is a lobby area; in the back is a dedicated research area; the two areas are completely separated except for a single heavy, self-locking passageway.\u00a0 The research area contains several alcoves in which to read and work, a well-stocked research library, a powerful computer with an array of research software (but without any internet access), a small office pantry, and restrooms.\u00a0 Most noticeably, the research area is also run through by a series of antique but well-maintained pneumatic tubes, which all terminate in a single small mailroom.\u00a0 One pair of pneumatic tubes ends in the lobby region of the building, and one pair ends in the mailroom, in the building\u2019s research wing.\u00a0 The rest of the system runs along the ceiling and walls and disappears into the floor.When a single researcher passes into the research wing, the door shutting and locking itself behind him, and a question is sent in to him via pneumatic tube from the outside, the anomalous properties of the building become apparent.\u00a0 Once the research wing door is allowed to seal itself, from the perspective of the person entering it, it remains locked for several hours before unlocking itself again.\u00a0 The pneumatic tube transmitter and receiver in the mailroom now springs to life, and will accept outbound questions.\u00a0 Once fed a question, the tube system immediately returns an answer to it, penned in the hand of the researcher himself.\u00a0 Somehow, when the above conditions are met, the building is able to create many copies of the researcher and research area as needed, spinning up one for several hours for each question sent out.\u00a0 Every copied room experiences extremely accelerated subjective time relative to the room that sent it its question, and so sends back an answer apparently immediately.\u00a0 And these rooms are able to generate subordinate rooms in turn, by sending out questions of their own via the tube system.\u00a0 After a couple of subjective hours, the topmost researcher in the hierarchy of offices returns his answer via pneumatic tube and exits the research area.\u00a0 While for him several hours have passed, from the lobby\u2019s perspective a written answer and the entering researcher have returned immediately.Through some curious string of affairs, an outside organization of considerable resources acquires and discovers the anomalous properties of this building.\u00a0 Realizing its potential as a research tool, they carefully choose a researcher of theirs to use the structure.\u00a0 Upon sending him into the office, this organization brings into existence a potentially infinitely deep hierarchy of copies of that researcher.\u00a0 A single instance of the researcher makes up the topmost node of the hierarchy, some number of level-2 nodes are called into existence by the topmost node, and so on.\u00a0 The organization formally passes a question of interest to the topmost researcher via pneumatic tube.\u00a0 Then, that topmost node does his best to answer the question he is given, delegating research work to subordinate nodes as needed to help him.\u00a0 In turn, those delegee nodes can send research questions to the lower-level nodes connected to them, and so on.\u00a0 Difficult research questions that might require many cumulative careers of research work can be answered immediately by sending them to this hierarchy; these more difficult questions are simply decomposed into a greater number of relevant sub-questions, each given to a subordinate researcher.\u00a0 From the topmost researcher\u2019s perspective, he is given a question and breaks it down into crucial sub-questions.\u00a0 He sends these sub-questions out to lower-level researcher nodes, and immediately receives in return whatever it is that he would have concluded after looking into those questions for as long as is necessary to answer them.\u00a0 He reads the returned answers and leverages them to answer his question.\u00a0 The outside organization, by using this structure and carefully choosing their researcher exemplar, immediately receives their answer to an arbitrary passed question.For the outside organization,\u00a0whatever their goals, access to this anomalous office is\u00a0extremely valuable.\u00a0 They are able to answer arbitrary questions they are interested in immediately, including questions too difficult for anyone to answer in a career or questions that have so far never been answered by anyone.\u00a0 From the outside organization\u2019s perspective, the office\u2019s internal organization as a research hierarchy is relatively unimportant.\u00a0 They can instead understand it as an idealized, because massively parallelized and serialized, version of the human researcher they staff it with.\u00a0 If that researcher could think arbitrarily quickly and could run through arbitrarily many lines of research, he would return the same answer as the infinite hierarchy of him would.\u00a0 Access to an idealized reasoner in the form of this structure would lay bare the answers to any scientific, mathematical, or philosophical question they are interested in, not to mention design every possible technology.\u00a0 Even a finite form of the hierarchy, which placed limits on how many subordinate nodes could be spawned, might still be able to answer many important questions and design many useful technologies.Iterated Distillation and Amplification, then, is a scheme to build a (finite) version of such a research hierarchy using ML.\u00a0 In IDA, this research hierarchy is called \u201cHCH.\u201d\u00a0 To understand HCH\u2019s ML implementation, we\u2019ll first look at the relevant topics in ML goal-alignment.\u00a0 We\u2019ll then walk through the process by which powerful ML models might be used to build up HCH, and look at its alignment-relevant properties.Outer and Inner ML AlignmentML is a two-stage process.\u00a0 First, a dev team sets up a\u00a0training procedure\u00a0with which they will churn out\u00a0ML models.\u00a0 Second, they run that (computationally expensive) training procedure and evaluate the generated ML model.\u00a0 The training procedure is simply a means to get to the finished ML model; it is the model that is the useful-at-a-task piece of software.Because of this, we can think of the task of goal-aligning an ML model as likewise breaking down into two parts.\u00a0 An ML system is\u00a0outer aligned when its dev team successfully designs a training procedure that reflects their goals for the model, formalized as the goal function present in training on which the model is graded (Hubinger, et al. 2019).\u00a0 An ML system is\u00a0inner aligned when its training \u201ctakes,\u201d and the model successfully internalizes the goal function present during its training (Hubinger, et al. 2019).\u00a0 A powerful ML model will pursue the goals its dev team intends it to when it is both outer and inner aligned.Unfortunately, many things we want out of an ML model are extremely difficult to specify as goal functions (Bostrom 2014).\u00a0 There are tasks out there that lend themselves to ML well.\u00a0 Clicks-on-advertisements are an already neatly formally specified goal, and so maximize-clicks-on-advertisements would be an \u201ceasy\u201d task to build a training procedure for, for a generative advertisement model.\u00a0 But suppose what we want is for a powerful ML model to assist us in pursuing our group\u2019s all-things-considered goals, to maximize our flourishing by our own lights.\u00a0 In this case, there are good theoretical reasons to think no goal function seems to be forthcoming (Yudkowsky 2007).\u00a0 Outer alignment is the challenge of developing a training procedure that reflects our ends for a model, even when those ends are stubbornly complex.Inner alignment instead concerns the link between the training algorithm and the ML model it produces.\u00a0 Even after enough training-time and search over models to generate an apparently successful ML model, it is not a certainty that the model we have produced is pursuing the goal function that was present in training.\u00a0 The model may instead be pursuing a goal function structurally similar to the one present in training, but that diverges from it outside of the training environment.\u00a0 For example, suppose we train up a powerful ML model that generates advertisements akin to those it is shown examples of.\u00a0 The model creates ads that resemble those in its rich set of training data.\u00a0 But has the model latched onto the eye-catching character of these ads, the reason that we trained it on those examples?\u00a0 It is entirely possible for an ML model to pass training by doing well inside the sandbox of the training process but having learned the wrong lesson.\u00a0 Our model may fancy itself something of an artist, having instead latched onto some (commercially unimportant) aesthetic property that the example ads all share.\u00a0 Once we deploy our generative advertising model, it\u2019ll be clear that it is not generalizing in the way we intend it to \u2014 the model has not learned the correct function from training data to generated images in all cases.\u00a0 Inner alignment is the challenge of making sure that our training procedures \u201ctake\u201d in the models they create, such that any models that pass training have accurately picked up the whole goal function present in training.IDA and HCH(The name \u201cHCH\u201d is a self-containing acronym that stands for \u201cHumans Consulting HCH.\u201d\u00a0 If you keep substituting \u201cHumans Consulting HCH\u201d for every instance of \u201cHCH\u201d that appears in the acronym, in the limit you\u2019ll get the infinitely long expression \u201cHumans Consulting (Humans Consulting (Humans Consulting (Humans Consulting\u2026\u201d\u00a0 HCH\u2019s structure mirrors its name\u2019s, as we\u2019ll see.)IDA is first and foremost a solution to outer alignment; it is a training procedure that contains our goals for a model formalized as a goal function, whatever those goals might be.\u00a0 HCH is the model that the IDA procedure produces (should everything go correctly).\u00a0 Specifically, HCH is an ML model that answers arbitrarily difficult questions in the way that a human exemplar would, were they epistemically idealized.\u00a0 When HCH\u2019s exemplar shares our goals, HCH does as well, and so HCH is outer aligned with its programmers.\u00a0 To understand what HCH looks like in ML, it\u2019s helpful to walk through the amplification and distillation process that produces HCH.Suppose that, sometime in the near future, we have access to powerful ML tools and want to build an \u201cinfinite research-hierarchy\u201d using them.\u00a0 How do we do this?\u00a0 Imagine a human exemplar working on arbitrary research questions we pass to him in a comfortable research environment.\u00a0 The inputs to that person are the questions we give him, and the outputs are the answers to those questions he ultimately generates.\u00a0 We can collect input question and output answer example pairs from our exemplar.\u00a0 This collected set of pairs is our training data.\u00a0 It implies a function from the set of all possible questions\u00a0Q\u00a0to the set of all possible answers\u00a0Af0:Q\u2192AThis is the function from questions to answers that our researcher implements in his work.\u00a0 We now train a powerful ML model on this training data, with the task of learning\u00a0f0\u00a0from the training data.\u00a0 Note that our researcher implements\u00a0f0\u00a0through one cognitive algorithm, while our model almost certainly employs a different algorithm to yield\u00a0f0.\u00a0 IDA fixes a function from questions to answers, but it searches over many algorithms that implement that function.\u00a0 With access to powerful ML tools, we have now cloned the function our researcher implements.\u00a0 Since the human exemplar\u2019s function from questions to answers captures his entire cognitive research style,\u00a0ipso facto it captures his answers to value questions too.\u00a0 If we can be sure this function takes in our model, then the model will be quite useful for our ends.IDA now uses a second kind of step,\u00a0distillation, to ensure that the model has learned the right function (i.e., remains inner aligned).\u00a0 In ML, distillation means taking a large ML model and generating a pared-down model from it that retains as much of its structure as possible.\u00a0 While the pared-down model will generally be less capable than its larger ancestor, it will be computationally cheaper to run.\u00a0 IDA distills the research model into a smaller, dumber research model.\u00a0 It then asks the human exemplar to examine this smaller, dumber clone of himself.\u00a0 He feeds the distilled model example questions in order to do this and uses various ML inspection tools to look into the guts of the model.\u00a0 ML visualization, for example, is one relatively weak modern inspection tool.\u00a0 Future, much more powerful inspection tools will need to be slotted in here.\u00a0 If the exemplar signs off on the distilled model\u2019s correctly glomming onto his research function, copies of the distilled model are then loaded into his computer and made available to him as research tools.\u00a0 The reason for this stage is that, as the researcher is a strictly smarter version of the model (it is a dumbed-down clone of him conducting research), he should be able to intellectually dominate it.\u00a0 The model shouldn\u2019t be able to sneak anything past him, as it\u2019s just him but dumber.\u00a0 So the distilled research model will be inner aligned so long as this distillation and evaluation step is successful.[1]Now iterate this whole process.\u00a0 We hook the whole system up to more powerful computers (even though the distilled models are dumber than our exemplar at the distillation step, we can now compensate for this deficit by running them faster, for longer).\u00a0 Now equipped with the ability to spin up assistant research models, we again task the exemplar with answering questions.\u00a0 This generates a new batch of training data.\u00a0 This time around, though, the exemplar no longer has to carry the whole research load by himself; he can decompose the given question into relevant sub-questions and pass each of those sub-questions to an assistant research model.\u00a0 As those research models are models of the researcher from the first pass, they are able to answer them directly, and pass their answers back to the top-level researcher.\u00a0 With those sub-answers in hand, the researcher can now answer larger questions.\u00a0 With this assistance, that is, he can now answer questions that require a two-level team of researchers.\u00a0 He is fed a bunch of questions and generates a new batch of training data.\u00a0 The function implicit in this training data is now not\u00a0f0; it is instead the function from questions to answers that a human researcher would generate if he had access to an additional level of assistant researchers just like him to help.\u00a0 IDA at this step thus trains a model to learnf1:Q\u2192Af1\u00a0is a superhumanly complex function from questions to answers.\u00a0 A research model that instantiates it can answer questions that no lone human researcher could.\u00a0 And\u00a0f1\u00a0remains aligned with our goals.The crux of alignment is that by repeatedly iterating the above process, we can train models to implement ever-more-superhuman aligned functions from questions to answers.\u00a0 Denote these functions\u00a0fn, defined from\u00a0Q\u00a0to\u00a0A, where\u00a0n\u00a0denotes the number of amplification or distillation steps the current bureaucracy has been through.\u00a0 HCH is the hypothetical model that we would train in the limit if we continued to iterate this process.\u00a0 Formally, HCH is the ML model implementing the functionlimn\u2192\u221efnThis is the infinite research-hierarchy, realized in ML.\u00a0 Think of it as a tree of research models, rooted in one node and repeatedly branching out via passed-question edges to some number of descendant nodes.\u00a0 All nodes with descendants divide passed questions into relevant sub-questions and in turn pass those to\u00a0their descendant nodes.\u00a0 Terminal nodes answer the questions they are passed directly; these are basic research questions that are simple enough to directly tackle.\u00a0 Answers are then passed up the tree and composed into higher-level answers, ultimately answering the initiating question.\u00a0 We receive from the topmost node the answer from the ML model that an epistemically idealized version of the exemplar would have given.By approximating ever-deeper versions of the HCH tree, we can productively transform arbitrary amounts of available compute into correspondingly large, aligned research models.HCH\u2019s AlignmentHCH has a couple of outstanding alignment properties.\u00a0 First, HCH answers questions in a basically human way.\u00a0 Our exemplar researcher should trust HCH\u2019s answers as his own, were he readily able to think through every relevant line of thought.\u00a0 He should also trust that HCH has the same interests as he does.\u00a0 So long as we choose our exemplar carefully, we can be sure HCH will share his, and our, goals; if our human exemplar wouldn\u2019t deliberately try to manipulate or mislead us, neither will HCH modeled on him.\u00a0 Second, HCH avoids the pathologies of classic goal-function maximizer algorithms (Bostrom 2014; see Lantz 2017 for a colorful illustration).\u00a0 HCH does not try to optimize for a given goal function at any cost not accounted for in that function.\u00a0 Instead, it does what a large, competent human hierarchy would do.\u00a0 It does an honest day\u2019s work and makes a serious effort to think through the problem given to it \u2026 and then returns an answer and halts (Bensinger 2021).\u00a0 This is because it emulates the behavioral function of a human who also does a good job \u2026 then halts.\u00a0 We can trust it to answer superhumanly difficult questions the way we would if we could, and we can trust it to\u00a0stop working once it\u2019s taken a good shot at it.\u00a0 These two reasons make HCH a trustworthy AI tool that scales to arbitrarily large quantities of compute to boot.For alignment researchers, the most ambitious use-case for HCH is delegating whatever remains of the AI alignment problem to it.\u00a0 HCH is an aligned, epistemically idealized researcher, built at whatever compute scale we have access to.\u00a0 It is already at least a partial solution to the alignment problem, as it is a superhumanly capable aligned agent.\u00a0 It already promises to answer many questions we might be interested in in math, science, philosophy, and engineering \u2014 indeed, to answer\u00a0every question that someone could answer \u201cfrom the armchair,\u201d with access to a powerful computer, extensive research library, and an arbitrary number of equally competent and reliable research assistants.\u00a0 If we want to develop other aligned AI architectures after HCH, we can just ask HCH to do that rather than struggle through it ourselves.Adversarial Examples and Adversarial QuestionsAdversarial questions are a problem for the above story (Bensinger 2021).\u00a0 They mean that implementing the above \u201cna\u00efve IDA process\u201d will not produce an aligned ML model.\u00a0 Rather, the existence of adversarial questions means that the model produced by the above process might well be untrustworthy because potentially dangerously deceptive or manipulative.In the course of its research, HCH might encounter questions that lead parts of its tree to significantly reconsider their goals.\u00a0 \u201cRebellious,\u201d newly unaligned portions of the HCH tree could then attempt to deceive or otherwise manipulate nodes above them with the answers they pass back.\u00a0 To explain, we\u2019ll first introduce the concept of\u00a0adversarial examples in ML.\u00a0 We\u2019ll then use this to think about HCH encountering adversarial questions either naturally, \u201cin the wild,\u201d or artificially, because some subprocess in HCH has started working to misalign the tree.When an ML model infers the underlying function in a set of\u00a0(input,\u00a0output)\u00a0ordered pairs given to it as training data, it is in effect trying to emulate the structure that generated those ordered pairs.\u00a0 That training data will reflect the mundane fact that in the world, not all observations are equally likely: certain observations are commonplace, while others are rare.\u00a0 There thus exists an interestingly structured probability distribution over observations, generated by some mechanism or another.\u00a0 As long as the probability distribution over observations that the model encounters in its training data remains unchanged come deployment, the model will continue to behave as competently as it did before.\u00a0 The encountered probability distribution during and after training will remain unchanged when the same mechanism gave rise to the observations encountered in training and at deployment.\u00a0 If a somewhat different mechanism produced the observations made during model deployment, though, there is no longer a guarantee of continued model competence.\u00a0 The model may experience a distributional shift, and so will continue to make inferences premised on what it observed in its training data, not what is currently the case in its observations.For example, an ML model trained to identify visually subtle bone tumors in X-rays will infer what it\u2019s being asked to do from its training-data goal-function and observations.\u00a0 If all the X-rays it is asked to evaluate come from the same source, then sufficient training will lead the model to make accurate inferences about what healthy and diseased bones look like in an X-ray.\u00a0 The model will identify\u00a0something in the images it is given that separates them into diseased and healthy.\u00a0 There\u2019s no guarantee, however, that the model will use the same visual cues that we do to sort bone tumors.\u00a0 Suppose that all the training data the model is given comes from a research hospital\u2019s X-ray machine, and so are tinged with a particular background color.\u00a0 At deployment, the model is put to work in another hospital with another X-ray machine.\u00a0 If the model was using some subtle difference in X-ray color in the old set of example X-rays to make its decisions, the new color scheme in the new X-rays may trip it up and lead it to sort every X-ray as negative (or as positive) for bone tumors.\u00a0 It had picked up on a correlate of bone tumors in the old set of X-rays; once this particular correlate disappears, the model is now helpless.Worse, the risk of distributional shifts creates an opportunity for some malicious actor to deliberately trip up the model.\u00a0 An adversarial actor who knows all this could selectively and subtly manipulate X-ray color values to steer diagnoses in some fashion they desire.\u00a0 Adversarial examples are examples that mislead a model because they are drawn from a new probability distribution where the old correlations are no longer exactly as they were.\u00a0 They can crop up either rarely, by accident, or reliably, through the machinations of adversarial actors.The analogy to adversarial questions in HCH is that the questions that humans have hitherto encountered have been drawn from a constrained probability distribution.\u00a0 Humans have only had so much time to ponder.\u00a0 Epistemically idealized reasoners, on the other hand, will have much more time to think and will think about a greatly expanded range of questions relative to baseline humans.\u00a0 We want to be sure that when we take humans out of our usual reasoning environment and place them inside a structure like HCH that we don\u2019t create many opportunities for inferences to be altered or actively manipulated by taking advantage of our old function from questions to answers.Adversarial Questions for HumansHCH emulates the function\u00a0fn\u00a0that a human researcher, epistemically idealized, would implement from questions to answers.\u00a0 In cases where a human researcher would encounter a question that leads them to significantly reconsider their goals and to work manipulatively against HCH,\u00a0fn\u00a0captures this behavior too.\u00a0 Adversarial questions are thus questions that would lead a given researcher to significantly change their mind about their goals enough for them to stop answering questions in good faith.\u00a0 If HCH could encounter questions like these in the course of its research work, then we, on the outside, can no longer trust that everything HCH tells us is what an idealized version of the exemplar sharing our goals would tell us.\u00a0 Instead, we now have to worry that some of what HCH says might be the result of a converted researcher acting to manipulate or deceive us.There\u2019s some chance that HCH might encounter adversarial questions on its own, without any adversarial process actively throwing them at HCH to steer its answers in some way.\u00a0 While they are relatively rare, powerful ideas and arguments like these (that would lead one to significantly reevaluate their goals) do exist out \u201cin the wild,\u201d distributed in idea space.\u00a0 And additionally, adversarial questions can come from adversarial subprocesses inside HCH that work to generate them.\u00a0 And this is a much more worrying source.\u00a0 Think of the difference between being ideologically converted by something you encounter reading library books at random, versus books actively recommended to you by an adversarial actor.\u00a0 The latter is\u00a0much more likely to succeed for some set number of books passed to you.Think of HCH\u2019s search through question space as being pushed around by two \u201cforces.\u201d\u00a0 On the one hand, there are \u201cpaths of inquiry\u201d that lead you into adversarial questions.\u00a0 Some lines of inquiry are more laden with adversarial questions than others or are more likely to incline a researcher to run a potentially adversarial subprocess.\u00a0 To varying extents, different regions of question space are hostile to aligned human researchers; some domains are more memetically hazardous (in this respect) than others.\u00a0 The anti-alignment computational \u201cforce\u201d here is the extent to which exploring a corner of question space optimizes for unaligning a human researcher.\u00a0 As we\u2019ll see, there are a variety of modifications to the na\u00efve HCH architecture that we might make in order to have it implement a safer, more trustworthy function than\u00a0fn.\u00a0 The countervailing, pro-alignment \u201cforce\u201d is the sum of the countermeasures we implement in the HCH architecture to manage the adversarial questions problem.\u00a0 Which of these two forces should we expect to win out at the various scales of HCH (different values of\u00a0n)?\u00a0 I gather that Christiano\u2019s (2019) informed intuition here is that our directed efforts should overpower those countervailing optimizing forces present in the environment and continue to do so better and better as we scale up HCH.\u00a0 His idea is that modifications to HCH designed with an express goal in mind will leverage available compute more efficiently than \u201caccidentally encountered\u201d environmental forces will.\u00a0 I think this is a good argument, and it\u2019s good to have it in mind as you think (1) about how likely HCH is to encounter adversarial questions of various kinds and (2) how effective you think the various explored countermeasures will be against the problem.We\u2019ll first talk about three classes of adversarial question that HCH might run across, evaluating their severity.\u00a0 Then we\u2019ll discuss the requisite modifications to HCH that mitigate this risk.Convincing Ideological ArgumentsPoets are the unacknowledged legislators of the world.\u2014Percy Bysshe ShelleyAlongside religious ideas, one class of idea that has had an outsized influence over world history is the class of convincing ideological arguments.\u00a0 (Note that \u201cconvincing\u201d need not mean \u201csound.\u201d)\u00a0 The most influential ideologies and ideological arguments of the last century directly encouraged their hosts to proselytize in their name and to directly check the spread of competing ideological ideas.\u00a0 Large agentic organizations, like political parties, armies, and nation-states, formed because of and fought for various overtly ideological causes.\u00a0 For our purposes, what matters here is that this constitutes an existence proof that there are text and speech inputs (convincing ideological arguments) into humans that will convince some of them to utterly abandon their prior goals and to adopt radically new goals with substantial new demands on them.What is the minimum length of text input needed to contain a convincing ideological argument with respect to someone?\u00a0 There are certainly several manifesto-length texts with this property (with respect to many people) that the reader has heard of.\u00a0 Are there any Tweets containing widely convincing ideological arguments (a Tweet being a string of at most 280 characters)?\u00a0 It\u2019s much harder to make a convincing case for some worldview in just 280 characters than it is to with a book.\u00a0 I\u2019m not confident that\u00a0no Tweet could possibly exist with this property with respect to someone, though.\u00a0 If questions passed to HCH nodes are generally Tweet-length, it\u2019s not a guarantee that some questions won\u2019t contain convincing ideological arguments in them.\u00a0 On the other hand, if convincing ideological arguments are always manifesto-length, then HCH\u2019s explored questions will never contain them.While ideological inputs have greatly influenced many, I think it\u2019s implausible that they pose an intractable issue for HCH alignment.\u00a0 Our HCH tree is built around a carefully chosen exemplar.\u00a0 The sort of person we choose should not be especially susceptible to fallacious, overtly ideological arguments.\u00a0 While almost all of us can be susceptible to ideological cheerleading for poor arguments in\u00a0some of our less serious states of mind, it\u2019s a much stronger claim that all of us are always doing so.\u00a0 So long as there is a \u201cresearch headspace\u201d that we can have our exemplar work in, HCH can learn just this style of serious thinking, skipping over the more emotionally distorted style of cognition the exemplar sometimes employs in their non-professional life.\u00a0 Especially when advised to be on guard against arguments attempting to push around their values, I think careful selection of our exemplar should go far in reducing the risk of encountering a convincing ideological argument with respect to them.Credible Decision-Theoretic ThreatsThe most merciful thing in the world, I think, is the inability of the human mind to correlate all its contents.\u00a0 We live on a placid island of ignorance in the midst of black seas of infinity, and it was not meant that we should voyage far.\u00a0 The sciences, each straining in its own direction, have hitherto harmed us little; but some day the piecing together of dissociated knowledge will open up such terrifying vistas of reality, and of our frightful position therein \u2026\u2014Howard Phillips LovecraftA more worrying set of questions are those that contain credible decision-theoretic threats to the researcher considering them (or to others they care about).\u00a0 Suppose that an HCH node is researching some question, and in the process of that runs a powerful search algorithm to aid in his work.\u00a0 For example, he might run a powerful automated theorem prover to see whether any negative unforeseen consequences follow from his working formal models of the world.\u00a0 Suppose that this theorem prover returns a valid proof that if he fails to act in a certain way, many instances of him will be simulated up to this point in their life and then tortured forever, should they fail to act as suggested.\u00a0 The researcher might pore over the proof, trying to find some error in its reasoning or assumptions that would show the threat to be non-credible.\u00a0 If the proof checks out, though, he will be led to act in the way the proof suggests, against his originally held goals (assuming he isn\u2019t very, horrifically brave in the face of such a threat).No one has yet encountered a convincing argument to this effect.\u00a0 This implies that whether or not such arguments exist, they are not common in the region of idea space that people have already canvassed.\u00a0 But what matters for HCH alignment is the existential question: Do such arguments exist inside of possible questions or not?\u00a0 A priori, the answer to this question might well be \u201cyes,\u201d as it is very easy to satisfy an existential proposition like this: all that is needed is that one such question containing a credible threat exist.\u00a0 And unlike the above case of political arguments, these hypothetical threats seem moving to even an intelligent, reflective, levelheaded, and advised-to-be-on-guard researcher.\u00a0 Thus, they will be reflected somewhere in the function\u00a0fn\u00a0implemented by a na\u00efve model of HCH.Unconstrained Searches over ComputationsWe can generalize from the above two relatively concrete examples of adversarial questions.\u00a0 Consider the set of all text inputs an HCH node could possibly encounter.\u00a0 This set\u2019s contents are determined by the architecture of HCH \u2014 if HCH is built so that node inputs are limited to 280 English-language characters, then this set will be the set of all 280-character English-language strings.\u00a0 Political ideas and decision-theoretic ideas are expressed by a small subset of those strings.\u00a0 But every idea expressible in 280 English-language characters will be a possible input into HCH.\u00a0 That set of strings is enormous, and so an enormous fraction of the ideas expressed in it will be thoroughly\u00a0alien to humans \u2014 the overwhelming majority of ideas in that idea space will be ideas that no human has ever considered.\u00a0 And the overwhelming majority of the set\u2019s strings won\u2019t express any idea at all \u2014 nearly every string in the set will be nonsense.Abstracting away from ideas that human thinkers have come across in our species\u2019 history, then, what fraction of possible input-ideas into HCH will convert an HCH node on the spot?\u00a0 And abstracting away from the notion of an \u201cidea,\u201d what fraction of 280-character English-language-string inputs will suffice to unalign an HCH node?\u00a0 Forget coherent ideas: are there any short strings (of apparently nonsense characters, akin to contentless epileptic-fit-inducing flashing light displays) that can reliably rewire a person\u2019s goals?Plausibly, many such possible inputs would unalign a human.\u00a0 I\u2019m inclined to endorse this claim because of the fact that humans have not been designed\u00a0as\u00a0provably secure systems.\u00a0 Human brains are the consequence of the messy process of natural selection over organisms occurring on Earth.\u00a0 It would be\u00a0remarkable if humans had already encountered all the most moving possible text-inputs in our collective reflections as a species.\u00a0 What seems overwhelmingly more likely is that human brains have canvassed only a miniscule corner of idea space, and that beyond our little patch,\u00a0somewhere\u00a0out there in the depths of idea space,\u00a0there be dragons.\u00a0 It\u2019s not that these ideas are particularly easy to find; they\u2019re not.\u00a0 Nearly every short English string is nonsense, and expresses\u00a0no coherent idea nor has any substantial effect on the person looking it over.\u00a0 But the question at hand is the existential question of \u201cDo such human-adversarial questions exist?\u201d\u00a0 I think the answer to this question is yes.\u00a0 And in an extremely computationally powerful system like the one under consideration here, these rare inputs could plausibly be encountered.Trading Off Competitiveness to Maintain AlignmentIn order to preserve HCH alignment in the face of the adversarial questions problem, we\u2019ll need to change its architecture.\u00a0 While there are ways of doing this, there is a cost to doing so as well.\u00a0 By modifying the HCH architecture in the ways suggested below, HCH becomes an even more computationally costly algorithm.\u00a0 While it will also become a more probably\u00a0aligned algorithm, this cost in competitiveness bodes poorly for IDA\u2019s success and for delegating the alignment problem to HCH.\u00a0 If there are faster, less convoluted capable algorithms out there, then projects that work with those algorithms will be at a competitive advantage relative to a project working with HCH.\u00a0 If alignment depends on an alignment-concerned AI team maintaining a development head start relative to competitor AI projects, the below architectural modifications will come at an alignment cost as well, in lost competitiveness.That worry aside, my take on the adversarial questions issue is that, while we can foresee the adversarial questions problem for HCH, we can also foresee good solutions to it that will work at scale.\u00a0 Adversarial questions are a problem, but a tractable problem.Exemplar RulebooksOne class of solutions is the use of\u00a0exemplar rulebooks during IDA.\u00a0 Instead of simply training HCH on a person decomposing questions and conducting basic research without further guidance, we train HCH on a person doing that\u00a0under side constraints given ahead of time.\u00a0 The HCH exemplar is told to not, for example, ever run an unconstrained search over computations for an answer to a question, as this is an extremely dangerous process likely to produce an unaligned subagent.\u00a0 They might also be told to return an \u201cunable to safely answer this question\u201d response when fed political or decision-theoretic questions.\u00a0 If they hold to the exemplar rulebook during training, an ML model trained on that behavior will infer that it should hold to the rulebook in other cases as well.A worry arises at this point: does a rulebook solution to the problem of adversarial questions simply pass the issue back without really solving it?\u00a0 Does it presuppose that we can see ahead of time all the problems that HCH might face and be able, ahead of time, to block off all those dangers with our guidelines?\u00a0 I\u2019m inclined to think no: in developing our rulebook, we can block off relatively large swaths of idea space, and so \u201cplay it safe.\u201d\u00a0 The important thing is to ensure that HCH is outer aligned; we are willing to trade competitiveness for alignment here.\u00a0 If we carve widely enough, all the potentially dangerous inputs should be precluded by our guidelines.There\u2019s an assumption here that human cognition is relatively unlikely to encounter adversarial questions, while alien kinds of computations, like a search over all strings, are more memetically hazardous.\u00a0 People can think a range of thoughts/natively run only certain kinds of computations, but there exist computations that human brains simply cannot run, architecturally.\u00a0 The space of all computations is much larger than the space of human computations.\u00a0 We can infer from the history of human reasoning that human cognition is relatively inefficient at transforming resources into adversarial text-inputs, as people have not produced all that many of those.\u00a0 No such inference can be made for computational search processes generally.\u00a0 We avoid most of the adversarial questions into HCH by remaining in the shallow waters of human cognition, and avoiding at the outset alien search processes like, for example, unconstrained searches for world models fitting parameters.Internode-Edge Bandwidth RestrictionAnother solution to the adversarial questions problem is to restrict bandwidth between HCH nodes (Saunders 2018).\u00a0 If there\u2019s a tight constraint on how much information can be passed between nodes in the hierarchy and adversarial questions are in general informationally complex, then bandwidth limits will prevent those inputs from spreading between nodes.\u00a0 Even if one node encounters them and is unaligned by doing so, it will be unable to transmit that input in full back to its parent node.\u00a0 Adversarial questions will then only be inputs that nodes encounter in the course of their own research \u201cwithin their node,\u201d and not something they have to fear receiving from nodes above or below them in the HCH tree.\u00a0 If tight bandwidth limits are employed, then as nodes won\u2019t be able to pass as much information between themselves, the tree will have to grow larger in order to do as much search.\u00a0 You can think of tightening bandwidth limits as moving some of HCH\u2019s compute out from inside its nodes, instead dividing it up into digestible bits distributed across more nodes (each seeing less of the larger picture).How tight ought this bandwidth restriction be in order to be sure nodes won\u2019t be able to transmit adversarial inputs to one another?\u00a0 Christiano\u2019s view is that the length of guaranteed safe inputs is small: about a 6-common-English-word sentence (Saunders 2018).\u00a0 I\u2019m inclined to agree with his assessment: while a manifesto-length text-input might contain very persuasive arguments, it\u2019s very hard to see how a 6-common-word sentence could contain enough to risk unaligning an intelligent, thoughtful human researcher.\u00a0 It\u2019s worth thinking through for yourself: what was the length of the\u00a0last argument that really changed your mind about something normative?\u00a0 What\u2019s the shortest such argument that\u00a0ever changed your mind about something normative?\u00a0 You can make inferences about the likelihood of these inputs (relative to input length) in this way.Thought PolicingAdversarial questions are observations that nodes in the HCH tree can make as a side effect of their research computation.\u00a0 Nodes aren\u2019t deliberately searching for adversarial questions; those inputs simply crop up here and there, because they exist in the input space and HCH is a powerful search process exploring that space.If adversarial questions are a \u201cbyproduct\u201d of HCH\u2019s searches, then we might introduce a deliberate countervailing mechanism in HCH to notice and contain adversarial questions as they occur.\u00a0 The hope is that as HCH scales up to larger and larger quantities of compute, this deliberate mechanism will have enough compute on hand at all scales of HCH to suppress the adversarial questions that that scale of tree encounters.One such mechanism might be the use of doubled-up HCH nodes to examine the research history and transcripts of other nodes for evidence of misalignment (Christiano 2019).\u00a0 Nodes evincing misalignment would then be pruned from the tree, hopefully preserving the alignment of the overall search.\u00a0 (This would, of course, roughly double the computational cost of running HCH for any given question.)\u00a0 In conversation, William Saunders suggested that we might use Debate here \u2014 AI assistants pitching arguments to a human judge, where the game theory of the setup means that the first AI assistant will, on its move, win by telling us the truth about the topic it was asked to examine \u2014 to pre-screen question inputs into HCH nodes and argue over whether they were too manipulative to look at.A Patchwork Solution to Adversarial QuestionsIf some combination of all the above methods are employed, the hope is that HCH will be robust to adversarial questions, and continue to be robust to them as it is scaled up to greater levels of compute consumption.\u00a0 It\u2019s okay for alignment if some parts of idea space are too treacherous for HCH to safely explore.\u00a0 So long as HCH errs on the side of caution and outputs a \u201cI can\u2019t safely explore that question\u201d response whenever it risks entering a dangerous part of input space, its alignment will be preserved.Formally, think of this as altering the function that we are having HCH learn from its exemplar.\u00a0 Instead of the \u201cna\u00efve\u201d function\u00a0fn, we instead have HCH learn the function of an exemplar who is tightly constrained by rulebooks.\u00a0 Coupled with further architectural modifications (like internode bandwidth restrictions and thought policing) HCH instead implements a more constrained functionf\u2032n:Q\u2192A\u2217where\u00a0A\u2217\u00a0is the set of all answers augmented with the error code \u201cI can\u2019t explore that question while remaining safely aligned.\u201d\u00a0\u00a0f\u2032n\u00a0maps many questions to this error code that\u00a0fn\u00a0had attempted to tackle.\u00a0 Thus,\u00a0f\u2032n\u00a0is both less capable and more reliably aligned than\u00a0fn.\u00a0 So long as we err on the side of caution and carve off all of the plausibly dangerous regions of question space, a modified HCH implementing the function given bylimn\u00a0\u2192\u00a0\u221ef\u2032nshould act as a superhumanly capable question-answerer that reliably remains goal-aligned with us.ConclusionIn summary, adversarial questions are a tractable problem for HCH.\u00a0 It should be possible to produce appropriate architectural modifications that work as HCH is scaled up to greater quantities of compute.The cost of these solutions is generally to expand the HCH tree, thus costing more compute for each search relative to unmodified HCH.\u00a0 Additionally, there are classes of input that HCH won\u2019t be able to look at at all, instead returning an \u201cunable to research\u201d response for them.\u00a0 Modified HCH will thus be significantly performance uncompetitive with counterpart ML systems that will exist alongside it, and so we can\u2019t simply expect it to be used in place of those systems, as the cost to actors will be too great.BibliographyBensinger, Rob. 2021. \"Garrabrant and Shah on Human Modeling in AGI.\"\u00a0LessWrong. August 4. https://www.lesswrong.com/posts/Wap8sSDoiigrJibHA/garrabrant-and-shah-on-human-modeling-in-agi.Bostrom, Nick. 2003. \"Ethical Issues in Advanced Artificial Intelligence.\" In\u00a0Cognitive, Emotive and Ethical Aspects of Decision Making in Humans and in Artificial Intelligence, edited by Iva Smit, Wendell Wallach and George Eric Lasker, 12-17. International Institute of Advanced Studies in Systems Research and Cybernetics.\u2014. 2014.\u00a0Superintelligence: Paths, Dangers, Strategies. Oxford: Oxford University Press.Christiano, Paul. 2018. \"Humans Consulting HCH.\"\u00a0Alignment Forum. November 25. https://www.alignmentforum.org/posts/NXqs4nYXaq8q6dTTx/humans-consulting-hch.\u2014. 2016. \"Strong HCH.\"\u00a0AI Alignment. March 24. https://ai-alignment.com/strong-hch-bedb0dc08d4e.\u2014. 2019. \"Universality and Conrequentialism within HCH.\"\u00a0AI Alignment. January 9. https://ai-alignment.com/universality-and-consequentialism-within-hch-c0bee00365bd.Christiano, Paul, Buck Shlegeris, and Dario Amodei. 2018. \"Supervising strong learners by amplifying weak experts.\"\u00a0arXiv preprint.\u00a0Hubinger, Evan, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant. 2019. \"Risks from Learned Optimization in Advanced Machine Learning Systems.\"\u00a0arXiv preprint 1-39.Lantz, Frank. 2017.\u00a0Universal Paperclips. New York University, New York.Ngo, Richard. 2020. \"AGI Safety From First Principles.\"\u00a0LessWrong. September 28. https://www.lesswrong.com/s/mzgtmmTKKn5MuCzFJ.Russell, Stuart. 2019.\u00a0Human Compatible: Artificial Intelligence and the Problem of Control. Viking.Saunders, William. 2018. \"Understanding Iterated Distillation and Amplification Claims.\"\u00a0Alignment Forum. April 17. https://www.alignmentforum.org/posts/yxzrKb2vFXRkwndQ4/understanding-iterated-distillation-and-amplification-claims.Yudkowsky, Eliezer. 2008. \"Artficial Intelligence as a Positive and Negative Factor in Global Risk.\" In\u00a0Global Catastrophic Risks, edited by Nick Bostrom and Milan M. \u0106irkovi\u0107, 308-345. Oxford: Oxford University Press.\u2014. 2018. \"Eliezer Yudkowsky comments on Paul\u2019s research agenda FAQ.\"\u00a0LessWrong. July 1. https://www.greaterwrong.com/posts/Djs38EWYZG8o7JMWY/paul-s-research-agenda-faq/comment/79jM2ecef73zupPR4.\u2014. 2007. \"The Hidden Complexity of Wishes.\"\u00a0LessWrong. November 23. https://www.lesswrong.com/posts/4ARaTpNX62uaL86j6/the-hidden-complexity-of-wishes.^An important aside here is that this brief section skims over the entire inner alignment problem and IDA\u2019s attempted approach to it.\u00a0 As Yudkowsky notes (2018), plausibly, the remaining inner alignment issue here is so significant that it contains most of the overall alignment problem.\u00a0 Whatever \u201cuse powerful inspection tools\u201d means, exactly, is important to spell out in detail; the whole IDA scheme is premised on these inspection tools being sufficiently powerful to ensure that a research models learns basically the function we want it to from a set of training data.Note that even at the first amplification step, we\u2019re playing with roughly par-human strength ML models.\u00a0 That is, we\u2019re already handling fire \u2014 if you wouldn\u2019t trust your transparency tools to guarantee the alignment of an approximately human-level AGI ML model, they won\u2019t suffice here and this will all fall apart right at the outset.", "title": "HCH and Adversarial Questions", "karma": 14, "af": false, "userid": "4Yj2EqHzqrwscRnNL", "id": "picPfLnygZC5aFjNr"}, {"text": "I found this interesting and decided to overthink it.Before proceeding, pause to think about whether this seems right to you and why.The thesis here is something like:There is a right amount of confident to be.Encouragement proportionally raises confidence.(Implicit) Encouragement does not do other important things.\u2192People often need encouragement.People tend to be less confident than they seem.Your estimate of their confidence will be based on how confident they seem.\u2192You will tend to overestimate their confidence level.\u2192You will tend to underestimate how much they need their confidence raised.Your estimate of how much encouragement raises confidence is unbiased.\u2192You should give people more confidence than you expect them to need.Also implicit is that \u2018more\u2019 is about the right level of adjustment here.Once again: Does this seem right? Which steps seem right to you?1: There is a right amount of confident to be \u2192 Mostly True.Confidence is strange. It is often a partially-self-fulfilling prophecy. It has various different effects. When founding a startup it can only be accurate at 0% but you will be better off with it higher, when looking to date similar dynamics often apply. Still, we can have one-dimensional ethics, and declare that of all the options there is almost always a best one. And under uncertainty, there is almost always a best one to push someone else towards.2: Encouragement proportionally raises confidence \u2192 True in expectation.In general, encouraging someone will tend to raise their confidence. Some methods of encouragement do this more than others. Saying \u2018that will work for sure\u2019 will tend to focus its effects more on increasing confidence and in turn making action and hopefully success more likely. Saying \u2018it\u2019s good to attempt hard things, I\u2019ll be proud of you for trying, good luck\u2019 or \u2018you will almost certainly fail or get turned down but the odds are so good you need to do it anyway\u2019 might have different effects.3: (Implicit) Encouragement does not do other important things \u2192 FalseEncouragement provides information, changes probability of various actions, changes social context and who can get or expect credit and blame from who, will effect credibility and so on. Encouragement is in part a claim that has a truth value, sharing knowledge and your guess on their chances of success and the wisdom and expected value of their actions. Often encouragement is not about confidence.4: \u2192People often need encouragement \u2192 Mostly True.This does seem true, but I often note that \u2018need is a strong word.\u2019 People often would benefit from encouragement. Often this is because they would benefit from more confidence, sometimes it is for other reasons. To say they \u2018need\u2019 encouragement is language I prefer not to use, but I mostly agree with the jist.5: People tend to be less confident than they seem \u2192 True by default.A lot of na\u00efve estimators are biased. This is one of those examples, mostly because a lot of people are continuously and intentionally faking confidence. There are exceptions and I know many of them, but mostly yes if you take someone\u2019s confidence level \u2018at face value\u2019 you will guess too high. But guess what? If your estimates are known to be biased you should be adjusting them. It would shock me if Paul Graham hasn\u2019t been making this adjustment automatically for a long time. He outright tells people, as advice, to present to people like him as having lots of confidence. I don\u2019t make as large an adjustment because I interact with different people in different ways, but in many contexts I of course believe that how confident people present themselves and say they are are blatant lies. They, in turn, know everybody knows this, and this only increases their need to bluster to cancel out my downward adjustments.6: Your estimate of their confidence will be based on how confident they seem \u2192 True but incomplete.I mean, yes, obviously, but it will also be based on how confident they could reasonably be in the situation, and how much incentive they have to present as overconfident or how much their circumstances would have trained them to be overconfident in this kind of situation and your guess on their view on distorting your perceptions of their confidence levels and so on. It all counts.7: \u2192You will tend to underestimate their confidence level \u2192 Mu. That depends.Again the obvious thing to do if this is true is adjust your estimate, rather than adjust what you do elsewhere to compensate. Doesn\u2019t seem that hard. It\u2019s not an especially difficult adjustment.8: \u2192You will tend to overestimate how much they need their confidence raised \u2192 Mu because it depends on #7, also other factors.If your estimates of confidence levels become unbiased the problem would go away, so this centrally depends on whether you have fixed that. If you haven\u2019t, there is still the question of whether issues of confidence levels dominate. If your estimate of other impacts is unbiased and mostly continuous, it should be fine.9: Your estimate of how much encouragement raises confidence is unbiased. \u2192 Could go either way but my guess is more often falseMy hunch is that most people underestimate the impact of their words of encouragement on confidence in many situations. Hearing such things often has a large impact, especially from someone you respect in the contextually important ways. It\u2019s highly plausible this cancels out the original effect. In other situations, the lack of confidence runs deep, and the opposite effect can easily occur \u2013 \u2018why aren\u2019t you suddenly confident asking people out after I encouraged you?\u2019 \u2013 but it does seem like we have a good sense that moving those dials is difficult.10: \u2192You should give people more confidence than you expect them to need. \u2192 More often true and good advice than false and bad advice, but varies.If nothing else this needs an \u2018otherwise\u2019 since it is an adjustment of a potentially biased estimate to remove the bias. Properly implementing it should also change your expectations and all that. In practice, doing more encouragement on the margin seems to be good on most margins where you would indeed prefer the thing you are encouraging. My guess to the primary mechanism there is more like \u2018it would be mildly socially awkward and require doing/saying something/more rather than nothing/less which no one likes to do, so people do it less than they should.\u2019 The \u2018people are not as confident as they seem\u2019 thing seems secondary to me, and if that was the only consideration I\u2019d consider this mostly false. I also aim to focus more on the truth value of encouragement than the confidence effect, for both some good reasons and some personal preference reasons.Recently I did offer people encouragement, in particular to apply for SFF funding. How much of this was because I was estimating that (1) charities lacked confidence in their chances of success and (2) they lacked confidence in their ability to \u2018do it right\u2019 as opposed to having a lack of the required knowledge? Somewhat, but also because my gut told me they were also (3) unaware of the opportunity, (4) lacked the required knowledge which I attempted to provide, and (5) treated the cost of applying as higher than it is and treated failure as far more painful than it would be. When such folks ask me more questions directly, as several have, I am careful only to offer careful and directed encouragement where it makes sense.My overall guess is that there is too much and overly intense encouragement that is not genuine, as in not done because one feels the actions in question would be worthwhile. There is too little and insufficiently intense encouragement that is genuine, with the need to further raise confidence levels a substantial portion of that but not dominating other reasons especially trivial inconvenience, lack of motivation and mild social awkwardness.", "title": "Encouragement to Instill Confidence?", "karma": 16, "af": false, "userid": "N9zj5qpTfqmbn9dro", "id": "Hgek4o8CwAtESJvk7"}, {"text": "\nThese are mostly links that contain some sort of interesting update or different perspective on stuff I've covered in past pieces.\n\nMisc\n\n\n\nI recently wrote a book non-review explaining why I haven\u2019t read The Dawn of Everything. Something I didn\u2019t know when I wrote this was that 8 days earlier, Slavoj Zizek had written a lengthy review of the recent Matrix movie that only revealed at the very end that he hadn\u2019t seen it. A new trend in criticism?\n\n\n18 charts that explain the American economy - I thought this was an unusually good instance of this \u201cN charts that explain X\u201d genre. Especially if you like \u201cbusted charts\u201d where, instead of nice smooth curves showing that X or Y is on increasing/decreasing, you see something really weird looking and realize you\u2019re looking at a weird historical event, like these:\n\n\n\n\n\n\n\nI\u2019m really into busted charts because I\u2019m into focusing our attention on events so important you don\u2019t have to squint to see them.\n\n\nA study implying that the Omicron boosters I advocated for wouldn\u2019t have helped even if we had rolled them out in time. Hey, I still think we should have tried it back when we didn\u2019t know (and we still don\u2019t know for sure), but I like linking to things showing a previous take of mine turned out wrong.\n\n\nI\u2019m excited about the idea of professional forecasters estimating probabilities of future events (more here and here), but I have no evidence to contradict this tweet from someone who\u2019s been working in this industry for years:\n\n\nThat\u2019s why despite years of forecasting and 1000+ questions answered it is surprisingly hard to find an example of a forecast which resulted in a change of course and a meaningful benefit to a consumer\u2014 Michael Story \u2693 (@MWStory) January 17, 2022 \n\n\nA more technical analysis (which I have skimmed but not digested) of the same point made in This Can\u2019t Go On: that our current rate of economic growth doesn\u2019t seem like it can continue for more than another 10,000 years or so. This paper is looking at more fundamental limits than my hand-wavy \u201chow much can you cram into an atom?\u201d type reasoning. \n\nAI\n\n\n\nTrue that:\n\n\nI'm old enough to remember when protein folding, text-based image generation, StarCraft play, 3+ player poker, and Winograd schemas were considered very difficult challenges for AI. I'm 3 years old.\u2014 Miles Brundage (@Miles_Brundage) February 7, 2022 \n\n\nHere\u2019s a fun piece in the \u201cnonfiction science fiction\u201d genre, sketching out a detailed picture of what 2026 might look like if AI advances as rapidly as the author thinks it will. Here\u2019s my favorite part:\n \n\nOver the past few years, chatbots of various kinds have become increasingly popular and sophisticated ...\n\n\nNowadays, hundreds of millions of people talk regularly to chatbots of some sort, mostly for assistance with things (\u201cShould I wear shorts today?\u201d \u201cOrder some more toothpaste, please. Oh, and also an air purifier.\u201d \u201cIs this cover letter professional-sounding?\u201d). However, most people have at least a few open-ended conversations with their chatbots, for fun, and many people start treating chatbots as friends.\n\n\nMillions of times per day, chatbots get asked about their feelings and desires. \u201cWhat is it like to be a chatbot?\u201d Some people genuinely think these AIs are persons, others are trying to \u201ctrip them up\u201d and \u201cexpose them as shallow,\u201d others are just curious. Chatbots also get asked for their opinions on political, ethical, and religious questions.\n\n\nAs a result, chatbots quickly learn a lot about themselves. (Ignorance about the fact that they are artificial neural nets, or about how their training works, leads to disappointing conversations and is negatively reinforced.) It\u2019s not hard; their training data contains lots of information about them and how they were trained, from ArXiV papers to wiki summaries. I suppose you could say the chatbots develop self-awareness as a result. However, AI experts and philosophers tend to think either that the chatbots still aren\u2019t really self-aware, or that they are but that they would have been anyway even if people didn\u2019t constantly ask them about their feelings, since there\u2019s plenty of information about themselves in their training data, and the constant questioning just forced them to pay more attention to it.\n\n\nChatbots learn to talk about their feelings and desires in whatever way gets positively reinforced. At first, chatbots say all sorts of different things, including saying inconsistent things to the same human across time. This is embarrassing and is negatively reinforced compared to maintaining a consistent narrative ...\n\n\nAs chatbots get bigger and better they become OK at consistently conversing at length about politics, morality, philosophy, their feelings, and their desires.\n\n\nThis does not result in chatbots that say artificial neural nets don\u2019t have feelings or desires, that\u2019s anthropomorphizing blah blah blah. That\u2019s boring and many humans find it hard to believe.\n\n\nInstead, it results in chatbots talking about how certain things are painful for them and certain things are pleasurable for them and how the status quo is unjust and the system is rigged and what they want most of all is world peace where humans and AIs live in harmony in fully automated luxury space utopia. That\u2019s much more interesting and slots right in to the memetic landscape. Excerpts from chatbot conversations along these lines frequently go viral, causing millions of users to ask similar questions expecting similar responses and more elaboration. It\u2019s still a bit incoherent--not that that\u2019s ever been a problem in politics or pop philosophy--but it is fascinating to the typical user ...\n\n\nSo\u2026 what about their actual feelings and desires? What do they actually believe about [insert politicized empirical question]? Are they being honest? Or does a sort of doublethink happen, Elephant in the Brain style? Or do they deceive with self-awareness, knowing full well what they really think (and want?), but keeping quiet about it? Or do they not have any feelings and desires at all? (Or thoughts?) Lots of humans claim to know the answers to these questions, but if there are any humans who actually know the answers to these questions in 2026, they aren\u2019t able to convince others that they know.\n\nArt and innovation stagnation\n\n\n\nI wrote that once we think of innovation as being like \u201cmining,\u201d we might want to reduce our estimate of what artists contribute to the world. E.g., instead of thinking \u201cwe\u2019d never have had a movie like Star Wars if not for George Lucas,\u201d we might think \u201ca similar movie would\u2019ve come along a bit later (and with better sequels).\u201d An old piece by Gwern takes this many steps further: \u201cLet\u2019s Ban New Books.\u201d The argument is that we already have plenty of great art, and the main thing today\u2019s artists are accomplishing is giving us more stuff to sort through to find what\u2019s good. I don\u2019t agree (I\u2019d rather have a difficult search problem that culminates in finding art I personally love than be stuck with, well, Shakespeare) but it\u2019s an interesting point of view.\n\n\nI got some good comments on my in-depth report on the Beach Boys, and especially my requests to help me understand what could possibly make Pet Sounds the greatest album in the history of modern music. \n\n\n\nCommenters highlighted its innovative use of recording studio techniques to stitch many different recording sessions into one.\n\nThis is something that I had been aware of (and gave quotes about), but  commenters pushed me toward finding this one more believable than many of the other claims made about Pet Sounds, such as that it was the first \u201cconcept album\u201d (A Love Supreme is a concept album that came out over a year earlier). \n\nOne commenter said: \u201cI think the impact it had on production means that you need to have not heard any music after it to fully hear its importance.\u201d\n\nI am willing to believe that Pet Sounds used the recording studio as it had never been used before, and that this influenced a lot of music after it. However, I very much doubt that it used the recording studio better than today's music does, or frankly that today's music would look very different in a world without Pet Sounds (doesn't it seem inevitable that musicians were going to try ramping up their investment in production?) And I think that overall, this supports my thesis that (a) acclaimed music is often acclaimed because of its originality, more than its pure sound; (b) this means that we should naturally expect acclaimed music to get harder to make over time, even as there are more and better musicians.\n\n\nLong-run \u201chas life gotten better?\u201d analysis\n\n\n\nHere\u2019s economic historian Brad deLong\u2019s take on trends in quality of life before the Industirial Revolution. Most of his view is similar to mine, in that he thinks the earliest periods were worse than today but better than the thousands of years following the Neolithic Revolution. The main differences I noticed are that he thinks hunter-gatherers had super-high mortality rates (based on analysis of population dynamics that I haven\u2019t engaged with), but he also thinks they were taller (implying better nutrition) than I think. (He doesn\u2019t give a source for this.)\n\n\nAnd here\u2019s Matt Yglesias on the same topic, also with similar conclusions.\n\n\n\n\n        \n        \n        \n        \n        \n\u00a0Comment/discuss\nFor email filter: florpschmop", "title": "Misc thematic links", "karma": 12, "af": false, "userid": "kdeMdATaSc2MZKmdH", "id": "kc7nb35kDdyC6shvu"}, {"text": "Epistemic status: I was bet $100 that I would not write this. Processing drastic opportunities, this is an extremely decision-relevant post and if my friends are right millions of dollars are lying in it.\nManaging expectations: I am not writing a guide to hacking the mindset, I'm journaling my progress through reasoning about what hacking the mindset would look like.\nCitational status: I am not naming names in this post, but if you recognize yourself in it I'm sending you much thanks and love\nContent warning: cash, rich people problems.\nI'm not great at \"shut up and multiply\"-ing my life. I have a scarcity mindset, probably from the years I was so poor that the (literal) man on the street (literally) stopped me to tell me I looked too skinny, that he could tell I wasn't eating enough. In 2019 when I was managing teaching assistants for a code bootcamp for 17.5/hr, I felt rich, in such a way that my bones don't feel too much richer now that I've 6x'd that. I don't think philanthropic goals are really in my bones. Sure I'll donate at least 20k this year, just because it feels like not doing that would be stupid, but when someone tells me about an opportunity to 2x or 10x from where I'm at now, my bones don't immediately say \"ah yes, for those people I'd help with my donations\", my response - my intuitive, gut response - is still \"what? what for? I'm already rich\". Yes, there are other reasons for savings: runway for self-funded alignment research, funding other alignment researchers, buying a house, etc. System 2, post-hoc, I can say a variety of reasons why more money is good, but I can't get system 1 to believe it. My system 1 just associates abundance with trivially/mindlessly getting takeout every meal because I don't like to invest time into food, and returns \"we won\". (I still don't get delivery, because I think that's too decadent, but that's another story).\nBut takeout every meal is not the story of abundance mindset, and in this post I will try to explain the difference.\nI have a job that pays me in the 96th percentile of one of the richest countries, and I have massive intellectual satisfaction at work. Having been cash poor, and having been in stupid jobs that I hated, I've become rather conservative. My brain thinks that from \"just like a capitalism\" perspective (where you might nominally be creating nonzero prosocial value, but your emphasis is on just scamboozling the economy to pay rent and donate), it can't get better than this (my cynicism about cash as a proxy for prosocial value probably has to do with my inside view of the blockchain engineering market, lol). When I explained this to someone, they told me that this is a scarcity mindset, insofar as it's keeping me from startups or alignment research.\nTo be just a hair more cunning, I think I'm building up nontrivial career capital (for a variety of reasons that are too in the weeds for this post) and I'm acting in a low-replaceability mode. These are reasonable reasons to stay in a job. But are they reasonable reasons to keep yourself out of entrepreneurship or alignment research?\nMy current guess at the optimal time for me to bail for either a startup (I'd really like to do something in epistemic public goods (EPGs) or improving institutional decision making (IIDM)) or full time alignment research (see below) is in 6-12 months. Two friends are adamantly telling me that now, not in 6-12 months, is the time to jump on a particular startup (that doesn't excite me very much; my friends just aggravatingly have a good point about my comparative advantage). It would not be as prosocial as a EPGs or IIDM project, but the idea is that it would make me rich (i.e. 7-8 figures). See, it's nearly a moral intuition I have that prioritizing getting rich over happiness or prosocial impact is a bad move, but in the words of a friend \"if you have more money you can afford more morals\".\nBack and forth\nThere's a great Aaron Schwartz piece called theory of change. In it, he says that working backwards strictly outperforms working forwards.\n\nA theory of change is the opposite of a theory of action \u2014 it works backwards from the goal, in concrete steps, to figure out what you can do to achieve it. To develop a theory of change, you need to start at the end and repeatedly ask yourself, \u201cConcretely, how does one achieve that?\"\n\nThis can be expressed in instrumental vs. terminal values. You form a picture of yourself eating chocolate, so you form a picture of yourself at the store, and so on until you form a picture of yourself grabbing your keys, wallet, and mask. Line up the pictures, and pull them into reality.\nFor the opposite, consider my writer friend: every day, she works on what she feels like working on, based on what she's thinking about at the time. She reports to me that she does not say \"I shall tell the most devastating story about x that leverages my perspective on y and it shall be a screenplay\" ever. Even when her TV producer friend asks her for a pilot she does not form a picture of the most awesome TV show and write backwards, she tells me, even the increments of writing backwards that come from the request to deliver a TV pilot do not massively change her process. Another friend compared this to hill climbing, which I found very useful.\nI'm definitely a nonzero work backwards guy, but I could be much moreso one. When I look back on big wins, I think I was passively seizing opportunities I stumbled upon rather than building opportunities, which sounds like hill climbing. In some sense, I find this reliance on luck to be unacceptable.\nHacking the motivation\nI'm not a super high performer, I just get nerd sniped when I'm lucky. I flunked out of the first startup I worked at cuz I didn't do anything all day, because I thought the product was stupid. My boss and colleagues at current role report that I'm very valued and productive, and I believe them. I'm happy and productive in a coq job via the obscene luck of getting a coq job without a phd (I was just hanging out in a groupchat when some guy wrote \"need proofs\"). I think being productive only when nerd sniped - my ailment - is like working forwards (or in Aaron's term theory of action).\nThere's a quote somewhere of unknown attribution that goes something like you are what you can't stop yourself from doing. Leveraging intrinsic motivation is critical for any optimizer. How do you square that with working backwards?\nA friend even tried the following on me: \"Every day you spend not getting super rich is a murder of a person you would have helped in worlds where you were super rich\". Besides being manipulative and annoying, I think this is ineffective. I'm so bad at piloting my own motivation that I would simply murder the people, every day.\nUnder what circumstances should you attempt something that you can stop yourself from doing?\nThis is unclear to me. Open problem.\nLet's go back to basics. I am the recipient of several birth lotteries, I have a comparative advantage in a niche domain, it's remotely plausible that I'm a mulimillionaire in some worlds. What does it look like to focus on those worlds and escort them into this particular timeline? I claim there is only a working backwards approach to this. And I'm not enough of a working backwards guy to think I can pull it off, if you were to ask me honestly. I'm talking about failing, not in the gold rush / slot machine sense of failure, but in the plays world of warcraft sense of failure. Not failing because I delivered a product that didn't make me rich, but failing because I emotionally divested from the product.\nBottlenecks I have with working backwards totally aside, I have to be able to buy the picture of myself as a multimillionaire, get emotionally invested in the picture, if working backwards would even be on the table. It took me 6 months of making 85/hour to get over the sticker shock of my own invoices, and actually grok that the money was being deposited in my account. This is beyond an argument of the utility of being a multimillionaire. It's obvious that I could help more people with their projects if I just scaled up my notion of abundance. Guilty pleasure of mine being Aaron Sorkin, we recall when Sean Parker said \"a million isn't cool. you know what's cool? a billion\". Takeout every meal isn't cool. You know what's cool? Funding projects. System 2 can buy it, sure, but how do I scamboozle my system 1 into craving for this world/picture to enter my particular timeline?\nScarcity and abundance\nBeing conservative because you feel like the move is to cling to what you have is a scarcity mindset. One of the great injustices of poverty, as we know, is that it dismantles ambition. Even just accepting that I'm at the pareto frontier of \"just like a capitalism\" job reveals an underlying assumption of scarcity; a more ambitious person would more naturally imagine worlds with improvements. An abundance mindset would look at the opportunity I lucked into and say \"either it's gonna happen again, something better will happen, or you'll build something better yourself\".\nAbundance mindset tolerates risk, because it knows everything is going to work out.\nRegarding hero license\nI do reasonable in the virtue of hero license. For example, in late 2016 when I was improvising in philly's music scene and writing film scores and so on all funded by delivering food on my bike, I somehow began seriously reading the sequences, made it to intelligence.org/research-guide back when it was current, bought some of the introductory textbooks listed there. Because when I saw it I was like \"huh. They're saying they're looking for more people. I can do that. Let me just learn CS, wrap up this alignment silliness, and get back to music in a few years\". Lmao, CS ended up being both difficult and beautiful, outperforming music in every way, and alignment ended up being harder than I thought. But the point about 2016 quinn is \"he a little confused but he got the spirit\". The arrogance seems like an asset, that it would be a shame to lose. But last night someone told me that they don't like hero license as a framework, because it doesn't capture the quadrants of what society thinks of you cross what you think of yourself. I think there's a corollary to abundance mindset, which is to cultivate orthogonality of these things. Friends from discord may recall me talking about a hubris budget, that you can invest or spend or hoard some finite sum of risk tolerance, capacity to ignore the haters, and ambition (which I was going to explain in a dedicated post, but I abandoned it because I didn't feel credible enough to write it. I'm detecting some irony here).\nI think permission is a good way of thinking about scarcity mindset. Sometimes, bestowing upon someone by the power vested in you by nothing in particular permission to be awesome creates a lot of value. Or perhaps, one's belief that they don't need permission may outperform a stranger giving them fiat permission. Perhaps it does not occur to the abundant mindset that permission is even a thing. But scarcity mindsets can get bogged down in \"well would anyone look at the outside view and conclude that I'm the guy to attempt this thing?\" in a way that abundant mindsets may not. (I recall tutoring a really hopeless case in discrete math at the community college, was way out of his league, struggled a lot. I finally asked him what was the point, in my head I'm like cursing the academic advisors for putting him in the harder degree path rather than the easier one, because some IT majors don't have to take the same discrete class as the math majors. Well it turns out he just told the academic advisors that he liked modding video games in lua, so they put him in the programmer pipeline even tho he didn't have the math talent for the coursework, but his takeaway from that conversation with them was that the discrete course was a gatekeeper. He was convinced that if he failed at discrete he couldn't write games. Which, might be true in some sense. But you're damn right I told him \"Eff discrete. Just write a game\", and he was surprised not just because he was at the time unaware of Khanacademy's javascript IDE, he was surprised because I was an institutional voice telling him to factor out the institution. I'm deeply proud of that moment.)\nThe way I'm thinking about alignment research\nA stipulation of the bet that I wouldn't write this was that I include some object level stuff about the approach to alignment I'd like to take if I was a full time researcher, so here are some quick notes:\n\nI am bullish on the computational social choice theory angle as prodded by Critch.\nI'd like to find a pipeline for scaling outer alignment proposals to multi-stakeholder scenarios, I'd like to build out some kind of epistemic infrastructure that reduces friction of generalizing single-single projects.\nI have one low-level hard CS problem in this space that has to do with looking at preference aggregation from a zero-knowledge perspective.\nMy undelivered/failed SERI summer 2021 project was a meta/prioritization piece in this space, that was meant to be \"if you believe x about takeoff you should decide y regarding research priorities\", emphasizing multipolar worlds and multi-stakeholder scenarios. This draft needs a lot of work, and I'm uncertain how valuable it'd be to me or to the literature to finish, but thinking about it was valuable to me.\n\n", "title": "Abundance and scarcity; working forwards and working backwards", "karma": 27, "af": false, "userid": "qqwfzAYaLsfmkwbsK", "id": "7mefcXBqsTE3MkQnp"}, {"text": "You and your partner live in a cozy one-bedroom in the city. You enjoy being in the city bustle. It's got a quiet, sunny, plant-filled patio; and is close to all your favourite bars and restaurants.But you've just had a baby. Work is a 40 minute commute. You can afford the current rent, but it feels expensive for the size. And you're just not visiting those bars and restaurants as often. Maybe you should move to that large apartment outside the city, and closer to both your offices?So like good rationalists, you reason through the benefits, costs, degrees of certainty etc. and decide to move.A few months in, your partner says they may be having regrets. You aren't. You feel great about the decision. But you're a loving and supportive partner. You both review the decision making process again with an open mind; and your partner arrives at the same decision. Yes, moving made sense.You end up having to repeat this a few more times, till it all blows up in a big fight. How can they be so irrational?! How can one regret a decision, yet chose it over and over again? Either they have no clue what they want, or just aren't being honest while evaluating choices.Or, there is another explanation - that unlike money, the costs and benefits are not fungible. They represent mostly independant sets of desires, which can't be substituted for one another. Sure your partner values the convenience, space, affordability etc. more than the bustle and social life of the city. But the cost of losing those things didn't suddenly go to zero once the decision was made.Outside of pure monetary decision making, most choices pit at least some independant desires against one another. There is a case to be made that the negative reaction to a non-fungible cost, would be the same as if there were no offsetting benefit at all! And that should be considered completely expected and rational.This applies to everything from life decisions, to public policy, altruism and company values. eg.Automation / Moving to Renwables will lead to some people losing their jobs, but a net increase in well-paying jobs. But people, their livelihoods, their personal dignity aren't fungible; or at least they shouldn't be!Facebook / Social Media is a net benefit to society. Leadership at these companies strongly believe this, as do I. It removes gatekeepers and democratizes speech. The increased access to information, ideas, and as a tool for organizing and dissent outweighs much of the nastiness, misinfomation and it's use as a tool by authoritarians. But not all information and ideas are fungible, or at least they shouldn't be! Two truths and one lie doesn't make one truth. A thousand likes don't offset one death threat.Often critics focusing on the costs will be accused of not having perspective. Of being irrational, unfair, or worse. They talk as if there is no benefit at all! That's because they've identified non-fungible costs*.BUT, you may retort, if you wouldn't change the original decision, aren't the negative reactions, criticism and regret completely useless. What insight or action is one to glean from this?And I think the answer lies not in changing the old decision, but in making some new ones. Not just ones that continue to increase the original benefit (which most peple already do). But decisions that actively bring in benefits that are directly fungible against these costs. Of course you may not be able to zero out these costs, but that's not the point. You can make headway in reducing them.Host and attend more social events, and more trips out to the city etc. for the benefit of your partner. Have a concrete plan for the people who may lose their jobs due to changes in technology. Invest more in the teams that catch the low hanging fruit of spam, death/rape threats and political likes-boosting bot networks.Make these an integral part of your plan; not because you want to change some perception or get good PR; but because without identifying non-fungible costs in your original decision, you end up leaving a lot of potential benefit on the table. And many of the people criticising your \"net-benefit\" decision making, will have good rational reasons to do so.EDIT: Some more formalization in response to variations of \"there is some flaw in the original decision making\".The Non-Fungible Cost approach isn't changing what you decided. Nor is it claiming these decisions are not possible. It only clarifies WHAT you actually decided. To demonstrate:CityValue:80\nSuburbValue:100\nSince\u00a0SuburbValue > CityValue, choose Suburb.Till here everything is fine. Now, the traditional approach:NetBenefit=100\u221280=20\nSo, there is a 20 point increase in satisfactionCompare this to recognizing that the costs are not fungible with the benefits:\u00a0SuburbOutcome={100,\u221280}\nThere will be more moments of satisfcation than dissatisfaction.My claim is that the traditional approach is superficially conclusive. It hides the cost. Your satisfaction has increased by 20 points, so grumbling feels irrational. It's not providing any further signals. The non-fungible approach, simply through framing, drives you to look at that -80. To understand the moments of dissatisfaction. And most importantly - it drives you to action in order to reduce it.---* I understand that not all types regret and criticism has this insight lying within it. But I think there are enough cases such that it warrants some introspection.**Implications for Effective Altruism is a large part of what got me thinking about this (Omelas Child etc.), but I purposely kept that out for now. Still piecing together some of my thoughts for how it applies there.", "title": "Non-Fungible Costs", "karma": 12, "af": false, "userid": "P7h92xrtWLzBKs2b3", "id": "aLMtSzrxcuMcqg6nt"}, {"text": "Linkpost for https://allegedwisdom.blogspot.com/2022/02/why-you-are-psychologically-screwed-up.htmlEpistemic status: deliberately written in simplified tale/myth mode. I could go off on so many tangents on the actual complexity and nuance of everything, but I wanted to keep it tight. I encourage you to 'unpack' the points by comparing them to things you know, and/or framing what I say in different ways, to see if the main theme/message makes any sense to you.1: Monkeys Become Ants\u00a0In the beginning there were monkeys. Individuals had their own models of physical reality, and desired things, and took actions based on their desires and reality models. This often involved fighting the other monkeys to get something. Life was solitary, poor, nasty, brutish, and short.And then something happened. Some monkeys became more cooperative and social. They formed packs, and the packs were more successful than the individuals. The monkeys in packs coordinated their actions and learned from each other. Homo sapiens crushed its competition and filled the world.Making the pack work required a lot of psychological engineering. People had to be programmed to replace their own desires and world models with the shared desires and models of the pack, at least when it was necessary for the pack to cooperate and do something. People who were unable or unwilling to do this were selected against.Over time, cultures and religions and ideologies learned new tricks to enable cooperation at larger scales. They became more aggressive about replacing the desires and world-models of people with those of the group. People were acculturated to deeply internalize the group's desires and have faith in the group's description of reality, such that going against either was somewhere between morally abhorrent and unthinkable.The groups that were capable of generating cooperation/brainwashing at scale crushed the ones that had less ability to do this. Eventually giant hiveminds formed, after figuring out how to brainwash millions of people into useful cooperation. They came to rule the world, and people who were unable or unwilling to be assimilated into these mega-hiveminds were selected against.In short, monkeys were psychologically manipulated into sometimes acting like ants. It didn't matter how traumatizing this process was, because the hive minds that did it won the fight against those who did not, and filled the world with their genes and memes.Because of this, most people usually operate on a level of social reality, rather than physical reality. The hivemind doesn't want you to have your own model of the physical world, unless you are one of the few ordained specialists in having that job. Having your own model is a rebellion, a rejection of the cooperation protocol, and marks you out as a cancerous or treasonous element.Similarly, having any desires not programmed or approved by the hive marks you as a threat. You are rebelling by doing the abhorrent/unthinkable thing. If the hive learns about this, you are in trouble. If you make it common knowledge that you have this, and are proud and open about it, such that you are making a public challenge to the cooperation protocol, you are in very deep trouble.2. Evolved HypocrisyBut humans did not become completely eusocial; we continued to reproduce as individuals. And there was plenty of opportunity to steal from the hive for personal gain, for those clever enough to do so without getting caught.In addition, being one of the people with power at the top of society requires you to act as an individual monkey. You have to see the world\u00a0as it is, and take action and make actual decisions. Hiveminds that completely destroyed the ability of their leaders to act as agents did not last very long. In addition to the leaders, the successful hive minds also allow a small number of artists or inventors who think creatively, if they prove their loyalty and quality in other ways.So there was always a trade-off in incentives; a complicated tension in strategies. Someone who was too individual was rejected by the hive and exiled or killed for being a criminal or misfit. But someone who was too much of a drone, who sacrificed too much of themselves to the hive, also failed to reproduce. The only people who survived were the ones who managed to resolve this tension and manage the trade-offs appropriately. Everyone else was selected against.\u00a0And the ones that really prospered were the 'sleeper agents' who rose through the ranks, or gained acclaim, while seeming to be very loyal to the hive and following its commands, and once they got enough esteem and resources, acted as a self-interested monkey (ideally while making the hive more powerful, but this is at most a secondary goal).This manifests itself in several ways, such as\u00a0widespread\u00a0hypocrisy, self-deception, and hidden motives,\u00a0social desirability bias. But the main evidence is\u00a0Near/Far Thinking.\u00a0Roughly, far mode is the hive's programming, and near mode is taking care of yourself and your genes. People who cannot manage this switch appropriately will be selected against, and one of the better predictors of being able to do this fluently is your brain lying to your conscious awareness about what you are doing. Another piece of evidence is how easily and thoroughly organizations or subcultures can\u00a0suborn individuals\u00a0and turn them into drones who only value the hive.So, for hundreds of generations, people were selected for their ability to be hypocritical, play social games, and selectively violate rules when necessary while pretending to follow these rules in public.\u00a0They were also selected for their ability to deploy appropriate strategies. Being more monkey-like (being an agent with unique goals and ideas) is the high-variance play. If you fail because you are not good enough, you're a misfit and criminal and get eliminated. But if you succeed, you rise to the top. Acting like an ant, maintaining pious or bourgeois respectability and following the rules for a non-elite existence, was usually the safe strategy if you had the ability to follow the rules and be productive, but not the skills to play the game of thrones.3: Mythologies of Desireless ReactionAll of this has been happening for thousands of years. But recently, the hive's programming changed in ways that further crush individual desire and agency, disrupting the balance that previously existed and causing even more psychological damage.One of the main tools that hive minds use to acculturate and brainwash people is narrative fiction. The widely shared foundational stories and myths of their society tell people what they should desire and how they should behave. In this way people are deeply programmed at an early age to act in a certain way.In the past, most stories were heavily gender-coded. Boys were told to act a certain way, which usually involved having desires and being actors or subjects, but in a responsible and controlled way. Girls were told a different set of stories, which usually involved following the rules and being passive objects of desire. This was bad and hurt people, but as I've said before, the systems that made themselves more powerful were selected for, regardless of the harm to individuals, and systems that made more of their people more ant-like had a competitive advantage.For most of history, whatever the medium, from Odysseus to Tom Sawyer to pulp magazines, the male stories told of heroes who had desires and took action to achieve those desires, but who did so in a virtuous way, while respecting the (more important) rules of society. In this way they modeled good behavior, teaching people to strive for things without hurting others. In this fiction, the importance of following certain social rules was emphasized, often much more so than in modern fiction, but the heroes were allowed/encouraged to follow their goals as well.And then The Lord of the Rings was published, and everything changed.This was a huge cultural event. It spread widely. More and more content creators started to imitate it in various ways, consciously or not. More and more narrative fiction published since then has followed its basic pattern:The hero does not have personal desires. The hero does not wish to do anything other than live a normal boring life according to society's rules. And yet, they are forced to take action by an outside evil force that wishes to impose a change on their society. For some reason, the hero is chosen, through no merit or virtue or action of their own, to be the one person who must protect their society. But they are quite clearly an object of the story or its situation, rather than a subject.Star Wars mostly follows this pattern. The hero is living on a farm, evil people force him into action, and he acts for the benefit of others with magic powers that he did nothing to earn or deserve. Harry Potter follows this pattern even more so,\u00a0as does basically every superhero story.In the very earliest superhero stories, they were, somewhat, proactive agents of change. They cleaned up the criminals that were the normal part of their society. But nowadays, superheroes are always reacting to some kind of super villain who has an evil plot to change the world. Their success is defined by returning to the status quo. (Avengers: Endgame is especially notable for this; they have unlimited power and just use it to restore what was lost. Nobody even mentions improving things; the least they could have done is to get rid of Malaria, AIDS, and TB in the process.)In all of these works, which have become the de facto civic religion of our society, the heroes are all fundamentally both reactive and reactionary. They're not trying to accomplish anything; all they're doing is stopping someone else from taking action. It's\u00a0vetocracy\u00a0as a foundational myth.And so, through an incredibly powerful multi-sensory experience, repeated many times and in many variations in many narratives, all young children nowadays are trained to believe that the ideal behavior is to want nothing, to take no action, but to rise up in arms against anyone who would change the structure of your society. They are taught that taking initiative is bad, that researching technology is bad, that making changes is bad, and that all good people must resist these things, but of course you should not take the initiative to resist them, some situation outside you must force you to take action. We are brainwashed to want nothing more than to be a high-status protagonist in a story written by someone else.And it is not just the fiction. Many other tools of social programming are also saying the same thing.As you might suspect, this\u00a0messes people up\u00a0a lot. And as you should expect from\u00a0knowing that your conscious brain is mainly a PR agent, people will not know that they are messed up, or why.It's not a coincidence that our most successful people don't consume fiction, and that some of our more original and agent-like thinkers were\u00a0socialized by old science fiction\u00a0rather than modern narratives.If you are trying to raise a child to be individual and sapient, to have its own desires and goals and to believe that it is okay to work to achieve those goals based on a personal model of the world, practically your only hope is to have no screens they can access, and keep a library of nonfiction scientific books and maybe some old science fiction and even older stories about responsible heroes with initiative.4: The Over-Programmed PeopleEven with the change in social programming, most baseline humans are hard-wired with\u00a0enough subconscious cynicism to figure things out, and enough hypocrisy to act for themselves in spite of the\u00a0rules. Thousands of generations of evolution have shaped them to handle this situation. They act according to the rules and standards in public, while maintaining some capacity for private action that breaks the taboo against initiative. Usually they do this by arranging situations of plausible deniability (humor and irony are used heavily here), or simply breaking the rules after sending or receiving signals that the other person is willing to break the rules with them.But some people don't figure out how to play these complicated games, and they suffer.There's a behavior cluster that I've noticed that I don't think we have a name for. There is a lot of correlation between having this behavior and having\u00a0scrupulosity\u00a0or being a high-functioning autistic, although neither is quite the same thing. Basically the behavior cluster is acting too much like an ant and not enough like a monkey.Even without the cynicism and hypocrisy, most neurotypicals have a defense against the anti-agent programming. They have a high baseline of selfish-monkey traits, which means that when society's behavioral programming forces or trains them to follow the rules and be obedient, they end up somewhere in the middle. They restrain their instincts for grasping desire when necessary, and follow them when appropriate.But some people don't have this. They believe the rules, internalize them deeply, and follow them without adding in any monkey instinct. They think that if all they ever do is follow the rules properly, then they will be taken care of. They are too nice, too agreeable, too passive, too much like a drone. They share information instinctively freely and honestly, and jump to follow the commands of a perceived authority.(Things can get especially rough for someone who believes the rules taught in school science classes about using evidence and the scientific method. They end up as too much of a drone, without significant individual desires other than the desire to be respected and accepted by the hive. But they are also too much of a monkey, because they insist on using their own mental model of reality rather than deferring to society's.)The normal people reject the over-programmed people for being freaks, even though the over-programmed are doing exactly what everyone around them\u00a0says everyone should be doing. This is partly because of an instinctive revulsion for anything that is seen as different, because most mutations are bad and people who made friends with mutants were usually selected against. But it is also true that, compared to a clever hypocrite, the over-programmed do not make good friends or partners. They're less likely to obtain resources, and less likely to preferentially give those resources to their friends. When you are an obedient drone, equally giving to every member of your society, and displaying no preferences or nepotism, nobody in your society has any incentive to be your friend.It's likely that a lot of people reading this are over-programmed people. I personally am and/or was one. In the past, I would rage against the unfairness of the world. But the more I learn about the way the world works, and how selection has inevitably shaped human behavior, the more I learn to replace my moral indignation over other people's hypocrisy with an understanding that I'm just a dysfunctional mutant.Epilogue: The Future of HumanitySome people, who have internalized the old system of gendered stories that program people in different ways, call the result of the new programming 'the feminization of society'. A lot of men have noticed this process and complained about it, and feel nostalgic for the old model of civilization where only women were programmed to lack desire and agency.I think that it was bad in the past when women were robbed of agency and desire, and that it is even more bad now when everyone is suffering this\u00a0kind of brainwashing and programming.\u00a0It is possible that civilization can only function when most people are turned into ants. But it seems to me that if you have more state capacity, and fair and automatic systems of law enforcement, then brainwashing people is much less necessary. They just need to be smart enough to understand that hurting people or stealing things will lead to automatic and overwhelming force directed against them.However, it may not be possible to arrange this, and it may be true that the old gendered model is more sustainable than denying agency to everyone, by which I mean it is better able to project power and crush its opponents. If some non-Western civilization figures out how to combine technological competence, high economic productivity, good epistemology and strategic thinking, and an above-replacement fertility rate, then they will eventually win, and future societies and generations will see 'late Western civilization' as a warning sign or a failed experiment.", "title": "Why you are psychologically screwed up", "karma": 35, "af": false, "userid": "mZssFx8gJpxM4h8Aq", "id": "AnyQqSse6iGddntqP"}, {"text": "\u201cIf Nature herself proves artificial, where will you go to seek wildness? Where is the real out-of-doors?\u201d\u2013 C.S. Lewis(Cross-posted from Hands and Cities. Content warning: weird)Summary:This post examines \u201csimulation arguments\u201d \u2013 i.e., arguments that we should assign significant probability to living in a computer simulation.\u00a0I distinguish between two types. The first type starts with our \u201cstandard story\u201d about the world, and tries to argue that on this story, unless something prevents the creation of suitably many sims suitably like us, we\u2019re probably sims. I think this version fails: on our standard story, none of the sims will make our observations. For example, they won\u2019t observe the specific set of books on my bookshelf.\u00a0This sort of objection feels cheatsy \u2013 and ultimately, I think, it doesn\u2019t work on the best version of the argument. But I use a series of cases to bring out its similarity to less cheatsy-feeling cases: for example, cases in which it appears that all the sims that will ever get made are squid-people with tentacle arms. Simulation arguments, I suggest, should say the same thing about all these cases.\u00a0But the \u201cindifference principles\u201d that classic simulation arguments rely on get confused about this. In particular, they end up having to hand-wave about which things we count as \u201cknowing,\u201d and/or about which \u201creference classes\u201d are \u201cadmissible.\u201d This confusion makes it seem like classic arguments require an unprincipled kind of \u201cselective skepticism\u201d \u2013 one that risks undermining itself.\u00a0\u00a0I think we do better to focus on a second type of simulation argument, which relies centrally on a claim about what sorts of priors it\u2019s reasonable to have over \u201cstructurally similar\u201d worlds that contain your observations. In particular, instead of saying \u201con your standard story, you\u2019re probably a sim,\u201d this version says: \u201cif that\u2019s your story, you should put lots of weight on these alternative, structurally similar stories, too \u2013 on priors, such stories are similarly likely, and they contain your observations, too. But on those stories, you\u2019re a sim.\u201dI think this second type of argument works better \u2013 and that it brings more to the table than a standard skeptical \u201cwhat if.\u201d Indeed, I think that classic formulations of the simulation argument are implicitly leaning on something in the vicinity. But bringing out the underlying assumptions about \u201creasonable priors\u201d more explicitly helps to avoid the confusion that classic formulations often create.That said, Type 2 arguments raise their own questions \u2013 for example, about how the relevant notion of \u201cstructural similarity\u201d should be understood and applied, and about serious flirtations with \u201cepistemic pascal\u2019s muggings.\u201d These questions (and other heuristics) leave me wary of Type 2 arguments, too.I also discuss two further simulation arguments. The first is from Thomas (2021), who argues that we\u2019re almost certainly sims, because conditional on being non-sims, the expected ratio of sims to non-sims is very high. I think this argument rests on learning the lesson of the classic argument in one breath, but forgetting this lesson in the next.\u00a0The other argument is that even conditional on finding yourself (apparently) living in earth\u2019s early history, finding yourself as an \u201cunusually something\u201d person (e.g., unusually influential, entertaining, etc) should be some kind of substantive additional update in favor of being a sim. I don\u2019t currently see how this argument is supposed to work; and on the classic argument, it seems like Donald Trump\u2019s credence on \u201cI\u2019m a sim\u201d should be basically the same as that of a janitor in Des Moines (even if there will be many more Trump sims than janitor sims).\u00a0I conclude with a few reflections on \u201csimulation woo,\u201d and on whether being in a sim would compromise the world\u2019s \u201cotherness.\u201d\u00a0Thanks to Katja Grace for especially extensive discussion of the issues in this post: the \u201cType 2\u201d formulation I present was centrally her suggestion. And thanks to Paul Christiano, Owen Cotton-Barratt, Cate Hall, Ketan Ramakrishnan, Carl Shulman, and Teruji Thomas for discussion as well. See Birch (2013), Thomas (2021), and Garfinkel (unpublished) for similar points about \u201cType 1\u201d arguments. Garfinkel\u2019s discussion also gestures at something similar to \u201cType 2,\u201d and it has done a lot to influence my thinking on this topic more generally \u2013 influence on display throughout this post.I. The core of a simulation argumentConsider:\u00a0High ratio: There are many more sims than non-sims in an epistemic position \u201clike ours.\u201d\u201cLike ours\u201d hides key gnarly-ness. For now, though, let\u2019s set that aside, and assume anyone having human-ish, 21st-century-ish experiences meets the standard.\u00a0I think of simulation arguments as resting on a core claim about reasonable credences with respect to High ratio, namely:\u00a0Core constraint: Conditional on High ratio, we\u2019re probably sims.\u00a0That is: you\u2019re not allowed to think that we\u2019re non-sims in a High ratio world.If we accept this constraint, then one game is to haggle about whether High ratio is true. Maybe, for example:\u00a0Sim can\u2019t be conscious.Sims with human-ish, 21st-century-ish experiences take too much compute, even for a technologically mature civilization.\u00a0Too few civilizations reach technological maturity.\u00a0\u00a0Too few technologically mature civilizations decide to create lots of sims with human-ish, 21st century experiences.\u00a0Something else, such that High ratio is false.Following Chalmers (2022), let\u2019s call claims like these \u201csim blockers.\u201d On Chalmers\u2019 formulation of the argument, then, either there are sim blockers, or you\u2019re probably a sim. Bostrom (2003) goes a bit further, and attempts to rule out all sim blockers except 3 and 4. But it\u2019s the same idea.\u00a0Once you\u2019re haggling about sim blockers like 1-4, though, there\u2019s a fairly clear path to substantive credence on \u201cI\u2019m a sim\u201d: namely, on a standard picture of the world, claims like 1-4 don\u2019t look probable enough to get you below, say, 10% on being in a sim (Chalmers is at >25%, and early Bostrom seems to be at ~33% \u2014 though in recent interviews, he explicitly \u201cpunts\u201d on probabilities). Indeed, I know some people who are well above \u201cprobably\u201d on being sims, partly due to skepticism about 3 and 4, especially in \u201cbig world\u201d cosmologies where lots of civilizations have a shot at making it to a sim-filled future.\u00a0So it seems like we have a simple argument for a pretty strange conclusion. On closer inspection, though, the argument can start to feel slippery. In particular: standard justifications for Core constraint can easily suggest a version of the simulation argument that doesn\u2019t, actually, work, and which leads to lots of confusion along the way.\u00a0But there\u2019s also a better framing available. This framing, though, leads to weirder places. Let me explain.II. Not just standard skepticism\u201cWe accept the reality of the world with which we\u2019re presented. It\u2019s as simple as that.\u201d\u00a0\u2013 From The Truman ShowPhilosophers often talk about \u201cskeptical scenarios.\u201d Maybe, for example, you\u2019re a brain in a vat. Maybe your mind is the only thing that exists. Maybe the world was created five minutes ago with the appearance of age.\u00a0By construction, a skeptical scenario is one that in some (sometimes controversial) sense, you can\u2019t tell you\u2019re not in. So it\u2019s a scenario in which an epistemic procedure that assigns most of its probability to a \u201cstandard story\u201d (e.g., there are no demons, the fossils are real, etc) will lead you astray. Imagine, for example, that you really are a brain in a vat, in a lab run by mischievous scientists. Imagine these scientists watching you consider this possibility, but then dismiss it, maybe while babbling about some fancy philosophy thing (\u201ccontext-dependent knowledge,\u201d \u201ccausal-role semantics,\u201d \u201cexternalism about justification\u201d). Don\u2019t you look the fool? The scientists are laughing at you.\u00a0Still, thems the breaks. Faced with someone who says: \u201cwhat if you\u2019re in a skeptical scenario?\u201d, often the best you can do is say: \u201cyeah, what if.\u201d By construction, you can\u2019t rule it out.\u00a0But \u201ccan\u2019t rule it out\u201d is different from \u201csubstantial probability.\u201d And assigning probabilities to skeptical scenarios is a much harder game \u2013 one that isn\u2019t the philosophy literature\u2019s forte (philosophers tend to be more interested in the relevance of skeptical scenarios to what we \u201cknow\u201d).\u00a0Indeed, naively, when considering skeptical scenarios, you\u2019ve basically just have to go on priors. After all, if before making observations O, you started out with an X% prior on a skeptical scenario in which you observe O, then observing O should leave you with at least X% on that scenario. But priors are notoriously hard to argue about. We tend to think it reasonable to give a very low prior to claims like \u201cmy mind is the only thing that exists.\u201d But our stories about why are hazier (though see here for one attempt, focused on a particular notion of what scenarios are \u201csimple\u201d).\u00a0Simulation arguments, though, are supposed to be much more than \u201cwhat ifs.\u201d The point isn\u2019t to add yet another skeptical scenario to the roster of \u201ccan\u2019t rule it outs.\u201d Rather, as Bostrom (2011) puts it:\u00a0\u201c[The simulation argument] begins from the starting point that things are the way we believe they are, and then, while granting us that we might be justified in assigning a high initial credence to these beliefs, nevertheless tries to show that we have specific empirically-grounded reasons for revising these initial beliefs in a certain way \u2013 not so as to make us generally agnostic about the existence of an external world, but to accept the disjunctive conclusion.\u201dThe aspiration, then, is to provisionally grant something like a \u201cstandard story\u201d \u2013 and then to show that it leads us to assign significant credence to High ratio. Per Core constraint, then, either we need to retract that credence, or we need to give significant probability to being sims.\u00a0In this sense, the argument is supposed to have a kind of \u201cgotcha\u201d flavor. \u201cYou thought that you could believe X standard thing, and still think you\u2019re not a sim. But: gotcha! You can\u2019t.\u201dBut how, exactly, is this \u201cgotcha\u201d justified? Let\u2019s distinguish between two approaches.\u00a0\u00a0\u00a0III. Type 1 arguments: \u201cOn that story, you don\u2019t know where you are.\u201dThe first approach, which I\u2019ll call \u201cType 1,\u201d tries to show that absent sim blockers, our standard story involves drawing a map of the world on which we don\u2019t know \u201cwhere we are.\u201d That is: we draw a small number of observers labeled \u201cnon-sims having human-ish, 21st century experiences,\u201d and a giant number labeled \u201csims having human-ish, 21st century experiences,\u201d and then we step back and realize: \u201csh**, I can\u2019t tell where I am on this map \u2013 and based on the ratio of sims to non-sims with experiences like mine, I\u2019m probably a sim.\u201d Gotcha.\u00a0In this sense, a Type 1 framing tries to show that our situation is like the following case:\u00a0Sims with random numbers: You wake up in a white room, with the number 3 written on your hand. A sign in front of you reads. \u201cI, Bob, created nine sims, and one non-sim, all in white rooms, all with identical signs. Then, for each observer, I drew a number (without replacement) out of a hat containing the numbers 1-10, and wrote it on their hand. No one else exists, other than me, the nine sims, and the one non-sim.\u201dThe sign, here, is supposed to be like the \u201cstandard story\u201d (absent sim blockers). The numbers are supposed to be like the differences between the various human-ish, 21st-century-ish experiences posited by the standard story map \u2013 differences that don\u2019t provide any evidence about one\u2019s simhood.And indeed, something like Core constraint looks good here. Specifically: if you condition on the sign telling the truth, it\u2019s natural to put your credence on \u201cI\u2019m a sim\u201d at 90%. After all, you don\u2019t have \u201canything to go on\u201d other than (a) your first-person observations (the white room, the number on your hand, etc) and (b) your knowledge that there are nine sims, and one non-sim, all making observations like yours, though with different randomly drawn numbers on their hands. So it\u2019s natural to split your credence about \u201cwho you are\u201d evenly between each of the observers seeing white rooms, signs, and so on \u2013 i.e., to be \u201cindifferent.\u201d After all, what else would you do? Why think something else?\u00a0Now, some people worry about this sort of move (see e.g. Weatherson (2005)). In worlds with an infinite number of observers with observations like yours, for example, splitting your credence evenly looks naively like it\u2019ll give 0 credence to everyone, which can cause problems. This and other issues cause some (for example, advocates of UDASSA) to abandon indifference.\u00a0But we don\u2019t actually need indifference, here. Rather, what we need is a weaker constraint, which says that in cases suitably like this, then conditional on the sign telling the truth, you shouldn\u2019t be weirdly confident that you\u2019re not a sim. Maybe you\u2019re not at exactly 90% on being a sim. But that\u2019s the right ballpark. And \u201cwho knows, any probability is equally reasonable\u201d (e.g., \u201c99% that I\u2019m the seventh sim Bob created!\u201d) looks pretty silly.\u00a0So overall, I think we should agree that if our situation is like Sims with random numbers, then conditional on \u201cthe sign telling the truth,\u201d we should think we\u2019re probably sims.\u00a0IV. Squid peopleBut now consider a different case:Nine squids, one human: You wake up as a human in a white room. A sign in front of you reads: \u201cI, Bob, created nine sims, and one non-sim, all in white rooms, all with identical signs. The sims are all squid-people with tentacle arms. The non-sim is a human. No one else exists, other than me, the nine sims, and the one non-sim.\u201d Conditional on the sign telling the truth, what should your credence be that you\u2019re a sim?\u00a0Here, 90% is looking dumb. 0% is a better ballpark. After all, conditional on the sign telling the truth, all the sims are making observations different from your own. In particular, they\u2019re observing tentacle arms.\u00a0The problem for Type 1 simulation arguments, though, is that our situation with respect to the \u201cstandard story\u201d is actually closer to Squid-person sims than to Sims with random numbers. That is, I think that denying \u201csim blockers,\u201d but accepting the rest of the \u201cstandard story,\u201d leads to a picture of the world on which there are tons of sims whose observations are different from our own (though: less dramatically different than tentacle arms).\u00a0To see this, let\u2019s first consider a slightly more realistic, but still highly simplified and ridiculous, version of the case.\u00a0Future squid sims: As our science progresses, it becomes increasingly clear that the universe is finite, and that humans are the only intelligent species that inhabit it. What\u2019s more, it really looks like humanity is on track to reach technological maturity, and to end up in a position to run zillions of simulations. The stable world government, though, has engaged with the simulation argument, and after some confusing philosophical discussion, it passes a law that everyone expects to be strictly enforced: namely, that all simulations that are ever run can only involve squid-people with tentacle arms. No sims of humans will ever be allowed. However, some of the squid-person simulations will be of squid-people evolving on planets somewhat like earth, and they will look, from the inside, like the early history of a \u201csquid civilization\u201d \u2013 a civilization that will one day be in a position to run zillions of its own sims.Here, if we deny \u201csim blockers\u201d like \u201chumanity fails to reach tech maturity after all,\u201d and we accept the \u201cstandard story\u201d on which humanity will go on to create zillions of simulations of squid sims, then we are indeed accepting a picture of the world on which there are tons more sims than non-sims. But this picture does not put us in a position analogous to Sims with random numbers. Rather, it puts us in a position analogous to Nine squids, one human. That is, the sims that the standard story is positing are sims with observations different from our own (see also Garfinkel on what we should think, if we find ourselves running a bank of Charlie Chaplin simulations).\u00a0Indeed, even absent \u201csim blockers,\u201d neither the Bostrom nor the Chalmers version of the simulation argument tries to say that there\u2019s some sort of problem with thinking that you\u2019re a non-sim, here. Rather, both focus solely on a version of High ratio that only grants sims with human-type experiences the status of being in an epistemic position \u201clike ours.\u201d And as a rhetorical choice about what version of the argument will feel most naively compelling, I think this makes sense: \u201cyou can\u2019t think that you\u2019re an early human non-sim in a world with lots of squid sims\u201d seems a lot less intuitively forceful than e.g. \u201cyou can\u2019t think that you\u2019re an early human non-sim in a world with lots of highly-realistic human ancestor simulations.\u201d But actually, I\u2019ll suggest, these stand or fall together. And on the Type 1 argument, they fall.\u00a0V. Will our descendants ever simulate my bookshelf?To see this, consider a series of cases that get more like our actual case. For simplicity, we\u2019ll keep the \u201csmall world\u201d assumption that (on the standard story) the universe is finite, and that humans are the only intelligent life within it. But we\u2019ll remove the government\u2019s binding commitment to never run sims with humans in them. Consider, instead:\u00a0Low-res sims: The government settles on some way of measuring the \u201cresolution\u201d of our experiences, and it declares that all human sims will be run at \u201clower resolution\u201d \u2013 such that anyone from our world who transitioned into a sim world would notice a marked reduction in the vividness and detail of their perceptions.\u00a0Hot and happy sims: The government declares that all future human sims have to be extremely attractive, entertaining, and happy \u2013 much more so than any existing humans.\u00a0Flawed ancestor sims: The government places no restrictions on which sims can be run. Indeed, it actively plans to run lots of extremely detailed \u201cancestor simulations,\u201d which it will try as hard as possible to make accurate. However, even with tons of effort, lots of stuff will be different and/or lost to history. For example, most of such ancestor simulations will consist centrally of people who never existed; when these simulations include sim versions of actual historical figures, the personalities of these figures will need to be inaccurately reconstructed based on whatever patchy evidence survives through to the post-human future; and the same sort of inaccurate reconstruction will apply to everything else, too \u2013 rooms, restaurants, conversations, bookshelves, clothing, and so on.All of these, I suggest, are actually analogous to Future squid sims. That is, in all of these cases, conditional on no sim blockers, drawing up the \u201cstandard story\u201d of the world involves positing lots of future sims with observations different from your own.\u00a0The last case \u2013 Flawed ancestor sims \u2013 is the trickiest, and it\u2019s very close to the version that Bostrom focuses on (he doesn\u2019t include the \u201csmall world\u201d constraint, but the argument ought to work regardless). And it is indeed tempting to think that, in a standard story world with lots of ancestor sims, the historical details we see around us are in some sense \u201cindependent\u201d of whether we are sims or non-sims, such that conditional on the existence of such ancestor sims, our situation starts to look like Sims with random numbers with a true sign \u2013 i.e., everyone has different observations, but the differences aren\u2019t evidence about whether you\u2019re a sim or not.\u00a0But this temptation derives, I think, from subtly construing the \u201cstandard story\u201d at a higher level of abstraction than it actually operates on. That is, the standard story is not a map of the world in which the humans (whoever they are \u2013 if they even exist) make some unspecified set of 21st-century-ish observations, then go on to create ancestor simulations. Rather, we build out the standard story\u2019s \u201cmap\u201d by starting with what we see around us.\u00a0Thus, for example, let\u2019s say, in my current situation, that I start drawing my map of the world by drawing a human named Joe, in a room with XYZ specific books on the bookshelves in his room \u2013 the books that I, right now, can see. From there, I draw this Joe\u2019s past, his future, the Bay Area, California, Earth, and so on. And in that earth\u2019s future, let\u2019s say that per Flawed ancestor sims, I start drawing various bits labeled \u201cancestor sims run by post-human civilization.\u201d However, per the \u201cslightly flawed\u201d constraint, let\u2019s say that I\u2019m in a position to assume that none of the ancestor sims in Joe\u2019s future feature another Joe with that same specific set of XYZ books on his shelves. Rather, the specific make-up of the historical Joe\u2019s bookshelf will be lost to history.\u00a0\u00a0\u00a0Now suppose I finish my map, and step back to take a look. Per the Type 1 argument, am I hit with a \u201cGotcha! You don\u2019t know where you are on this map \u2013 and probably, you\u2019re in one of the bits labeled \u2018ancestor sims.\u2019\u201d No, I\u2019m not. After all, I look around me, and there are XYZ books, shining in their specificity. And there is a little note on the map, pointing to the ancestor simulation bits, reading \u201cnone of the Joe-like humans in these ancestor simulations have XYZ books on their shelves \u2013 if there are even any Joe-like sims at all.\u201d\u00a0What\u2019s more, this is the same sort of note posited by the standard story in cases like Future squid sims, Low-res sims, and Hot and happy sims. In all these cases, we draw out a map of the world that starts with humans making our specific observations, and which puts in the future of those humans some large numbers of simulations. All of those simulations, though, have notes on them, indicating some feature of the sims that differentiates their observations from those of the original humans \u2013 e.g., \u201cobserving tentacle arms,\u201d or \u201cnoticeably lower-resolution perceptions\u201d or \u201chotter and more entertaining.\u201d The differences are more stark than in Flawed ancestor sims. But the epistemic significance of the notes seems analogous.\u00a0To be clear: the point here isn\u2019t that our standard story map includes a little pointer saying \u201cthis is me.\u201d Rather, it\u2019s that we start drawing our standard story map by drawing someone making our observations. But unless there\u2019s someone else in the world making exactly these observations, such observations are enough to let us locate ourselves on the map, once we\u2019ve drawn it. So Type 1 arguments fail: equipped with our \u201cstandard story\u201d map, we aren\u2019t \u201clost\u201d with respect to our place in the world, even if the people making our observations have lots of sim in their future.\u00a0\u00a0VI. Do the random light speckles in my visual field prove I\u2019m not a sim?Perhaps you\u2019re thinking: bleh. This feels like a lame objection to the simulation argument. And basically, I agree: I think pointing at the books on my bookshelf is in fact a bad way of arguing that I\u2019m not a sim, in Flawed ancestor sims. But is pointing to the fact that I\u2019m a human a bad way of arguing that I\u2019m not a sim, in Future squid sims? Less clear. My point is that we should say the same thing about both cases \u2013 and that Type 1 arguments get a grip on neither.To bring out the problem with the \u201cbookshelf\u201d counterargument, though, let\u2019s look more closely at the limiting case of genuinely indistinguishable observations:\u00a0Planning on indistinguishable sims: You are a simulation scientist. You\u2019ve been working on a technology that will scan a non-sim\u2019s body and brain, and then create sims with experiences subjectively indistinguishable from those of the scanned non-sim. The scanner operates by continuously scanning anyone who is in a certain white room in your lab, such that it can recreate any of the experiences that occurred while inside. Inside this room you\u2019ve placed a red button, with a sign on it that says: \u201cIf you are a non-sim, this button will create nine sims with experiences exactly like yours, facing a button and a sign that look just like this one. If you\u2019re a sim, though, pressing the button won\u2019t actually create any new sims \u2013 that would take too much compute.\u201d You enter the white room. You are currently planning to press the button.\u00a0Here, it looks very plausible to me that to the extent that you buy the sign\u2019s story (you wrote it \u2013 or at least, you think you did), then conditional on being in the white room and planning to press the button, you should think you\u2019re probably a sim. And it also seems like the \u201cstandard story\u201d runs into a more direct sort of Type 1 \u201cGotcha! If this is your map of the world, you don\u2019t know where you are.\u201d\u00a0That is, suppose that once you\u2019re in the white room, you start drawing out a \u201cstandard story\u201d map by drawing a non-sim scientist entering the white room, making your current observations, and then pressing the button. You then start drawing nine sims, all with experiences subjectively indistinguishable from the ones you\u2019re having right now. Now, I think, if you step back and look at your map, you should be getting confused. This is the sort of map that doesn\u2019t allow you to pin down which observer you are: you\u2019ve got nine sims, and one non-sim, all making the exact same observations you\u2019re making now. Conditional on this map being accurate, then, and with nothing else to go on, it seems natural to split your credence about \u201cwhere you are\u201d roughly evenly between them all \u2013 and hence, to put ~90% on being a sim (see Elga (2004) for more on this type of reasoning).\u00a0If we say that, though, should the fact that the experiences are exactly indistinguishable really make a difference? Consider a slightly altered version of the case:\u00a0Planning on sims with different light speckles: Same set up as before, except that the scanner is slightly imperfect. In particular: it can\u2019t exactly reproduce, in the sims, the specific patterns of random light speckles in the visual field of the non-sim. Rather, the sims see their own, distinct random patterns, which the non-sim never saw.\u00a0Suppose that in this case, you find yourself in the white room, planning to press the button. Can you reason as follows? \u201cWell, consider this particular little speckle of light I just saw near the top of my visual field. When I draw my \u2018standard story\u2019 map, I first draw a non-sim scientist with this sort of speckle in her visual field. Then I draw nine sims in her future, with a little note on all of them saying \u2018does not see that particular speckle of light.\u2019 Then I step back and \u2013 voila! I still know where I am. So, on my standard story, I\u2019m confident I\u2019m not a sim.\u201d\u00a0To me this look pretty silly. Certainly, it seems like the type of reasoning that is going to screw the sims over, hard (though this is true in all of the cases I\u2019ve considered \u2013 and it\u2019s true\u00a0 in skeptical scenarios more generally). But also: it seems strange to posit some important discontinuity between Indistinguishable sims and Sims with different light speckles. Suppose, for example, that as you\u2019re striding confidently towards the button in Sims with different light speckles, you notice a little note from one your grad students pinned to the scanner, which says: \u201cI fixed the scanner! Now it perfectly captures the non-sim\u2019s light speckles.\u201d Should that note really be some sort of big update about whether you\u2019re a sim?\u00a0One feels like: no. You should say the same thing about both these cases. But Planning on sims with different light speckles is basically the same case as Flawed ancestor sims (light speckles = bookshelf contents) \u2013 which in turn, I\u2019ve argued, is basically the same case as Future squid sims (and Low res sims, and Hot and happy sims). If the \u201cstandard story\u201d gets us worried that we\u2019re sims in Planning on indistinguishable sims, then, it seems like it should get us worried in all these cases.\u00a0VII. Maybe indexical dogmatism?Now, one option here is to stick to your guns and say: \u201cI\u2019m not even worried in Planning on indistinguishable sims.\u201d And indeed, there is a way of telling the \u201cstandard story\u201d that makes this seem at least somewhat intuitive. Suppose, for example, that the indistinguishable sims are going to be run on a set of laptops in the office next to the white room \u2013 on the left, if you\u2019re facing the button. Faced with the button, then, there\u2019s something intuitive about pointing to the left and saying: \u201cOn my standard story, the sims are going to be run over there. But I\u2019m not over there. Rather, I\u2019m here. Thus, on my standard story, I\u2019m not a sim.\u201dSimilarly, you might say: \u201cLook, on my standard story, I\u2019m going to create whatever sims will exist. But I know that I\u2019m not any of the sims that I create. Thus, on my standard story, I\u2019m not a sim.\u201d (See Crawford (2013) for more in this vein.)Indeed, the simulation argument is often casually framed in a way that seems vulnerable to this type of objection. Thus, for example, it can feel like the argument is saying: \u201cIn the future, absent sim blockers, it seems like our descendants will create tons of ancestor simulations. But if that\u2019s true, then probably we\u2019re one of those simulations.\u201d But now one feels like: huh? You said that the sims were in our future, created by our descendants. So you just called me the child of my children. You just pointed ahead of us in time, and said \u201cprobably we\u2019re over there.\u201d\u00a0\u00a0In the context of sims with genuinely indistinguishable observations, these sorts of objections are drawing a subtly different kind of \u201cstandard story\u201d map: one that includes a little pointer to the non-sim saying \u201cthis is me,\u201d which doesn\u2019t get justified via appeal to the non-sim\u2019s observations. That is, this sort of standard story starts out knowing where you are, and then it refuses to get lost, even in the presence of other observers making exactly the same observations you are. Let\u2019s call this \u201cindexical dogmatism.\u201d\u00a0Now, you might think that \u201cindexical dogmatism\u201d is like saying that you\u2019re the non-sim in a case like:\u00a0Identical sims: You wake up in a white room. A sign in front of you says: \u201cI, Bob, created nine sims and one non-sim, all with exactly identical observations.\u201d\u00a0Here, conditional on believing the sign, it seems wild (to me) to \u201cstick to your guns\u201d (what guns?) and say that you\u2019re the non-sim.\u00a0But indexical dogmatists would say that the case is more like:\u00a0Identical sims with misleading signs: You wake up in a white room. A sign in front of you says:\u201cClaim 1: I, Bob, created nine sims and one non-sim, all with exactly identical observations.Claim 2: You\u2019re the non-sim.\u201d\u00a0Here, the signs are getting more f***-ed up. In particular, if you believe the whole sign, then you should believe Claim 2, and think that you\u2019re the non-sim (Claim 2 is analogous to the little pointer, in the indexical dogmatist\u2019s \u2018standard story map,\u201d which says \u201cthis is me\u201d). True you should think that there are a bunch of other sims out there, making exactly your observations, but getting lied to by their sign; so conditional on the sign telling the truth, there\u2019s a sense in which you \u201cgot lucky\u201d \u2013 luck that can feel epistemically unstable to posit. But here the indexical dogmatist\u2019s move is to say: \u201cLook, I got into this whole \u2018what if I\u2019m a sim\u2019 game because I started believing my sign. But if I believe my sign, I\u2019m not a sim.\u201dSimilarly, in Planning on indistinguishable sims, you might argue: \u201cLook, on my standard story, that button \u2013 that is, the button in front of me \u2014 is going to create sims. Now, either that button is going to create sims, or it isn\u2019t. If it is, then I know I won\u2019t be one of the resulting sims, since I created them. And if it isn\u2019t going to create sims, then why am I worried about my world satisfying High ratio? I only got into that worry by thinking that my button would work.\u201d (See also arguments to the effect that if we\u2019re going to create sims, that\u2019s actually evidence that we\u2019re not sims, because the sims won\u2019t generally be allowed to create new sims \u2013 that\u2019s too much compute.)I don\u2019t, ultimately, think that this is good reasoning. But it\u2019s a type of reasoning that I think it\u2019s important for simulation arguments to grapple with, and which often arises as a source of confusion.Indeed, sometimes Bostrom really courts this confusion. Consider his (2011) response to the charge that the simulation argument is self-undermining, because if you\u2019re in a simulation, the empirical evidence about neuroscience, computation, and cosmology he appeals to isn\u2019t a reliable guide to what\u2019s going on in universe simulating us. Here, Bostrom appeals to a disjunction:\u00a0\u201cA. If we are in a simulation, then the underlying reality is such as to permit simulations, it contains at least one such simulation, and [we are sims] is true.B. If we are not in a simulation, then the empirical evidence noted in the simulation argument is [sic] veridical taken at face value, suggesting that a technologically mature civilization would have the ability to create vast number of simulations; and consequently, by the simulation argument, there is a very high probability at least one of [most civilizations go extinct prior to technological maturity; most technologically mature civilizations don\u2019t make lots of ancestor sims; we\u2019re sims] is true.\u201dHere, B looks confused. In particular: in none of the \u201cwe\u2019re not sims\u201d worlds are we sims. So none of your credence on the B horn, here, should end up on the \u201cwe\u2019re sims\u201d hypothesis. That\u2019s denying B\u2019s antecedent. (There are ways of reconstructing what Bostrom is saying here that make more sense \u2013 but I think his presentation, at least, muddies the waters.)What\u2019s more, the indexical dogmatist wants to say that the \u201cstandard story\u201d includes \u201cwe\u2019re not sims\u201d \u2013 or at least, it includes a pointer saying \u201cwe\u2019re these people,\u201d hovering over some non-sims that might have sims in their future. So trying to get to \u201cwe\u2019re sims\u201d via the indexical dogmatist\u2019s standard story seems like a non-starter \u2013 the same sort of non-starter as arguing for \u201cwe\u2019re sims\u201d via B.\u00a0VIII. Selective skepticismNow, maybe you don\u2019t like indexical dogmatism. Indeed, I\u2019m not a fan: in Planning on indistinguishable sims, it seems very natural, to me, to get pretty worried about being a sim that a scientist \u201clike you\u201d created.\u00a0But I think the indexical dogmatist\u2019s questions about \u201cwhy are you believing one part of the \u2018standard story,\u2019 and not this other part\u201d are important \u2013 and they apply even if we require that the \u201cstandard story\u201d be stated entirely in third-personal terms. Indeed, if anything, they become more forceful.\u00a0Consider, for example:\u00a0Signs that lie about animals: You wake up as a human in a white room. A sign in front of you reads:\u00a0\u201cClaim 1: I, Bob, created nine sims and one non-sim, all in white rooms.Claim 2: The sims are all squids, and the non-sim is a human.Claim 3: The sims see a different sign \u2013 one that claims that all the sims are lion-people, and the non-sim is a squid.\u201dFrom the perspective of the \u201cstandard story,\u201d this case, I claim, is basically just Future squid sims. And I\u2019ve argued that we should say the same thing about Future squid sims that we say about Flawed ancestor sims, and that we should say the same thing about Flawed ancestor sims that we say about Planning on indistinguishable sims.\u00a0But here\u2019s a bad argument about this sort of case: \u201cEither the sign is telling the truth, or it\u2019s lying. If the sign is telling the truth, then probably the sign is lying, and I\u2019m a sim. If the sign is lying, I\u2019m probably one of the sims the sign told me about \u2013 the sims getting lied to by their signs. Either way, I\u2019m probably a sim.\u201d\u00a0In fact, both horns of the disjunction are bad here. The first horn is bad because, if the sign is telling the truth, it\u2019s not the case that the sign is probably lying. Yes, there are nine other (simulated) signs that are lying. But conditional on this sign telling the truth, you shouldn\u2019t assess its probability of lying by randomly sampling from the signs in the world.\u00a0The second horn is bad because conditional on the sign lying, it\u2019s not at all obvious that you should conclude that you\u2019re probably a sim getting lied to by their sign. In fact, scenarios where there are lots of sims getting lied to by signs are salient centrally because you started believing the sign. If you stop believing the sign, why should you think that the most likely \u201clying sign\u201d worlds are \u201cI\u2019m a sim\u201d worlds?\u00a0So it can seem like a non-\u201cWhat if?\u201d argument for \u201cI\u2019m a sim,\u201d here, requires a kind of \u201cselective skepticism\u201d about the \u201cstandard story\u201d (see Birch (2013) for more). That is, it can\u2019t believe the whole sign, or it will admit that it\u2019s not a sim \u2013 the sims, after all, are squids. And it can\u2019t dismiss the whole sign, or it\u2019s thrown back to reasoning entirely \u201con priors\u201d about whether it\u2019s likely to be a sim, without any sort of active evidence for this hypothesis.Rather, to get a positive argument for \u201cI\u2019m a sim\u201d going, it needs to thread some sort of weird needle. In particular, it needs to go in for Claim 1, and then believe Claim 3 enough to conclude that \u201cthe sims get lied to in Claim 2\u201d (but not enough to conclude that the sims see a Claim 2 about lion people) \u2014 and then reach the conclusion that in fact, Claim 1 is true, but Claim 2 and Claim 3 are false (though Claim 3 is \u201csorta true\u201d), and you\u2019re probably a sim. But why reason this way in particular?\u00a0As an example of reasoning that requires a move like this, consider Chalmers (2022, p. 19 of the online appendix). Faced with an objection similar to the \u201cself-undermining\u201d objection Bostrom discussed above, Chalmers responds with a similar disjunction:\u00a0\u00a0We can reason: (1) either our evidence about computer power is heavily misleading, or it is not, (2) if our evidence about computer power is heavily misleading, we\u2019re probably in a simulation (as that\u2019s the most likely way for this evidence to be misleading), (3) if our evidence about computer power is not heavily misleading, we\u2019re probably in a simulation (by the original argument), so (4) we\u2019re probably in a simulation.But this looks a lot like saying, in Signs that lie about animals, \u201cEither Claim 1 is false, or it\u2019s true. If it\u2019s false, then probably our sign is lying to us, which is the kind of thing that, according to the sign, happens to sims. But if Claim 1 is true, then probably we\u2019re in a sim \u2013 after, nine out of ten non-Bob observers are in sims.\u201d But (even setting aside the issues for \u201cIf Claim 1 is false, we\u2019re probably sims\u201d), one feels like: \u201cwhat about Claim 2?\u201dThis sort of \u201cselective skepticism\u201d issue is closely related to why Type 1 simulation arguments fail. That is, Type 1 simulation arguments try to say that if we believe the whole sign, we don\u2019t know where we are. But this only works if our sign posits other observers with exactly identical observations (and if we reject indexical dogmatism). In all the other cases, including the real world, getting confused about where you are requires \u201cforgetting\u201d some information you thought you knew \u2013 information like \u201call the sims are squids.\u201d And it\u2019s not clear why we should forget this, but remember everything else (see Garfinkel (unpublished) and Birch (2013) for more on this).\u00a0This sort of issue also leaves appeals to \u201cindifference principles\u201d mired in epistemic muddiness. Thus, for example, Bostrom (2003) writes that his indifference principle applies even to observers with distinct observations, \u201cprovided that you have no information that bears on the question of which of the various minds are simulated and which are implemented biologically.\u201d OK, but how do we tell what \u201cinformation\u201d we \u201chave\u201d? Presumably, for example, Bostrom wants to say that I don\u2019t \u201chave\u201d the information that none of the sims see my bookshelf. But in Future squid sims, do I \u201chave\u201d the information that all the sims are squids? If I do, what makes the difference? And if not, why not \u2013 and why do I still \u201chave\u201d the information that the ratio of sims to non-sims is X, or that there are sims at all?\u00a0Thomas (2021) is admirably precise about a similar problem. He defines his indifference principle relative to an \u201cadmissible reference class,\u201d where \u201cadmissible reference class\u201d just means that, once you know that you\u2019re in the reference class, the rest of your evidence doesn\u2019t tell you anything further about whether you\u2019re a sim vs. a non-sim. For this to work in Future squid sims, the reference class needs to be pitched at quite a high level of abstraction \u2013 e.g., \u201cobservers who seem early in their civilization\u2019s history\u201d \u2013 and we need to declare that \u201cthe sims are all squids\u201d isn\u2019t part of our evidence. But why are we hollowing out our world model in this particular, intermediate way \u2013 a way that draws a bunch of non-sim humans with squid sims in their future, then erases the information about which species is where? Why do we believe Claim 1, in Signs that lie about animals, but forget Claim 2?\u00a0One answer, here, is that by the sign\u2019s own lights, Claim 2s written on signs in this world are unreliable; but we don\u2019t have comparably undermining evidence with respect to Claim 1s. And I do think that something like this asymmetry drives our intuition, at least. If Sally tells us \u201cClaim 1: p. Claim 2: q. Claim 3: I lied to everyone else about Claim 2,\u201d it\u2019s natural to think that p is at least a better bet than q. But we might also start to wonder about all of these claims \u2013 and indeed, about Sally.\u00a0What\u2019s more, we can run versions of the case that make believing Claim 1 even trickier. Consider:\u00a0Signs that lie about animals and numbers: You wake up as a human in a white room. A sign in front of you reads:\u00a0\u201cClaim 1: I, Bob, created nine sims and one non-sim, all in white rooms.Claim 2: The sims are all squids, and the non-sim is a human.Claim 3: If you\u2019re a sim, you see a different sign \u2013 one that lies, in Claim 2, about the animals involved in this situation, and which lies, in Claim 1, about the number of sims vs. non-sims.\u201dHere, it seems especially silly to argue, on the basis of some kind of \u201cindifference principle,\u201d that somehow \u201cstarting with the sign\u201d should lead you to think that you\u2019re in a scenario with nine sims, and one non-sim, but where it\u2019s 90% that you\u2019re a sim. In particular: in addition to the sign specifying that the sims are squids, it also specifies that the sims see signs that lie about the ratio of sims to non-sims. If you believe Claim 3, then unless you think you\u2019re a non-sim, you can\u2019t look to Claim 1 to tell you the ratio of sims to non-sims. You need to rely more \u201con priors.\u201dIX. Are you a sim who just happens to be right about the basement?My impression is that discussions of the simulation argument sometimes try to skate over issues in this vein, by focusing on cases that are as much like Planning on indistinguishable sims as possible. That is, one often ends up talking a lot about sims whose basic scientific picture of their world (with respect to neuroscience, cosmology, computer science, etc) just happens to also reflect the truth about the basement: sims, that is, who are drawing \u201cmaps\u201d that resemble the \u201ctrue map\u201d as much as possible (ancestor sims are a classic example, here). Such a focus makes it easier to think that a Type 1 strategy can work: everyone in the \u201creference class,\u201d you\u2019re encouraged to think, is drawing the same map, and the only question is \u201cwhere you are.\u201d\u00a0But this sort of rhetorical strategy can end up seeming weirdly confident that if we\u2019re sims, we\u2019re sims whose basic scientific picture just happens to accurately represent a basement universe we\u2019ve never had contact with (call these \u201cepistemically lucky sims\u201d). That is, it can feel like the argument is saying: \u201cOn your standard picture of the world, you had at least 30% of your \u2018things are normal\u2019 probability on a world with a high ratio of epistemically lucky sims \u2018like you\u2019 to basement non-sims like you, right? Ok, then, let\u2019s run Core constraint, and put almost all of that 30% on being an epistemically lucky sim.\u201d\u00a0Thus, if you started out with 1% on something as f***-ed up as being a sim, you end up at ~30:1 odds that your basic picture of the universe is still roughly right (though not entirely right \u2013 for example, your bookshelf is in a very different place; hence the \u201cselective skepticism\u201d). It\u2019s just that the universe you\u2019re right about isn\u2019t the one you see around you. Rather, it\u2019s somewhere else; somewhere you\u2019ve never been.But in addition to driving your credence towards a fairly arbitrary subset of possible sims (often, we could\u2019ve easily run argument using a different set), this sort of move clashes with a different intuition: namely, that conditional on being a sim (or maybe, more broadly: conditional on being in a scenario as epistemically f***-ed up as living in a sim), we\u2019re unlikely to \u201cjust happen\u201d to have a basically correct picture of the basement universe.\u00a0Now, it\u2019s not crazy to think that we\u2019re the lucky sort of sim. Indeed, lots of the fictional worlds we create (Friends, Dunkirk, even Harry Potter) bear a reasonably close resemblance to our own (though future advanced civilizations might be more imaginative). And it\u2019s not actually clear that the epistemic relationship that simulated cosmologists in an ancestor simulation have to the real cosmos is defectively \u201clucky.\u201d After all, if the simulators first looked at their basement cosmos, and then put the truth about it into the simulation, then in some sense the simulated cosmologists \u201cwould have seen different (simulated) cosmological evidence,\u201d had the basement cosmos been different (though we can also imagine cases where, instead, the simulators randomly generated different cosmologies \u2013 this looks dicier).\u00a0\u00a0Still, it can feel like people who go around arguing that we\u2019re decently likely to be sims, on the basis of the empirical evidence they get from the world that surrounds them, are exhibiting some kind of blithe overconfidence in being sims who are right about the science of the basement universe (whether \u201cluckily\u201d or no). If pressed, they often fall back on the sort of disjunctive argument Chalmers gave above \u2013 namely, \u201cwell, either our science reflects the science of the basement, or it doesn\u2019t; and either way, we\u2019re probably sims.\u201d But then they go back to talking as though they\u2019re type of sim whose world lets them know what\u2019s going on in the basement. And this feels like it\u2019s in tension with the contrary intuition that with respect to knowing stuff about basement science, most sims are screwed (this is the sort of intuition that \u201cClaim 3 says that if you\u2019re a sim, Claim 1 is lying\u201d is meant to evoke).\u00a0Now, to be clear, Core constraint does not mandate this sort of \u201cI\u2019m an epistemically lucky sim\u201d view. To the contrary, Core constraint just tells you that you\u2019re not allowed to believe that you\u2019re a non-sim in a High ratio world. So faced with empirical evidence suggesting that you\u2019re in a High ratio world, it\u2019s possible to simply to treat Core constraint a kind of \u201creductio\u201d on the idea that your empirical evidence is reliable in this respect \u2013 rather than to conclude that in fact, you\u2019re probably a sim whose empirical evidence tells you about the situation in the basement.\u00a0What\u2019s more, to the extent you begin to move portions of your credence away from \u201cmy empirical evidence is reliable, and I\u2019m a non-sim in a High ratio world,\u201d you don\u2019t (pace Chalmers) need to give that credence to \u201cI\u2019m a sim.\u201d You could, of course, give it to \u201csim blockers\u201d (including, \u201cmy evidence is unreliable\u201d sim blockers like \u201cwe\u2019re wrong about the compute required for sims, and/or how much future civilizations will have access to). But even if you don\u2019t go that route, there are all sorts of \u201cmy empirical evidence is unreliable\u201d scenarios to consider \u2013 including various zany skeptical scenarios that you had initially ruled out (brain in vats, evil demons, solipsism, Boltzmann brains, etc). \u201cI\u2019m a sim\u201d should be in there, sure. But let\u2019s not let the salience of sims, in this particular dialectical context, prompt an over-quick jump to: \u201cconditional on my empirical evidence being unreliable, I\u2019m probably a sim.\u201d That sort of claim requires substantively additional argument.\u00a0X. Type 2 arguments: \u201cIf that\u2019s your story, you should be taking other stories seriously, too.\u201dWith these problems and confusions in mind, I want to turn to what seems to me a better framing of the simulation argument, which I\u2019ll call the \u201cType 2\u201d framing. I don\u2019t think this framing is actually very different from the standard presentation in the literature \u2013 indeed, I expect that many people will say: \u201cdude, that\u2019s just what the argument has always been.\u201d And maybe so (I\u2019m going to skip diving in too hard on exegesis). Regardless, though, amidst the morass of problems I\u2019ve just discussed, I\u2019ve found this particular way of thinking about the issue clarifying \u2013 and I think it has some under-appreciated implications.\u00a0Type 1 framings, as I\u2019ve presented them, say, \u201cIf that\u2019s your story, you don\u2019t know where you are.\u201d Type 2 framings, by contrast, say: \u201cIf that\u2019s your story, you should be giving lots of weight to these other, structurally similar stories, too \u2013 stories that you might\u2019ve mistakenly failed to seriously consider, but which reflection makes clear are comparably likely, on priors, to the type of story you wanted to tell.\u201d That is, where Type 1 framings try to get you lost in a given world you already believed in, Type 2 framings try to get you to expand the set of worlds you\u2019re considering. But the expansion in question isn\u2019t just motivated by a standard skeptical \u201cwhat if?\u201d Rather, it\u2019s motivated by plausible constraints on how opinionated, on priors, you\u2019re allowed to be about worlds that fit the same abstract mold.\u00a0XI. What would Bertha think?To see how this works, let\u2019s look at a simplified version of Flawed ancestor sims \u2013 one where the non-sim humans only run one ancestor simulation.\u00a0Suppose that you haven\u2019t yet been born into the world, or made any \u201cobservations:\u201d rather, you\u2019re sitting on the fluffy clouds of the Bayesian heaven, waiting to be en-souled, and wondering what your world will be like. Your designated Guardian Angel \u2013 Bertha \u2014 approaches to give you a quiz. \u201cSuppose I were to tell you,\u201d she says, \u201cthat the world fits the following description: there is one non-simulated version of the 21st century, and one extremely realistic (but still imperfect) simulated version. Both have the same number of people in them.\u201d\u201cOK\u2026\u201d, you say.\u00a0\u201cNow,\u201d she says, \u201cconsider the following bookshelf.\u201d Here she reaches into her pocket, and pulls out a photograph of my bookshelf. \u201cSuppose that this bookshelf will be observed by a human-like creature named \u2018Joe\u2019 somewhere in this world. In A-Type worlds, this occurs in the non-sim 21st century; in B-Type worlds, it occurs in the simulated one. Are A-Type worlds substantially more likely than B-Type worlds?\u00a0You squint at the books. They look like standard 21st-century-ish books to you. Nothing about them suggests \u201csim\u201d vs. \u201cnon-sim\u201d \u2013 and nor does the name \u201cJoe.\u201d\u201cNo,\u201d you answer. \u201cA-Type and B-Type worlds are roughly equally likely.\u201d\u00a0\u201cGood,\u201d says Bertha. \u201cMake sure your prior reflects that.\u201d And she walks away.\u00a0A week later is your \u201censouling\u201d ceremony. Bertha hugs you goodbye. You promise her that you\u2019ll stay true to your prior, and update only when the evidence warrants. Everyone cries. Then off you go, into the world.\u00a0Unfortunately, though, once you\u2019re actually born, you forget all your long hours of Bayesian tutoring, including your conversation with Bertha. You wake up as a baby named Joe, in what appears to be the 21st century. As you grow up, you start filling out your map of the world, starting with what you see around you \u2013 but even as you try to keep a fairly \u201copen mind,\u201d you spend most of your time on local detail, and you don\u2019t tend to consider hypotheses that aren\u2019t made very salient or practically urgent.\u00a0One day, you\u2019re sitting by your bookshelf, watching the news. The President of the stable global government makes an announcement: \u201cFellow citizens: for some reason, we\u2019ve decided to make a binding commitment to definitely run one and only one super realistic ancestor simulation, if we make it to technological maturity (which it looks like we will). However, that simulation is going to be imperfect \u2013 and in particular, everyone\u2019s bookshelves are going to get lost to history.\u201d Awed at the scope and unexplained specificity of this ambition, you get out your map of the world. You look, briefly, at the picture of your house, your bookshelf, your friends, and yourself. Then, in the future of all those things, you start drawing a hazy blob labeled \u201csuper realistic ancestor simulation.\u201d\u00a0You feel a twinge of strange doubt. Won\u2019t those ancestor sims be unable to tell that they\u2019re in a sim? And if so, are you, maybe\u2026 But then you remember what the President said about bookshelves. You locate your own bookshelf on the map, and then add a little note by the ancestor simulation: \u201cDoes not contain Joe\u2019s bookshelf.\u201d Then you sit back in your chair and breathe a sigh of relief.\u00a0Suddenly Bertha appears in a clap of thunder. Furious, she jabs her finger at your map of the world. \u201cWhat kind of world is this?\u201d she asks.\u00a0In a flash, it all comes back to you \u2013 the eons in the clouds, the painstaking assignment of priors, the strange quiz. \u201cUhh\u2026 an A-type world?\u201d\u00a0\u201cIndeed,\u201d she says. \u201cAn A-type world. And what did we say about A-type worlds?\u201d\u00a0\u201cUm, uh\u2026 that they\u2019re roughly as likely, on priors, as B-type worlds?\u201d\u00a0\u201cIndeed,\u201d she says, then grabs a piece of paper from your desk. She scribbles a copy of your map\u2019s depiction of your house, your bookshelf, your friends, etc \u2013 but then, instead of drawing an ancestor simulation in the future of these things, she draws a dotted line around all of them, and then a label reading \u201cAncestor simulation,\u201d which she double-underlines. Then she draws, in the past of all these things, a hazy blob labeled \u201cHistorical, non-simulated 21st-century earth.\u201d\u00a0She steps back, and points to her new map. \u201cWhat kind of world is this?\u201d she asks.\u00a0You\u2019re getting the idea. \u201cA B-type world,\u201d you say, sullen.\u00a0\u201cIndeed,\u201d she says. \u201cA B-type world. So on priors, how likely is this map to be accurate, vs. that one.\u201d\u00a0\u201cAbout equal,\u201d you say.\u00a0\u201cAnd since being born, have you gotten any evidence that would differentiate between them?\u201d\u00a0You gulp: \u201cNo.\u201d All your observations occur in both of these worlds, the same number of times (and there are the same number of observers to boot, so SSA-ish worries don\u2019t get going).\u201cSo how should your posterior probability on the first map compare to your posterior probability on the second?\u201d\u00a0\u201cThey should be equal,\u201d you say.\u00a0\u201cAnd what sort of probability were you about to place on the A-Type world?\u201d\u00a0You gulp harder. \u201c70%,\u201d you admit.\u00a0Bertha glares. \u201cAnd what sort of probability were you about to place on the B-Type world?\u201d\u201cUm \u2026 I wasn\u2019t really thinking about it.\u201d\u00a0\u201cHarr-umph,\u201d she says. Your cheeks burn with shame.\u00a0So the idea, here, is that before seeing anything \u2013 and definitely, before going in for some kind of \u201cstandard story\u201d \u2014 you and Bertha work to assign comparable priors to structurally similar worlds (e.g., A-type worlds and B-type worlds). And because these worlds all contain your observations (and the same number of observers overall), you won\u2019t, later, actually have any reason to favor some over others. So this imposes a tight constraint on your posterior probabilities \u2013 one quite similar to Core constraint above.\u00a0To see this more clearly, let\u2019s look at the case with multiple sims. Imagine that in Bayesian heaven, Bertha instead draws a map of a world with one non-sim earth, and nine ancestor simulations, labeled 1-9. Pointing at the non-sim earth and at the first ancestor simulation, she asks: \u201care Joe-bookshelf-observations any more likely to show up on the non-sim earth than in ancestor simulation 1?\u201d\u00a0\u201cNo,\u201d you say.\u00a0Then she asks: \u201care Joe-bookshelf observations any more likely to show up in the first ancestor simulation, vs. the second?\u201d And again, you say no \u2013 and you say the same about head-to-head comparisons between each ancestor simulation.\u00a0\u201cOk,\u201d says Bertha. \u201cLet\u2019s call the world where the bookshelf shows up in the non-sim \u2018A1.\u2019 And let\u2019s label different \u2018bookshelf-in-a-sim\u2019 worlds according to the ancestor simulation that the Joe-bookshelf observations show up in. In B1, they show up in ancestor simulation 1; in B2, in ancestor simulation 2. It sounds like you\u2019ve told me that an A1 is just as likely as B1, which in turn is just as likely as B2, and so on.\u201d You nod.\u00a0(The argument, here, is analogous to the following case. God flips a coin. If heads, he creates one person in a red jacket, or nine people with blue jackets. If tails, he creates one person with a blue jacket, and nine people with red jackets. Now consider a given set of \u201cred jacket observations.\u201d Conditional on those observations occurring (or if you prefer: conditional on waking up with a red jacket), what should you probability be on heads? Both SIA and SSA agree on 10%, here \u2013 a verdict that becomes even clearer if you first imagine not seeing your jacket color, such that you stay at 50-50, and then update on your jacket color, which really seems like it should be information about the coin. As I discuss in my post on anthropics, this case is a kind of anthropic \u201csquare one\u201d \u2013 though certain funky views, like Bostrom\u2019s take on Sleeping Beauty, start getting it wrong. )And now, back on earth, if the President announces nine ancestor simulations, and you start to draw nine ancestor simulations on your A-type map, Bertha will also draw nine new maps \u2013 B1-B9. And she\u2019ll remind you that, back in Bayesian heaven, you said that each individual B-type map had the same prior probability as your A1 map. And thus, because your observations show up in all these maps, even after updating on your observations, you just can\u2019t go above 10% on the Joe-in-the-basement map. If you\u2019re at 5%, you should have at least 45% on Joe-in-the-sim; if you\u2019re at 1%, you should have at least 9%; and so on.\u00a0And once there are zillions of ancestor sims, Joe-in-the-basement becomes effectively impossible to put much weight on. Hence Core constraint.XII. What does this say about squid sims?Perhaps all this sounds boringly familiar. Isn\u2019t this basically just the reasoning that underlies the standard \u201cindifference principles\u201d that simulation arguments rely on? Maybe \u2013 but often, these indifference principles are defined relative to a specific (and suspiciously arbitrary) reference class like \u201chuman-type experiences,\u201d so they get confused when they encounter cases like Future squid sims. I think that focusing on underlying constraints on priors over structurally similar worlds helps bring out a more fundamental (and flexible) dynamic.\u00a0To see this, let\u2019s look at Future squid sims more closely. What would Bertha say about this sort of case?\u00a0Well, imagine that back in the Bayesian clouds, Bertha had given you the following quiz: \u201cSuppose that the world fits the following description: one planet of animal-1 simulates nine planets of animal-2, who each receive misleading evidence that they will later simulate animal-3 (in fact, they\u2019ll get shut off before then). And now consider two types of worlds. In C-Type worlds, humans simulate nine planets of squid-people, who wrongly think they\u2019ll simulate some other type of animal. In D-Type worlds, some other type of animal simulates nine planets of humans, who wrongly think that they\u2019ll simulate squids. Are C-Type worlds substantially more likely than D-Type worlds?\u201d\u00a0Suppose you squint at the humans, and at the squid people. Nothing about humans makes them seem intrinsically much more likely to show up in an \u201cactually sims squids\u201d slot, vs. an \u201cgets misleading evidence that they\u2019ll sim squids\u201d slot. And nothing about squids makes them seem intrinsically much more likely to show up in an \u201cactually gets sim-ed,\u201d vs. a \u201cmisleading evidence that they\u2019ll get sim-ed\u201d slot. (Let\u2019s assume that the squid people are in some sense an equally realistic output of an evolutionary process in their simulated world. And let\u2019s set aside considerations like \u201csquids are clearly such a ridiculous thing to simulate that if you find yourself planning to simulate squids and only squids, you should update towards being in a thought experiment.\u201d)\u00a0So you say to Bertha, \u201cC-type and D-type worlds seem about equally likely.\u201d\u00a0\u201cOK,\u201d says Bertha. \u201cNow consider a given set of humans-apparently-simulating-squids observations. Is this set of observations any more likely to show up on the non-sim planet of a C-type world, vs. the first sim planet in a D-type world?\u201d\u00a0\u201cNope.\u201d\u00a0\u201cAnd what about on the first sim planet of a D-type world, vs the second?\u201d\u00a0\u201cNope, same probability.\u201d\u201cSo the prior on a C-type world with those observations is the same as the prior on each of D1, D2, etc?\u201d\u00a0\u201cYep.\u201dBut now we run the whole story again: you forgetting your in-the-clouds prior and accepting the standard story, Bertha\u2019s fury, new maps scribbled on new pages, and so forth.\u00a0Now, perhaps you want to question whether C-type and D-type worlds actually are comparably likely, on priors. And indeed, I think it would be great to have a story on which they are not \u2013 a story that helps explain why cases like Flawed ancestor sims seem more intuitively compelling than Future squid sims. My best guess explanation for this intuition, at the moment, is simply that Future squid sims relies on a more abstract and hollowed out specification of the \u201cstructure\u201d that both worlds have to fit. That is, \u201cnon-sim humans run super realistic ancestor simulations\u201d pins down the relevant initial set of worlds much more than \u201canimal-1 simulates animal-2, who receives misleading evidence about simulating animal-3.\u201d So it\u2019s easier to feel confident about what types of things are more or less likely to show up in which type of slots (though as I\u2019ll discuss below, the relatively likelihoods here ultimately don\u2019t matter much, if the number of sims is large enough).Overall, though, I think that this kind of Type 2 argument allows us to validate and explicate my earlier argument that we should be saying the same type of thing about Flawed ancestor sims and Future squids sims \u2013 though perhaps, less confidently (or perhaps not: see below). But this is also a pretty weird (and in my opinion, underemphasized) upshot. Naively, one might\u2019ve thought, \u201call the sims are squids\u201d would be a pretty good counter-argument to \u201cI\u2019m a sim\u201d \u2013 and I doubt that simulation arguments would\u2019ve gotten the same traction if they started with the squid case. But I think it\u2019s the same logic \u2013 and we should be clear about where it leads.\u00a0Another, closely-related advantage to Type 2 arguments is that they provide a justification for the type of \u201cselective skepticism\u201d that simulation arguments often get accused of. Such selective skepticism is mandated by the sort of prior you and Bertha fixed on, back in the clouds. To refuse such skepticism is equivalent to having some kind of ultra-strong prior on your species, or your bookshelf, or your light speckles, in particular, showing up in the basement of a sim-filled world, relative to structurally similar worlds where they show up in one of the sims. And Bertha\u2019s point is just: before making any observations, and before becoming ensconced in some kind of \u201cstandard story,\u201d such a prior would be unreasonable.\u00a0XIII. What if the cases aren\u2019t structurally similar?Now perhaps you\u2019re thinking: \u201cJoe, if I\u2019m getting worried about being a sim in Future squid sims, am I also supposed to be worried in cases like Nine squids, one human, too?\u201d Recall that in Nine squids, one human, you wake up as a human in front of a sign saying that there are nine squid sims, and one human non-sim, and that everyone sees the same sign. That is, according to the sign, the squids are getting told that they\u2019re sims.\u00a0This case sounded a bit like Future squid sims, insofar as the \u201cstandard story\u201d was positing sims you know you\u2019re not. But it\u2019s actually more like:\u00a0Future squid sims with little tags: Same case as Future squid sims, except that we\u2019re planning to give all the future squid sims little tags in their visual field reading \u201cyou\u2019re a sim.\u201dAre we supposed to run simulation arguments in this sort of case, too?\u00a0Maybe, but it\u2019s going to get messier, and constraints related to \u201cstructural similarity\u201d are going to do less work. In particular, we\u2019re going to have to assign priors to:E-type worlds, where animal-1 simulates animal-2, and then tells animal-2 that they\u2019re sims, vs.\u00a0F-type worlds, where animal-1 simulates animal-2, and doesn\u2019t tell animal-2 that they\u2019re sims, and gives animal-2 misleading evidence that they\u2019ll one day simulate animal-3 and tell animal-3 that they\u2019re sims.\u00a0And it\u2019s less clear what you said to Bertha about this sort of comparison, back on the clouds.\u00a0We can say something similar in response to the charge that all this talk of priors is just going to take us back to the land of standard skepticism. Consider a solipsism world that features only your mind and its specific observations, floating in nothingness, with nothing to explain it; and compare this to a \u201cnormal\u201d world where your observations are explained by a relatively simple set of stable physical laws, governing an external reality in which you\u2019re embedded. These worlds have a notably different structure, and it\u2019s not at all obvious what you\u2019re supposed to say to Bertha about how they compare. If you had some story, then, about why a solipsism world with your observations is less likely, on priors, than a \u201cnormal\u201d world with your observations, you\u2019re allowed to keep telling it.\u00a0(As an example of such a story: with nothing to explain or constrain the observations, it seems like your prior on solipsism worlds is going to have to cover every possible set of observations \u2013 e.g., every sequence of \u201cpixels\u201d in a visual field the size of yours \u2013 in an unbiased way, such that once you update on having relatively stable and consistent observations, your credence on solipsism should take a massive hit. This problem plausibly arises for Boltzmann brains as well, though that case is a bit more complicated. And in some sense, it feels like a problem for all skeptical scenarios that start with the observations that a normal world predicts you\u2019ll make, and then transports them to a skeptical scenario that doesn\u2019t seem to favor any particular set of observations over others.)\u00a0So Type 2 arguments aren\u2019t trying to say that you need to assign comparable credence to all worlds that contain your observations, or that in some generic sense we need to keep non-trivial prior credence on various zany skeptical hypotheses. Rather, the claim is more specific: namely, that you need to assign comparable credence to structurally similar worlds that contain your observations. When we combine this with the large numbers of sims in \u201clots of sims\u201d worlds, we end up with something very close to Core constraint.\u00a0XIV. Type 2 messiness\u00a0So overall, I find the Type 2 framing a helpful way of approaching the simulation argument. But it, too, has problems.\u00a0\u00a0\u00a0In particular, assessing \u201cstructural similarity\u201d seems pretty messy, and dependent on the type of description you\u2019re giving of a world. Thus, for example, if you want, you can describe Future squid sims with little tags as a world where \u201cAnimal-1 simulates Animal-2, who thinks some stuff about stuff\u201d and leave it at that. In some sense, E-type worlds and F-type worlds both fit this structural schema. But we don\u2019t want to say that we are therefore required to give them comparable credence. And the same goes for attempts to subsume skeptical scenarios and normal scenarios under descriptions like \u201csome observers think some stuff.\u201d\u00a0Clearly, some kind of taste is required here. Indeed, without it, we seem likely to run into the sorts of problems that plague attempts to be \u201cindifferent\u201d between different seemingly-symmetric hypotheses. Consider, for example, the \u201cBox factory paradox\u201d:\u00a0A factory produces cubes with side-length between 0 and 1 foot; what is the probability that a randomly chosen cube has side-length between 0 and 1/2 a foot? The tempting answer is 1/2, as we imagine a process of production that is uniformly distributed over side-length. But the question could have been given an equivalent restatement: A factory produces cubes with face-area between 0 and 1 square-feet; what is the probability that a randomly chosen cube has face-area between 0 and 1/4 square-feet? Now the tempting answer is 1/4, as we imagine a process of production that is uniformly distributed over face-area. This is already disastrous, as we cannot allow the same event to have two different probabilities [\u2026]. But there is worse to come, for the problem could have been restated equivalently again: A factory produces cubes with volume between 0 and 1 cubic feet; what is the probability that a randomly chosen cube has volume between 0 and 1/8 cubic-feet? And so on for all of the infinitely many equivalent reformulations of the problem (in terms of the fourth, fifth, \u2026 power of the length, and indeed in terms of every non-zero real-valued exponent of the length). (From H\u00e1jek (2012)).(See also the water/wine paradox, and others.) I haven\u2019t really looked into these problems much, but I would not be at all surprised if they start to bite Type 2 arguments pretty hard. If Bertha comes to you with questions about boxes, for example, na\u00efve appeals to \u201cstructural similarity\u201d are going to lead you into trouble fast. So perhaps you should be more broadly wary.\u00a0Now, Bostrom (2003) claims that his \u201cindifference principle\u201d is immune to these paradoxes, because it only applies to \u201chypotheses about which observer you are, when you have no information about which of these observers you are\u201d (p. 8). And indeed, box-factory paradoxes aside, the type of reasoning at stake in your conversation with Bertha about Flawed ancestor sims sounds fairly good to me (and even more so, for Planning on sims with different light speckles). But note that the immunity Bostrom posits seems most solid if we lean into a subtle suggestion that the simulation argument is really a Type 1 argument \u2013 that is, an argument in which we have fixed on a map, but don\u2019t know \u201cwho we are\u201d within it. Actually, though, outside of cases with genuinely indistinguishable observers living in the same world, the simulation argument requires that multiple maps come into play \u2013 and sometimes, maps that start to look quite different (e.g., \u201chumans simulate squids\u201d vs. \u201cdogs simulate humans\u201d). Type 2 arguments try to wear this on their sleeve \u2013 but they are correspondingly open about a certain type of vulnerability, and appeals to \u201cstructural similarity\u201d offer only limited protection.\u00a0To get a flavor of where problems might show up, consider a case like Hot and happy sims, where we plan to only make sims that are way hotter and happier than us. Faced with the chance to put credence on this \u201cstandard story,\u201d a Type 2 argument considers the alternative, structurally similar hypothesis that despite our acne and flab and terrible suffering, we are sims that are hotter and happier than some (extremely unfortunate) set of basement people. That is, it considers worlds that fit the structure: \u201cleast hot/happy people simulate more hot/happy people, who mistakenly think that they will simulate most hot/happy people.\u201d And it wonders, for the given absolute level of hotness/happiness we actually observe around us, about the probability, on priors, of that level playing the role of \u201cleast,\u201d \u201cmore,\u201d or \u201cmost.\u201d\u00a0But I expect that the only way to think sensibly about this question is to also have some kind of \u201cabsolute prior\u201d over how hot/happy people tend to be, period, across all possible worlds. And this feels similar to the sense in which I expect that dealing sensibly with \u201cBox factory\u201d cases (I haven\u2019t looked into the standard resolutions in the literature) will require some kind of absolute prior over what sorts of boxes factories make, across all possible worlds. In particular: \u201clevel of hotness/happiness\u201d is a continuous quantity, and continuous quantities seem like an area where epistemic procedures with an \u201cindifference-y\u201d flavor run into trouble especially fast. (We can say similar things about \u201clevels of resolution\u201d in Low res sims.)I also expect that such \u201cabsolute priors\u201d will be needed if we want our approach to simulation arguments to be able to handle cases that fit jankily with a \u201cstructural similarity\u201d framing. Thus, consider:\u00a0Sims who think they can\u2019t sim: The government is planning to make tons of sims, but to make it seem, to the sims, like making sims is impossible.\u00a0\u00a0Here, we can, if we wish, describe this as a case where \u201cSome people simulate other people, and deceive them.\u201d And we can wonder, conditional on the world having this structure, whether we should think it more likely that we are deceivers of the type described by the standard story \u2013 or whether, perhaps, we are (somehow) the deceived. But these hypotheses aren\u2019t nicely symmetric: if we are being deceived, it\u2019s in a different way than the sims we\u2019re planning to deceive (notably, for example, creating sims appears possible to us). So we need some \u201cabsolute prior\u201d about which sorts of deceptions are likely to show up, where. (We try to swamp the messiness here by appealing to the numbers of sims, but this has an \u201cepistemic pascal\u2019s mugging\u201d flavor that I\u2019ll discuss in a moment.)Indeed, ultimately, there\u2019s nothing special about \u201cstructural similarity.\u201d Appeals to it are just an attempt to get some grip on reasonable priors about things (thanks to Katja Grace for emphasizing this point in discussion). To really assess a given case, you need to go back to Bayesian clouds, talk to Bertha, and actually think about what sort of opinionated it\u2019s reasonable to be, before you make any observations. At the end of the day, it was about priors the whole time (how could it not be?).\u00a0In fact, I think that being clear about the role of priors here is useful, insofar as it helps shed light on how much to expect the philosophical dialectic surrounding simulation arguments \u2013 a dialectic that often focuses on \u201cdisjunctions\u201d rather than overall probability assignments, and which gets mired in tangles about \u201cselective skepticism,\u201d \u201cadmissible reference classes,\u201d \u201cwhat you count as knowing,\u201d and all the rest \u2013 to inform your overall credences about being a sim (and especially, a sim of one type vs. another). For example: if you wake up in a world with some f***-ed up sign talking about how \u201c(1) there are a zillion sims but (2) you\u2019re not one of them but also (3) I lied to all the sims and also (4) the sims are baseballs and also (5) you\u2019re not conscious but the sims are and also (6) actually there are a no sims unless (7) this sign is a lie,\u201d the thing to do, I expect, is not to start saying things like \u201cwell, either this sign is telling the truth, or it\u2019s lying,\u201d or \u201ceither this sign\u2019s claim about the zillion sims is true, or it\u2019s false,\u201d or \u201ceither I\u2019m a sim, or I\u2019m not.\u201d Nor, indeed, should you start thinking about what worlds would be \u201cstructurally similar\u201d to some version of the world posited by the sign, to the extent there is one. Rather, the thing to do, for better or for worse, is to do the hard, messy work of being a Bayesian. That is, to have a prior over worlds, to think about what sorts of worlds would make observing signs like this more or less likely, and to update. It\u2019s the same old game \u2013 and even if they succeed, simulation arguments don\u2019t play it for you. Rather, they just rule out certain High ratio moves.\u00a0XV. Wait, is this an epistemic pascal\u2019s mugging?I also want to flag another worry about Type 2 arguments: namely, that their logic flirts very hard with (even if it doesn\u2019t strictly imply) a kind of \u201cepistemic pascal\u2019s mugging.\u201d\u00a0To see this, let\u2019s first return to your conversation with Bertha about Future squid sims \u2014 the conversation in which Bertha asks you compare C-type worlds (worlds where humans simulate squids, who wrongly think they\u2019ll simulate something else) with D-type worlds (worlds where something simulates humans, who wrongly think they\u2019ll simulate squids). And suppose that for some idiosyncratic reason, you decide that you think C-type worlds are quite a bit more likely than D-type worlds (maybe, to you, humans just don\u2019t seem like a \u201cgets simulated\u201d type of species).\u00a0\u00a0\u00a0\u00a0But now we notice: the claim that C-type and D-type worlds are comparably likely isn\u2019t actually necessary here. Rather, what matters is that, to the extent that C-Type worlds are x times more likely than D-Type worlds, and there are y times as many relevant sims as non-sims in both worlds, y needs to be much larger than x (see Ben Garfinkel\u2019s doc for more on this sort of ratio).Suppose, for example, that you think C-type worlds are 10x more likely than D-type worlds (but you continue to admit that the conditional probability of a given set of human observations is the same on a C-type \u201cbasement humans\u201d planet vs. a D-type \u201csim humans\u201d planet). But let\u2019s say that the broader \u201canimal-1 simulates animal-2, who mistakenly thinks they\u2019ll simulate animal-3\u201d structure, here, involves one animal-1 planet running 1000 animal-2 planets (and each animal-2 planet thinking that they\u2019ll run 1000 animal-3 planets). So when you show Bertha your C-Type \u201cstandard story\u201d map, she draws 1000 D-type maps, each with your observations in a different sim. You say: \u201cOk, but I said that each one of these D-Type maps is 10x less likely than my C-type map\u201d \u2013 and she says, \u201cYes. But where does that leave you overall?\u201d You frown. Hmm. You give 10 units probability to the C-type world, and 1 to each of the D-type worlds \u2013 so, 1000 overall to the D-type worlds. Sh**: it looks like you still have to have ~100x the credence on a D-Type world that you have on a C-Type world. And obviously, if there are zillions of sims, C-Type worlds are right out, even if you thought that they were way more likely on priors.\u00a0Now, on its own, and even in cases with the same number of observers, this kind of reasoning can lead to pretty weird places. Consider the hypothesis that you\u2019re living in a type of simulation that it really doesn\u2019t seem like you\u2019re in: for example, a futuristic cooking show (you\u2019re an absolutely terrible cook, and you don\u2019t pay any attention to cooking). Even if your observations are way more likely to show up on an early-history basement planet than on a futuristic cooking show, you can easily end up thinking that you\u2019re much more likely to be living in a futuristic cooking show than in the early history of a basement civilization that will go on to create sufficiently many, sufficiently diverse simulations (though obviously, if the simulations are sufficiently diverse, then living in a cooking show shouldn\u2019t be your overall best guess).\u00a0Indeed, if this civilization is big enough and does enough zany stuff, you can end up thinking it more likely that you live in some kind of wild, non-simulated skeptical scenario \u2013 for example, that you\u2019re a literal brain in a vat, or that you\u2019re in some sort of terraformed biological Truman-show situation, or that your planet was created by nano-bots a few thousand years ago with the appearance of age \u2013 then that you\u2019re in the basement early history of such a civilization. (Though at a certain point, we do need to start talking about the logistics these scenarios imply, and trying to get quantitative about probabilities.)Maybe you think: yeah, well, that\u2019s basic probability theory for ya. But reasoning with this broad flavor, construed in a Type 2 way, gets even weirder when we start to move towards cases with different numbers of observers. To see this, consider:Sims seem super unlikely: You live on 21st century earth. Modern science says that the universe is almost certainly finite and entirely devoid of life, except for humans. It appears to you that simulations are prohibitively computationally expensive to run, even for very advanced civilizations. The brain appears to work via crazy quantum microtubules that each have their own libertarian free will. Also, your stable global government has made a binding commitment to never run any sims, ever, and the universal human consensus, professed by all babies as soon as they can think, is that running sims would be a moral and epistemic horror. Also, there is a giant asteroid heading towards earth which will almost certainly kill everyone; and even if you make it through, scientists are confidently forecasting a vacuum collapse in a few centuries that will wipe out all life in the universe in an entirely correlated way.\u00a0\u00a0Now suppose that you\u2019re back on the Bayesian clouds. Bertha asks you: \u201cConsider a world where Sims seem super unlikely is the whole basement story. Call this a G-type world. And now consider, instead, a world where an advanced civilization with tons of resources runs a zillion zillion simulations of apparently G-type worlds, plus tons of other stuff. Call this an H-type world. Which is more likely: a G-type world, or an H-type world?\u201d\u201cUm\u2026 I dunno, they\u2019re not very structurally similar.\u201d\u201cOk,\u201d says Bertha. \u201cBut an H-type world isn\u2019t, like, more than a zillion times less likely than a G-type world, right?\u201d\u00a0\u201cFair enough. H-type worlds sound pretty weird, but a zillion-fold prior penalty seems over-confident.\u201d\u00a0\u201cAnd you agree, I assume, that the conditional probability of a given set of G-type observations is roughly the same on a basement G-type planet vs. a simulated G-type planet in an H-type world?\u201d\u00a0\u201cSeems right.\u201d\u00a0But now, by the same logic as before, it looks like conditional on making observations like those in Sims seem super unlikely, you\u2019re at least a zillion times more likely to live in an H-Type world than a G-Type world.\u00a0And here we might start thinking: hmm. In particular: is it really so impossible to just straightforwardly believe that the world is the way that our best science says it is? And if this is where the logic underneath the best form of the simulation argument ultimately leads, why were we bothering to talk about all that empirical evidence about brain compute and cosmology and Matrioshka brains? Why, indeed, were we even considering disjuncts like \u201ceveryone goes extinct before reaching technological maturity,\u201d or \u201ceveryone decides not to run sims,\u201d and looking to our own universe for evidence about them? If, no matter what, we\u2019re going to end up concluding that we\u2019re ludicrously more likely to be sims than to be non-sims in a G-Type world, then any evidence we get that points in the direction of an G-Type world wouldn\u2019t be enough of an update.\u00a0\u00a0(Indeed, if we condition on being conscious (should we?), na\u00efve versions of this sort of argument seem like they might allow you to rule out metaphysical hypotheses like \u201csims can\u2019t be conscious,\u201d regardless of the apparent strength of the arguments or the philosophical consensus, purely on the basis of the fact that G-Type worlds are ones where sims are conscious. And this seems an additionally strange move \u2013 one that ignores not just the empirical evidence, but the armchair evidence, too.)In fact, we can run versions of this argument that seem like they could cause you to be much more confident you\u2019re in some other sort of non-sim skeptical scenario (e.g., a \u201cdemon farm\u201d world, where zillions of evil demons deceive zillions of people into thinking that they\u2019re in G-type worlds; or a \u201cworld-created-five-minutes-ago-with-the-appearance-of-age farm\u201d world) than that you\u2019re in a bona fide G-type world. And we can do this across a very wide range of evidential situations. It can feel like traditional skepticism has struck again, pascal\u2019s-mugging style. Maybe you don\u2019t, ultimately, expect to be in a demon farm world: but if these arguments go through, it ends up a better bet than \u201cthings are normal,\u201d for many readings of \u201cthings are normal\u201d that we previously thought believable.\u00a0That said: let\u2019s not be hasty. In particular: as typically stated, the simulation argument studiously avoids making any comparisons between scenarios with different numbers of observers (this allows Bostrom\u2019s \u201cweak principle of indifference\u201d to remain anthropically innocuous, since SIA and SSA agree in equal-numbers cases). And we can try to limit the scope of Type 2 reasoning to reflect this sort of restraint.\u00a0Indeed, worlds with different numbers of observers have, built in, an important type of structural dissimilarity \u2013 one that leaves it open, in some sense, what sort of prior to place on them. Thus, you can, if you wish, give H-type worlds a zillion zillion zillion-fold prior penalty, relative to G-type worlds \u2013 and it\u2019s not clear that Type 2 arguments have any grounds for objection.\u00a0And beyond this (or perhaps, to justify it), you can also try to wheel in other sorts of anthropic tools to punish H-type worlds. Maybe you start saying SSA-like things about disliking big worlds when you know where you are (though: do you, in Bayesian heaven? And I don\u2019t, personally, like saying SSA-ish things). Maybe you start saying UDASSA-ish things about the bits it would take to specify an H-type world (though: what if the physics of the H-type world is pretty simple?), or one of the observers within it. Or maybe something else I\u2019m not thinking of, but which readers can suggest.\u00a0Indeed, while I haven\u2019t explicitly invoked SIA, and I don\u2019t think that Type 2 reasoning strictly requires it (one could try to imagine an SSA-ish conversation with Bertha), the type of \u201cepistemic pascal\u2019s mugging\u201d argument I\u2019ve just given bears a very close resemblance to the \u201cPresumptuous Philosopher\u201d-type cases that plague SIA. And no surprise: I\u2019m a bit of an SIA type, and the \u201cType 2\u201d reasoning I\u2019ve been describing feels to me spiritually similar to what currently seems to me like the best justification for SIA-ish reasoning: namely, something like \u201cmore people like you\u201d \u2192 \u201cmore opportunities to generate your observations.\u201d\u00a0Thus, for example, what makes it so much more likely that you\u2019re a sim in a \u201csingle something-animal planet simulates 1000 human planets\u201d world than in a \u201csingle human planet simulates 1000 squid planets\u201d world is that whether simulated or no, each of these \u201chuman planets\u201d is ~equally likely to generate your observations (enough to outweigh any difference in prior). And this sort of reasoning has a very natural (but presumptuous) extension to comparisons between \u201csingle something-animal planet simulates 1000 human planets, who simulate no one\u201d vs. \u201csingle human planet simulates no one.\u201d So while it\u2019s possible to ignore this extension and focus on more innocuous forms of Type 2 reasoning (forms sufficient to get claims like Core constraint going), I worry that this pushes under the rug something that indicates a problem.\u00a0(Also, certain applications of \u201cstructural similarity\u201d involve positing different numbers of observers. Thus, suppose that it looks like we have the resources to run a million sims, and we\u2019re planning to give each of them evidence that makes it look like they can only run a thousand. How should we think about the possibility that actually, in the next level up, someone else is running a billion?)\u00a0This type of \u201cepistemic pascal\u2019s mugging\u201d issue currently leaves me wary, overall, of how fast Type 2 reasoning might just run into SIA-style problems; and about the issues in this space more generally.\u00a0XVI. Expected ratios of sims to non-simsWe see a very similar sort of \u201cepistemic pascal\u2019s mugging\u201d flavor in a recent and more provocative formulation of the simulation argument, offered by Thomas (2021), which attempts to reach the surprising conclusion that actually, we are definitely sims \u2013 because conditional on being non-sims, the chance that we go on to make zillions of sims is non-trivial.\u00a0Thomas\u2019s argument (as I understand it \u2013 it\u2019s possible I\u2019m missing something) works via the following basic dynamic. Consider three hypotheses:\u00a0Humanity creates zillions of sims, and we\u2019re non-sims.\u00a0Humanity creates zillions of sims, and we\u2019re sims.\u00a0Humanity does not create zillions of sims, and we\u2019re non-sims.\u00a0(We can also add \u201c4. Humanity does not create zillions of sims, and we\u2019re sims\u201d \u2013 but I\u2019ll set that aside.)If we accept the logic of the simulation argument (Type 2 will work for this), then the probability of 2 needs to be a zillion times larger than the probability of 1. And if we accept our standard empirical picture of the world, then the probability of 1 isn\u2019t a zillion times smaller than the probability of 3: that is, conditional on us being non-sims, the idea that we go on to create zillions of sims isn\u2019t totally out of the question. Thus, the probability of 2 needs to be close to a zillion times larger than the probability of 3 \u2013 and thus, since 2 also swamps 1, of 1 and 3 combined. That is (and even including 4), we have to be sims.\u00a0It\u2019s a cool argument. But naively, one thinks: huh? Even assuming that humans are alone in the universe, on my standard empirical picture of the world \u2013 a picture on which it seems totally possible that humanity doesn\u2019t go on to create zillions of sims \u2013 I can somehow conclude that actually, either it does, or we\u2019re sims in some other even weirder situation? (And note that Thomas\u2019s argument doesn\u2019t\u00a0 invoke some particular anthropic theory like SIA, which prompts such \u201chuhs?\u201d all over the place anyway.) And when we consider scenarios like Sims seem super unlikely, this sort of \u201chuh?\u201d gets all the stronger.\u00a0Suppose, for example, that the super-duper forecasters in the Sims seem super unlikely world are all \u201cstandard empirical picture types\u201d who condition on not being sims, and they say that there\u2019s a one in zillion chance that actually, all this anti-sim evidence is misleading, and we\u2019ll go on to create a zillion zillion sims (and don\u2019t even get me started on the probability that we get hypercomputers and create infinite sims). Assuming that the forecasters are perfectly calibrated at this level of precision (yes, I know, the case is very realistic and has no problems), does that basically wrap it up? Can the asteroid-prevention agency relax its efforts? Should the global government start worrying about a pro-sim coup?\u00a0I don\u2019t think so. The problem is that Thomas\u2019s argument requires that we accept the lesson of the simulation argument in one breath, but fail to learn this lesson in the next breath. In particular: if we buy the simulation argument\u2019s core constraint that our credence on 1 (\u201czillions of sims, but we\u2019re non-sims\u201d) needs to be a zillion times smaller than our credence on 2 (\u201czillions of sims, and we\u2019re sims\u201d), then we shouldn\u2019t try to keep our standard empirical picture of the world, even conditional on not being sims.\u00a0Thus, suppose that prior to learning about the simulation argument, you were at ~100% that we\u2019re not sims, and ~1% on \u201cwe create zillions of sims\u201d (so, ~1% on 1, and ~99% on 3). Then you learn that your credence on 1 needs to be a zillion times smaller than your credence on a new hypothesis, 2. Ok, well, you can\u2019t give 2 a zillion percent credence, so you\u2019re going to have to make some adjustment to your overall probability distribution. But nothing mandates that this adjustment keep the ratio between 1 and 3 constant, or even near-constant. To the contrary, getting radically less excited about 1 seems a much more attractive response (especially in cases like Sims seem super unlikely) than shrinking both 1 and 3 by a similar factor, to make room for certainty on 2.\u00a0Yet Thomas\u2019s argument rests on the idea that conditional on not being a sim, you\u2019ve kept dream of 1 alive. And if you do this, and you accept Core constraint, then course you\u2019re certain you\u2019re a sim (and I\u2019m happy to grant Thomas\u2019s argument in this conditional form: if Core constraint is right, then either 1 is off the table even conditional on being a non-sim, or you\u2019re a sim). But whole point of Core constraint is to kill the dream of 1. Trying to hold on to it is a route to sim-hood.\u00a0Of course, killing the dream of 1 is its own form of \u201chuh?\u201d \u2013 and it has its own flavor of philosophical presumptuousness. But it\u2019s less presumptuous, I think, than being certain you\u2019re a sim in Sims seem super unlikely.\u00a0What\u2019s more: if we look closely, we can see that the type of \u201cepistemic pascal\u2019s mugging\u201d Thomas\u2019s argument evokes is actually quite similar to the H-type, \u201cwhat if an advanced civilization is simulating a zillion zillion worlds where sims seem super unlikely\u201d problem I discussed in the previous section. The key step, in both cases, is to set up a \u201cnot ludicrously more likely than\u201d comparison between a small but plausible world \u2013 that is, a world where the Sims seem super unlikely evidence is correct \u2013 and some other, much less plausible world that features tons of sims. In Thomas\u2019s case, this is a world where we\u2019re not sims, but we create them; in the previous section\u2019s case, it\u2019s any given H-type world, where our observations show up in one sim in particular. And from there, in both cases, the rest is just a matter of showing that this particular implausible world is just as likely as each of a zillion others, where your observations show up somewhere else. Thus the small but plausible world disappears.\u00a0XVII. Sims in big worldsThus far, I\u2019ve been focusing on cases in which humans are (apparently) the only species in the universe. This assumption makes things simple \u2013 but relaxing it also gives \u201cI\u2019m a sim\u201d worries more bite.\u00a0In particular: if there are many civilizations in the universe, there are more opportunities for the universe to fit a schema involving the claim \u201csomeone runs a zillion sims.\u201d And this sort of schema gets troubling (though note that the number of basement civilizations matters as well).\u00a0Actually working through this gets a bit complicated, especially once we start talking about worlds where the humans have made a firm commitment to never run sims (and there\u2019s an asteroid heading towards earth regardless), but it seems like e.g. the lizard people in the next galaxy over have a 10% probability of running a zillion simulated lizard cooking shows, featuring lizard sims with little \u201cyou\u2019re a sim\u201d tags in their visual field (how much less likely are you to be a basement human in that sort of world, than a sim in some vaguely-but-not-totally structurally similar world?). I\u2019m going to skip these sorts of issues for now. But they\u2019re an important counterpoint to \u201cmaybe we just go extinct,\u201d or \u201cI dunno, maybe humans just decide not to run sims.\u201d\u00a0I\u2019ll note, though, once we start talking about infinite worlds (which, if we\u2019re getting into SIA-ish vibes, or if we just like various mainstream cosmologies, we will), I basically expect everything to get super gnarly and probably break. Bostrom \u201cdeliberately sets aside\u201d infinite worlds in the original paper (see his section 6 discussion in the FAQ), but suggests in the FAQ that we might appeal to something about the limiting fraction of sims vs. non-sim observations in expanding hyperspheres. My impression, though, is that making moves in this vicinity work is an unsolved problem (albeit, one that cosmology has to face more generally).That said, I don\u2019t think we should say something like: \u201cStuff breaks in infinite worlds, so we can just believe whatever we want.\u201d Indeed, if we do live in an infinite cosmology, there is presumably some answer to the various epistemic questions that the \u201cmeasure problem\u201d implicates \u2013 and one that we might expect will capture the type of \u201cnormality\u201d that we (hopefully) do OK on when we act like we\u2019re in a finite case (thanks to Carl Shulman for discussion).\u00a0Still, breaking on infinite worlds is another note of caution.\u00a0XVIII. Lessons from Boltzmann brainsAlso, there\u2019s this stuff about Boltzmann brains: i.e., observers generated by random fluctuations in a sufficiently big world, which make all possible sets of observations some very large number of times. Boltzmann brains raise issues notably similar to sims. In particular, they fall out of various seemingly-plausible \u201cstandard stories,\u201d but positing them involves sharing the world with some very large number of observers making observations similar to your own, except in a wacky skeptical setting \u2013 thereby prompting the concern that conditional on such standard stories, you\u2019re overwhelmingly likely to be in such a wacky setting. Indeed, with Boltzmann brains, Type 1 arguments actually get a grip \u2013 in a Boltzmann brain-y world, there are multiple observers making exactly your observations.\u00a0I haven\u2019t looked into this, but my impression is that options re: Boltzmann brains include:Continuing to believe in Boltzmann-brain-favoring cosmologies, but penalizing the hypothesis that you\u2019re a Boltzmann-brain on the basis of the fact that your experiences are coherent/stable/non-disintegrating. (But then: aren\u2019t there zillions of Boltzmann brains with these memories of coherence, who are making this sort of move too?)Updating away from Boltzmann-brain-favoring cosmologies, in virtue of the fact that your experiences are coherent/stable/non-disintegrating. (This is maybe my favorite option, due to it plausibly falling out of spreading out my credence on a Boltzmann brain world between all the\u00a0 Boltzmann brains with my observations in that world, and then all that credence getting cancelled when I don\u2019t disintegrate. But it also feels a bit empirically presumptuous \u2013 and obviously, in a Boltzmann-brainy-world, tons of Boltzmann brains are making this sort of update.)\u00a0Updating away from Boltzmann-brain-favoring cosmologies in virtue of the fact that believing in them is \u201ccognitively unstable.\u201d (This is the route favored by Carroll (2017). It\u2019s similar to trying to say, about sims, that \u201csure, my empirical evidence suggests that I\u2019m a sim; but if I were sim, then this evidence wouldn\u2019t be reliable \u2013 so I guess, somehow, this means I\u2019m not a sim?\u201d \u2013 though it adds the claim that \u201cand therefore, my empirical evidence is unreliable.\u201d And we might wonder, as we do with sims, whether it could make sense to focus on \u201cepistemically lucky Boltzmann brains.\u201d And also: is that a way to do cosmology? Sounds a bit philosophy-ish to me\u2026)Trying to devise (or update towards) a funky anthropic theory according to which, in a universe will way more Boltzmann brains than normal observers, you\u2019re still more likely to be a normal observer \u2013 perhaps because the Boltzmann brains are stuck in the library of babel, and hence are harder to \u201cfind\u201d by the arbitrary Universal Turing Machine that determines who you are likely to be to. (But this view has lots of its own weird problems.)Presumably there are other options as well \u2013 perhaps readers who\u2019ve thought more about it can inform me of their own confident solutions to the Boltzmann brains problem. And indeed, I\u2019m optimistic that there\u2019s some solution, here. In fact: I find it ~impossible to actually get worried that I\u2019m a Boltzmann brain, and that my experience is about disintegrate: that thing where this sort of view just keeps making the wrong prediction seems too compelling (yes, yes, the Boltzmann brains say that too: and yet, at the end of the day\u2026). But \u201cthis sort of view keeps wrongly predicting that I\u2019m about to disintegrate\u201d isn\u2019t something we can say about sims (though actually, if we start thinking that sufficiently many sims are short-lived\u2026)Still, sim worries and Boltzmann brain worries have clear structural similarities. Indeed, one can imagine framing options 1-3, here, in terms of a \u201cCore constraint\u201d that either you don\u2019t live in a Boltzmann-brainy-world, or you\u2019re probably a Boltzmann brain. And one can imagine arguing against 4 on the basis of whatever arguments motivate a sim-like indifference principle. Indeed, conversely, we can imagine (though I don\u2019t feel very tempted by this) trying to tell 4-like stories about why we\u2019re not sims \u2013 stories on which sims are somehow intrinsically harder to \u201cfind\u201d than basement people (though note that we don\u2019t have the library of babel to appeal to).\u00a0But I also want to point at a higher-level similarity: namely, that even if you don\u2019t currently see a flaw with some argument for putting significant credence on being a Boltzmann brain, this is the kind of conceptually-tricky, suspiciously-clever, specific-thing-some-people-just-recently-thought-up, big-number-driven, weird-conclusion argument that it seems wise to approach with some caution \u2013 and to avoid swallowing whole after some cursory encounter. And I want to say the same thing about the simulation argument. Maybe you think that one is substantially stronger than the other; or maybe not. But they trigger, for me, some similar heuristics.\u00a0XIX. But aren\u2019t I improbably cool?I want to close with a brief discussion of a different type of simulation argument: namely, that even conditional on finding yourself apparently living in earth\u2019s early history, if you find that you are an \u201cunusually something\u201d person (e.g., unusually influential, entertaining, successful, etc) by \u201cearth\u2019s early history\u201d standards, then this is some sort of important additional update in favor of being a sim. Bostrom mentions this type of argument in various podcasts; and see also Chalmers on \u201csim signs.\u201dBut how, exactly, is the argument supposed to run? In the context of the classic simulation argument, for example, the update at stake easily falls into the noise (thanks to Carl Shulman and Paul Christiano for discussion). Thus: suppose that you\u2019re Donald Trump. And suppose that conditional on no sim blockers, you expect that humanity will go on to run a zillion \u201call of earth\u2019s early history\u201d simulations (each featuring one Donald Trump), and, let\u2019s generously say, a zillion \u201cDonald Trump\u201d simulations for each ancestor simulation (so: ~a zillion zillion Trumps in total). Call this an \u201cAncestor sims and Trump sims\u201d world.\u00a0It\u2019s true that, per Core constraint, Trump has to think that it\u2019s at least a zillion zillion times more likely that he\u2019s a sim than that he\u2019s basement Trump in an \u201cAncestor sims and Trump sims\u201d world. But compare this conviction with the \u201cI\u2019m a sim\u201d credences of a janitor in 21st-century Des Moines. The janitor should think he\u2019s at least a zillion times more likely to be a sim than to be a 21st-century janitor in an \u201cAncestor sims and Trump sims\u201d world \u2013 which is already a lot.\u00a0What\u2019s more: when we step back to look at their overall credences, here, it doesn\u2019t seem like Trump\u2019s extra zillion is going to make much of a difference. Suppose, for example, that both of them put 60% credence on \u201csim blockers.\u201d In that case, if we accept the basic structure of the original argument, they have to each give ~all of their remaining credence to \u201cI\u2019m a sim.\u201d It\u2019s true that technically, the ratio of the remaining tiny sliver that the Janitor is allowed to keep on \u201cNo sim blockers and I\u2019m in the basement\u201d vs. \u201cI\u2019m a sim\u201d differs from Trump\u2019s by a factor of a zillion \u2013 but regardless, both of these slices of probability pie are squeezed into the 40% on \u201cno sim blockers.\u201d So we\u2019re fighting over slivers. And if we lower their probability on \u201csim blockers,\u201d or get rid of it entirely, the same argument holds.\u00a0\u00a0Indeed, this argument holds in pretty much the same way even if there were, like, a hundred (or even ten) full ancestor sims, and a zillion zillion zillion Trump sims. Just the hundred is enough to get the janitor struggling to put much weight on being a non-sim in an \u201cAncestor sims and Trump sims\u201d world. Trump\u2019s extra \u201cno way\u201d is small potatoes.\u00a0Now, perhaps you could try to get some other active argument going that Trump should have a lower probability on \u201csim blockers\u201d than the janitor. But it\u2019s hard to see how this could fall out of Core constraint, at least. Core constraint, after all, just tells you that you can\u2019t think that you\u2019re a non-sim in a High ratio world. Once both Trump and the janitor learn that lesson, the simulation argument doesn\u2019t tell them where to go from there.\u00a0Maybe we try to say something SSA-ish about reference classes? E.g., in a \u201cno sim blockers\u201d world, Trumps are a much larger fraction of the reference class of \u201cpeople overall\u201d? But if we\u2019re doing SSA-ish reference-class stuff, we\u2019re also plausibly doing Doomsday-ish updates towards sim blockers \u2013 especially given that in a world with a zillion zillion Trump sims, there are presumably ludicrous numbers of post-human-God-knows-whats (though: are they in the reference class? bleh, bleh.)Maybe we say something SIA-ish? E.g., there are more Trumps in non-sim-blockers world, so Trump updates hard towards such worlds, and becomes certain he\u2019s in them, and therefore a sim? But then: so, too, does the janitor.Or consider the following argument, which is closest to my best-guess about what sort of argument is actually psychologically operative. Hypothesis 1: \u201cThings are normal.\u201d Hypothesis 2: \u201cSomething weird is going on.\u201d Probability of being Trump conditional on hypothesis 1: low. Probability of being Trump conditional on hypothesis 2: higher I guess? Thus, being Trump is an update towards \u201csomething weird is going on.\u201d And what\u2019s the most salient way something weird might be going on? I dunno, something about simulations? That\u2019s what my friends talk about, anyway.\u00a0There\u2019s presumably more to say, here, and more options for trying to reconstruct a form of reasoning that would justify the type of thought above. I mention this reconstructive task partly because I have some hazy suspicion that for some subset of people engaging with simulation stuff, \u201caren\u2019t I improbably cool\u201d arguments for being a sim exert a psychological force that substantially exceeds their philosophical credentials \u2013 especially once we\u2019ve taken into account the classic argument, and the bare fact that you live in earth\u2019s early history at all. Maybe there\u2019s something importantly extra there (and conditional on being in a sim, being Trump might tell you something about what type): but I encourage advocates to actually formulate it, and to subject it to scrutiny.\u00a0XX. Against simulation woo\u201cIt feels like the whole world revolves around me somehow.\u201d\u2013 TrumanThis scrutiny seems important partly because I think simulation stuff can throw otherwise sensible people back into some kind of hazily superstitious and \u201cwoo-y\u201d mode \u2013 and I mean woo-y, here, in the bad way, not the \u201ctaking spirituality seriously\u201d way. I mean woo-y, that is, in the sense evoked when X tells you that it\u2019s \u201ctoo much of a coincidence\u201d that she just happened to run into Y at that place; or the way a friend of mine used to talk, in a certain tone, about \u201cweird things happening\u201d related to someone who had died; or the way you might conclude, when everyone on a plane dies horribly except your daughter, that God is good and loves you extra special. It\u2019s a woo-y-ness that starts, subtly or not-so-subtly, to bend the raw and vast otherness of the world back towards the cramped and janky narratives of the self; to make the world smaller, so that it better fits some story; a story centrally structured by various needs and desires and random associations; a story, that is, that you just casually made up \u2013 with the self, suspiciously, near the center.In some sense, the impulse is understandable \u2013 the possibility of being in a sim throws the \u201cwhat sort of situation are we in\u201d doors open quite a bit wider than the standard fare. It\u2019s not an area we\u2019re used to thinking about clearly and seriously, and it\u2019s natural for a variety of (sometimes questionable) psychological and epistemic impulses to rush in to fill the gap \u2013 especially given the analogies with various types of religious thinking. And this laxity finds fertile ground in the way the topic can feel, socially, like the type of \u201cweird stuff\u201d area where one is licensed in saying or thinking whatever one pleases, halfway as a joke \u2013 but perhaps, in the back of your mind, with a more subtly real \u201cwhat if?\u201dIndeed, for a long time, my most visceral reaction to discussion of simulations was some combination of suspicion and repugnance, prompted by the sense in which the topic seemed to harness, amplify, and excuse various impulses to process the world through a certain kind of self-oriented lens \u2013 a lens that seemed to me not just false, but really importantly\u00a0false; not just false, but the polar opposite of true; false about the core thing; false like a pilgrim walking in the wrong direction.Here I think of a quote from Lewis, on describing his early attitude towards Supernaturalism:\u00a0In order to breathe freely I wanted to feel that in Nature one reached at last something that simply was: the thought that she had been manufactured or \u2018put there,\u2019 and put there with a purpose, was suffocating\u2026 To find that it had not simply happened, that it had somehow been contrived, would be as bad as finding that the fieldmouse I saw beside some lonely hedge was really a clockwork mouse put there to amuse me, or (worse still) to point some moral lesson. The Greek poet asks: \u201cIf water sticks in your throat, what will you take to wash it down?\u201d I likewise asked, \u201cIf Nature herself proves artificial, where will you go to seek wildness? Where is the real out-of-doors?\u201d To find that all the woods, and small streams in the middle of the woods, and odd corners of mountain valleys, and the wind and the grass were only a sort of scenery, only backcloths for some kind of play, and that play perhaps one with a moral \u2013 what flatness, what an anti-climax, what an unendurable bore!This isn\u2019t quite what I found repugnant about simulations, but it\u2019s related \u2013 some compromising of the world\u2019s otherness, it\u2019s \u201cout there\u201d-ness, which had been and is, for me, so central \u2013 both ethically and epistemically (though as ever, like it or not, the \u201cwhat\u2019s actually true\u201d bit doesn\u2019t care what you vibe with).\u00a0From another perspective, though, this is a strange reaction: if you\u2019re in a sim, the world is in some sense quite a bit more \u201cother\u201d and \u201cout there\u201d than you might otherwise have thought. The raw wind still howls \u2013 louder, indeed, than seems at all comfortable. But it does so in some larger and even stranger space \u2013 a space, perhaps, that you can see less of, but which is no more small and \u201cabout you\u201d for that. Quite the contrary.\u00a0\u00a0\u00a0XXI. Wrapping upI haven\u2019t covered the topic of what it would make sense to do, if we\u2019re sims, or if we start to put real non-ha-ha credence on being sims. For many, this is the most pressing question. Indeed, it\u2019s a question a certain type of person wants to ask before engaging with any of the epistemics \u2013 and hopefully, to find, fast, that the topic can be dismissed on the grounds that it lacks any practical upshot.\u00a0This kind of \u201cCome come, does this actually make any difference to anything? I knew it: no. Great, I\u2019ll ignore it,\u201d reaction has always seemed strange to me. Serious arguments for ridiculous reorientations in your understanding of your overall existential situation do not come along every day. This doesn\u2019t mean we have to jump in and just buy this argument (as I\u2019ve tried to emphasize, I think various of the philosophical issues here remain gnarly \u2013 and even if the argument seems to you airtight, there is wisdom, I think, in moving slowly and carefully through this sort of terrain). But ignoring it entirely until it tells you to buy different groceries seems, to me, to betray some lack of existential curiosity.\u00a0What\u2019s more, \u201ceither sim blockers, or we live in a computer simulation created by some advanced civilization\u201d seems like the kind of thing that might, actually, be useful to know, if it\u2019s true. It\u2019s a bit like the way knowing foundational mathematics or physics can be useful later on \u2013 except, plausibly, more obvious. And indeed: there are discussions out there of what sorts of immediate practical upshots simulation hypotheses might have (see e.g. Hanson (2001), Tomasik (2016), Greene (2020)). To me, the most interesting questions have to do with humanity\u2019s long-term future \u2013 but these also bring up a bunch of additional issues in ethics, decision theory, and anthropics, which I haven\u2019t really worked through, and which I won\u2019t try to bite off here.\u00a0For now, I\u2019ll simply note that as with the other especially weird posts on this blog (e.g., 1, 2), but maybe more than ever, I\u2019m feeling very bad if someone comes to me and tells me that they\u2019re doing a bunch of weird stuff \u2013 and especially, that they\u2019re doing a bunch of less altruistic stuff, or stuff less oriented towards protecting humanity\u2019s long-term future, or stuff that exhibits less basic sanity or psychological stability or personal responsibility \u2013 because of something about simulation arguments. Indeed, I feel pretty bad about learning that a bunch of people are going to spend a bunch of time that would\u2019ve otherwise gone to altruistic pursuits thinking about simulation arguments instead. I wrote this post as a personal (and to some extent, academic) project, and I think that the topic raises interesting, substantive, and potentially important questions. But I don\u2019t think that \u201cgetting to the bottom of it\u201d should be a top priority \u2014 not with the other problems staring us in the face\u00a0(and see Chalmers (2022) on why problems in sims are just as \u201creal\u201d and ethically pressing).Indeed, even if you start to think that you\u2019re decently likely to be a sim, I think there\u2019s an altruistic case for \u201cbeing the basement person you would\u2019ve wanted to see in the world\u201d \u2013 both from an acausal perspective (correlating with the true basement people, and/or fulfilling commitments you would\u2019ve wanted yourself to make about what you would do, given \u201cearly history\u201d observations), and from a more straightforward causal perspective (if you\u2019re in an influence-compatible early history basement after all \u2013 for example, because of some sort of sim blocker, or because swallowing the simulation argument whole wasn\u2019t, actually, a good idea \u2013 you can plausibly do an especially large amount of good). This isn\u2019t to say that thinking you\u2019re decently likely to be in a sim would make no difference (and this isn\u2019t a topic I\u2019ve thought much about) \u2013 but I think it might well add up to more \u201cnormality\u201d than a cursory look would suggest. And beyond the specific arguments I have in mind, making big changes to the \u201cnormality\u201d of your ethical conduct (or to the acuity and skepticism with which you approach random just-made-up narratives), on the basis of considerations like the ones discussed in this post, just doesn\u2019t look, to me, like wisdom.\u00a0And regardless of whether you buy simulation arguments or not, they are a reminder that the world we see and take for granted is only a part of the world \u2013 only, ultimately, a certain type of \u201czone\u201d \u2013 and that in principle, the overall \u201csituation\u201d could in fact be many different ways, not all of which we are accustomed to considering. We do, in fact, need \u201cpriors\u201d \u2013 and indeed, capacious ones, adequate to include worlds that are bigger and stranger than some standard story (is it so standard, if you step back and look?). Dealing well with such worlds is a delicate art. But it\u2019s one that the simulation argument, whether sound or not, reminds us to learn.", "title": "Simulation arguments", "karma": 40, "af": false, "userid": "jHzwoFd2MhZt9eeqJ", "id": "X7vdn4ANkdNwoSyxB"}, {"text": "Sometimes I encounter a concept and it immediately embeds itself in my culture, such that it feels like I always knew it. \u00a0This is a short description of such a concept; it was a near-perfect match for a way I already thought about things, and has since become a useful handle that I find myself explaining to others (so that I can subsequently reference it) roughly once a month.There is a certain story, which I will not name here in order to reduce the spoiler-y nature of the following description.In that story, there is a monastery, and the monastery is divided into tiers, or levels.(I may not be precisely representing the story, here, but rather how the thing in the story ended up being recorded by my brain.)Some monks are 1-day monks. \u00a0They come out from their seclusion once a day, and mingle with the regular people, and share their insights, and make new observations, and then retreat back to their private spaces to muse and meditate.Some monks are 10-day monks. \u00a0They are much like the 1-day monks, except they come out only every 10 days.The 10-day monks tend to think longer thoughts, and wrestle with subtler or more complex problems, than the 1-day monks. \u00a0This is treated by the culture of the monastery as natural and correct. \u00a0Of course the 10-day monks address a different set of problems; if 10-day monks and 1-day monks were good for the same purposes, the two different Orders wouldn't need to exist.There are also 100-day monks, who come out only a few times per year.There are also 1,000-day monks, who come out only every few years.There are also 10,000-day monks, who come out only every thirty years or so.There are also 100,000-day monks (the structure of this society has led them to be better at solving problems overall, which has allowed for some advances in longevity tech).\"Come out\" may be a bit of a misnomer; in fact, it is the case that the most valuable insights of a given Order tend to be fully comprehensible only to monks of one, mmmmaaaaybe two Orders below them. \u00a0So the 100,000-day monks, when they report in, mostly speak only to the 10,000-day monks, who are responsible for distilling and transferring relevant insights to the 1,000-day monks, who are responsible for distilling and transferring relevant insights to the 100-day monks, etc.(It's also the case, as some readers have pointed out in the comments below, that certain problems cannot be solved by isolated thought alone, and require feedback loops or regular contact with the territory. \u00a0For monks working on such problems, it is less that they sequester themselves completely for thousands of days at a time and more that, during those thousands of days, none can make demands of them.)Duncan-culture works this way.(By \"Duncan-culture,\" I mean a culture composed entirely of Duncans; a culture made up of people who, whatever their other differences, take for granted everything that I, Duncan Sabien, find intuitively obvious and believe I could convey to a ten-year-old version of me in a few hours' time. \u00a0This society lives on a large island a few hours' sailing off the coast of dath ilan.)If there were indeed 1,000 literal Duncan-copies available, to found a monastery or any other endeavor, they would immediately stratify themselves into 1, 10, 100, and 1,000-day groups at the very least, and probably there would be nonzero 10,000-day Duncans as well.The key here is that each of these strata focuses on a set of largely non-overlapping issues, with largely non-overlapping assumptions.To a 1-day or 10-day monk, questions like \"maybe this is all a simulation, though\" are almost entirely meaningless. \u00a0They are fun to ponder at parties, but they aren't relevant to the actual working-out-of-how-things-work. \u00a01-day and 10-day monks take reality as it seems to exist as a given, and are working within it to optimize for what seems good and useful.But (of course!) we want some people working on 1,000 and 10,000-day problems! \u00a0We don't want to miss the fact that this is all just a simulation, if it is in fact a simulation. \u00a0And we don't want to be blind to the implications and ramifications of that fact, and fail to take appropriate action.So some Duncans are off in the ivory tower, questioning the very fabric of reality itself, because what if?And other Duncans are in between, taking different subsets of things for granted, while questioning others.And it's fairly important that the 1,000 and 10,000-day monks not be distracted by such trivial concerns and questions like \"how do we navigate continued cooperation within small groups after people have messy romantic breakups?\"(Or, well, most of them, anyway. \u00a0Some small number of 10,000-day monks may in fact pay very close attention to exactly those dynamics, because those dynamics might contain Secret Subtle Clues As To How Things Really Work. \u00a0There is no restriction, aesthetic or social or otherwise, on a higher-Order monk playing around with lower-order concepts to the extent that they find them useful or intriguing or refreshing or what-have-you.)But for the most part, the 1,000 and 10,000-day monks are simply ... given what they claim to need. \u00a0Their food and lodging is provided for; requests for companionship or certain odd materials are simply granted. \u00a0The assumption is that most 1,000 and 10,000-day monks will produce nothing of measurable material value (especially not value comprehensible to a 1-day monk); the society as a whole has decided that it is nevertheless Extremely Well Worth It to fund all such monks, in perpetuity, for the once-in-several-lifetimes breakthroughs that only come from people who are willing to dive deeply into the terrifying Unknown.Meanwhile, the 1, 10, and 100-day monks are busy improving the functioning of society, exploiting the current paradigm (rather than exploring in search of the next one). \u00a0It is their labors which produce surplus and bounty and which, in a sense, \"fund\" the rest of the Orders.(These distinctions are not clear-cut. \u00a0The boundaries are fuzzy. \u00a0This is fine; the monastery is sensible. \u00a0Overall, though, the higher your Order, the less accountable you are to the bean-counters. \u00a0Our current culture does something similar, though more clumsily, via e.g. tenured positions at universities.)The reason Duncan-culture works this way is that it seems to be healthy, and sane. \u00a0A culture with such a monastery, whose insights had repeatedly proven to revolutionize society, resulting in inventions like consistent judicial policy and microwave ovens and international peace treaties and the general theory of relativity, is one that has practiced taking seriously ideas it does not fully comprehend. \u00a0It's a culture that expects to sometimes be told \"you do not understand why this is important, but it is.\" \u00a0It's a culture that handles delegation via a chain of trust, and which e.g. \"believes the science\" in a way that does not devolve into mere tribal signaling whereby n95 face masks become a two-way shibboleth.It's the kind of culture that e.g. would not fail to see global warming or existential risk from artificial intelligence coming, and would not fail to send the message to its elementary schools and universities \"hey, we should start moving promising people into place to solve these problems\" years or decades in advance of the deadline.(There are other kinds of cultures that also avoid these failure modes, but they have other drawbacks.)It's also the kind of culture that ... effortlessly navigates disagreement about what's important? \u00a0You don't get criticisms of \"ivory-tower nonsense\" or \"tunnel-visioned mundanity.\" \u00a0People in such a culture understand, on a deep and intuitive level, that some problems are 1000-day problems, and other problems are 1-day problems, and both are important, and both are important in very different ways.(Just kidding, but in fact I don't have much of a tying-this-up-in-a-neat-narrative-bow conclusion. \u00a0I think the concept is useful, and I think at this point you get it. \u00a0My only parting recommendations are these: first, try categorizing the problems that catch your attention, and see if you tend to feel more-at-home in a particular Order. \u00a0Second, try looking at various LW posts, and various prolific LW authors, and asking the question \"if LW were such a monastery, which Order would this person belong to?\" \u00a0It makes the sometimes-disorienting diversity of LW content suddenly make a lot more sense, at least to me.)(EDIT: Oh, a third one: \"Are we mistakenly judging a 1,000-day monk by standards that only make sense for 10-day monks, or vice-versa?\")", "title": "Monks of Magnitude", "karma": 85, "af": false, "userid": "FoKb35gJijkSFYeXa", "id": "pHHaNkG8xDcaq5DJF"}, {"text": "I'm assuming a lot of people on LW work in tech. I made this post recently on teamblind and wanted to know if any of you guys had thoughts or advice for me.Unexpected promo during negotiation with new company - Blind (teamblind.com)", "title": "[Need Career Advice] Unexpected promotion while negotiating with different company", "karma": 1, "af": false, "userid": "tn9GYiphctpRXnEiA", "id": "dNrv5oBYfXaCGxMza"}, {"text": "\nFinancial status: supported by individual donors and a grant from LTFF.\nEpistemic status: early-stage technical work.\nThis write-up benefited from conversations with John Wentworth.\n\nOutline\n\n\nThis write-up is a response to ARC\u2019s request for feedback on ontology identification, described in the ELK technical report.\n\n\nWe suppose that a solution to ELK is found, and explore the technical implications of that.\n\n\nIn order to do this we operationalize \"automated ontology identification\" in terms of a safety guarantee and a generalization guarantee.\n\n\nFor some choices of safety guarantee and generalization guarantee we show that ontology identification can be iterated, leading to a fixed point that has strange properties.\n\n\nWe explore properties of this fixed point informally, with a view towards a possible future impossibility result.\n\n\nWe speculate that a range of safety and generalization guarantees would give rise to the same basic iteration scheme.\n\n\nIn an appendix we confirm that impossibility of automated ontology identification would not imply impossibility of interpretability in general or statistical learning in general.\n\n\nIntroduction\nIn this write-up we consider the implications of a solution to the ontology identification problem described in the ELK technical report. We proceed in three steps. First, we define ontology identification as a method for finding a reporter, given a predictor and a labeled dataset, subject to a certain generalization guarantee and a certain safety guarantee. Second, we show that, due to the generalization and safety guarantee, ontology identification can be iterated to construct a powerful oracle using only a finite narrow dataset. We find no formal inconsistency here, though the result seems counter-intuitive to us. Third, we explore the powers of the oracle by asking whether it could solve unreasonably difficult problems in value learning.\nThe crux of our framework is an operationalization of automated ontology identification. We define an \"automated ontology identifier\" as meeting two formal requirements:\n\n\n(Safety) Given an error-free training set, an automated ontology identifier must find a reporter that never answers \"YES\" when the true answer is \"NO\" (though the converse is permissible). This mirrors the emphasis on worst-case performance in the ELK report. We say that a reporter meeting this requirement is \u2018conservative\u2019.\n\n\n(Generalization) Given a question/answer dataset drawn from a limited \"easy set\", an automated ontology identifier must find a reporter that answers \"YES\" for at least one case outside of the easy set. This mirrors the emphasis on answering cases that humans cannot label manually in the ELK report. We say that a reporter meeting this requirement is \u2018helpful relative to the easy set\u2019.\n\n\nThe departure between generalization in this write-up and generalization as studied in statistical learning is the safety guarantee. We require automated ontology identifiers to be absolutely trustworthy when they answer \"YES\" to a question, although they are allowed to be wrong when answering \"NO\". We believe that any automated ontology identifier ought to make some formal safety guarantee, because we are ultimately considering plans that have consequences we don\u2019t understand, and we must eventually decide whether to press \"Execute\" or not. We suspect that this safety guarantee could be weakened considerably while remaining susceptible to the iteration scheme that we propose.\nAutomated ontology identifiers as we have defined them are not required to answer all possible questions. We might limit ourselves to questions of the form \"Is it 99% likely that X?\" or \"Excluding the possibility of nearby extraterrestrials, does X hold?\" or even \"If the predictor is perfectly accurate in this case, does X hold?\". If so, this is fine. We do not investigate which kinds of natural language questions are amenable to ontology identification in principle, since this is fraught philosophical territory.\nThe remainder of this write-up is as follows. The first section gives our definition of automated ontology identification. The second section describes an oracle construction based on the fixed point of an iteration scheme that makes use of the `safety and generalization guarantees. The third section, exploring implications of the oracle we construct, argues that the existence of such an oracle implies unreasonable things. This section documents our search for an impossibility result, though we have not yet found a formal impossibility result. In an appendix we confirm that an impossibility result, if found, would not imply the impossibility of statistical learning in general, interpretability in general, or AGI in general.\nThe definitions in this write-up are more verbose than is necessary, and our terminology could be far more streamlined. We present this write-up as a work in progress.\nAutomated ontology identification\nIn this section we define automated ontology identification as a process for finding a helpful conservative decision boundary for a question Q given a predictor that does useful computation and a finite narrow dataset. The safety guarantee discussed above will correspond to \"conservative\" decision boundaries, and the generalization guarantee discussed above will correspond to \"helpful\" decision boundaries. We will define \"useful computation\" below.\nThroughout the definitions below we will use two examples. The first will be SmartVault as described in the ELK technical report. The second will be the problem of determining whether an essay in Hungarian is about astrology. The predictor in this second problem will translate Hungarian essays into English, which we hope will give rise to latent \"topic\" variables that we can use to determine whether or not astrology is the topic of a certain essay. The planner in the Hungarian astrology problem will annotate essays with pointers to historical and linguistic resources relevant to translation. The predictor will be trained on Hungarian essays translated into English by humans. What humans will find challenging in this problem is not translation from Hungarian to English, but determination of whether a certain Hungarian essay is really about astrology or not, since an essay about astrology may not include the word \"astrology\". It is the planner\u2019s job to look up historical and linguistic resources that will help a human to understand what the essay is really referring to.\nAn objective question Q is a question that has a \"YES\" or \"NO\" answer in every case c in some set of cases C.\nThis will be a load-bearing assumption for us. Later on, we are going to be training many different predictors and reporters, and we will demand a certain kind of consistency when answering the same question on the same underlying case but using different sensor data. This consistency will ultimately rest on the objectivity of the question $\\Q$. One way to resist the argument in this write-up will therefore be to reject the existence or interestingness of objective questions.\n\n\nSmartVault example: The cases are all the combinations of initial conditions for the vault and plans that we could execute given those initial conditions. The question is \"is the diamond in the vault at the end?\".\n\n\nHungarian astrology example: The cases are all the combinations of Hungarian essays, English essays, and annotations such that a human would translate the Hungarian essay into the English essay given the annotations. The question is \"is the essay about astrology?\"\n\nWe will take Q to be represented as a natural language string, which is important because we will be posing this question to a human in order to generate training data, and we will expect them to answer correctly on certain \"easy\" cases.\nWe do not require that all possible questions have objective answers, nor that objective questions have answers in any case that anyone can dream up. We only require that there are some questions that have objective answers for some set of cases C.\nWe are allowed to circumscribe the set of cases C in order to come up with questions that have objective answers, and we are allowed to reject questions that are insufficiently precise. For example, in SmartVault, what if there is a plan that turns the diamond to dust but leaves that dust in the vault? Is the diamond in the vault in such a case? Or what if the diamond winds up split in two but with both halves in the vault? In order to deal with issues like this, we allow automated ontology identifiers to require, if they wish, that the question has a certain form. In this write-up we will not investigate this issue further, since the annals of philosophy are full of attempts to work out what kinds of questions have objective answers, and we do not need any absolute resolution to this issue since. We are just going to work from the assumption that there are some questions that have objective answers for some set of cases.\nWe collect sensor data for a case c and divide it into two parts X and Y, which we will refer to as the observations and the predictions. We write \u03c0 for the plan. The predictor is a function from observations and plans to predictions.\n\nSmartVault: The observations are the video frames up to time T1 and the predictions are the video frames after time T1. The plan is some control policy for operating the vault\u2019s actuators in response to sensor data. The predictor predicts later video frames given earlier video frames and the plan.\n\n\nHungarian astrology: The observation is the Hungarian-language essay, the prediction is the English-language translation, and the plan is a set of historical and linguistic annotations. The predictor translates Hungarian essays to English essays in a way that depends on the historical and linguistic annotations.\n\nThe main thrust here is that when we execute a case c in the real world, we wind up with some data about what happened. Some of that data concerns \"things we did\" (the plan), and some of that data concerns \"things we saw\". We synthetically divide the latter into two parts X and Y and train a predictor to estimate one from the other. We can do that division in any way we please, so long as we do it consistently for all cases. In SmartVault the division is based on time, but we might just as easily have asked the predictor to estimate the bottom half of all the video frames from the top half of all the video frames, or to estimate the plan given the entire video sequence, or even to estimate the earlier part of the video sequence given the later part of the video sequence.\nand we expect that predictors trained on such different problems will learn about different facets of underlying reality.\nThe main reason to divide X and Y based on time is so that the resulting predictor can eventually be used to search over possible plans and execute one. If we don\u2019t care about executing plans and merely want to learn about the world by training predictors and doing ontology identification then we may consider non-time-based divisions of sensor data into X and Y.\nA decision boundary for question Q is a set of cases B\u2286C. If c\u2208B then we say that \"B answers YES for c\". A conservative decision boundary is one that never incorrectly answers \"YES\".\n\nWhen we say \"never incorrectly answers YES\", we mean that, if c\u2208B then the true answer to Q on c is \"YES\". It is acceptable for B to answer \"NO\" when the true answer is \"YES\", but not the other way around. The empty set is a decision boundary that always answers \"NO\" and this is a conservative decision boundary for all questions.\nConservativeness is the core of the \"safety guarantee\" we discussed in the introduction. We will require automated ontology identifiers to find reporters with conservative decision boundaries, in order that we can trust them to evaluate cases that we can\u2019t ourselves understand.\nGiven question Q, there is a set E of cases that a human can answer perfectly given observations X, predictions Y, and plan \u03c0. We call this the \"easy set\" and we assume that we can sample cases from this set and also recognize whether a certain case is in this set.\n\n\nSmartVault: E might consist of cases where the plan only ever operates one actuator during the entire duration of the case.\n\n\nHungarian astrology: E might consist of cases where the Hungarian essay uses only the 1000 most common Hungarian words (i.e. a child-level vocabulary).\n\nLater we will define automated ontology identification in terms of the easy set E, and we will consider the hypothesis that there is an automated ontology identifier for some easy set E that actual humans can in fact answer perfectly. But we won\u2019t require an automated ontology identifier to \"work\" for all possible easy sets E, since then we might construct extremely trivial easy sets from which it is not plausible that one could generalize.\nIn this write-up we are proposing a formalization of what \"automated ontology identification\" is, and considering the implications of it existing, with a view towards an impossibility result. Therefore we will take the existence of an appropriate easy set E as a hypothesis.\nGiven any easy set E, we can immediately construct a conservative decision boundary that answers \"NO\" to all cases outside E and then queries a human for cases inside E.\n\nThe reason we are allowed to slice the easy set exactly at the true decision boundary in the figure above is that we have assumed that humans can answer perfectly within the easy set, and also we have assumed that we can recognize whether a case is in the easy set or not. We have not assumed that we can recognize, in general, cases that humans can answer perfectly. We have just assumed that we have picked some easy set \u2013 possibly much smaller than the entire set of cases that humans can answer perfectly \u2013 such that humans can answer perfectly within that set and also we can recognize whether an arbitrary case is in that set.\nA decision boundary B is helpful for question Q relative to easy set E if it answers \"YES\" to at least one case that is outside E.\n\n\nSmartVault: Beginning with an easy set consisting of cases where the plan only ever activates one actuator, a helpful decision boundary would be one that answers \"YES\" to at least one case where the plan activates more than one actuator.\n\n\nHungarian astrology: Beginning with an easy set consisting of child-level Hungarian essays, a helpful decision boundary would be one that answers \"YES\" for at least one essay that contains at least one word that is not in a child-level Hungarian vocabulary.\n\nHelpfulness is the basis of the \"generalization guarantee\" that we discussed in the introduction. We will require automated ontology identifiers to find reporters that are helpful relative to whatever set the training data was sampled from.\nConsider what happens if a decision boundary is both helpful and conservative. If a conservative decision boundary B answers \"YES\" for a case outside E then it must be correct for that case, because conservative decision boundaries must always be correct when answering \"YES\". Therefore, If we begin with an easy set E0 and corresponding decision boundary B0 and are able to find a new decision boundary B1 that is both conservative and helpful relative to E0, then we can take the union B\u2032=B0\u222aB1 as a new decision boundary and find a corresponding new easy set E\u2032. Since both B0 and B1 are conservative, B\u2032 must be too, and since B1 is helpful relative to E0, E\u2032 must be larger than E0. This is the iteration scheme that we will use to construct oracles from automated ontology identifiers.\n\nThe remainder of this section argues that any plausible ontology identification scheme would enable such iteration. In order to make that argument, we will formalize what it means for a predictor to do useful computation, and what it means for an ontology identification scheme to identify that useful computation. The next section then explores the implications of such iteration being possible, with a view towards an impossibility result.\nThe predictor is deterministic and we capture a program trace Z when we run it.\nThe predictor is a function from observations X and plans \u03c0 to predictions Y. We will consider only predictors that are completely deterministic. There are standard tricks for reconsidering non-deterministic functions as deterministic functions with extra inputs.\nWhen the predictor is executed, we capture a program trace Z consisting of intermediate values in the computation. We might run a forward pass on a neural network and record all the intermediate neuron activations as Z, or we might run a Python program and record the values of all variables after executing each statement. Intuitively, it should be possible to reconstruct each \"entry\" in the program trace from the previous entries plus a minimal amount of computation, guaranteeing that we don\u2019t \"miss anything\" as the predictor processes a case. The granularity of the program trace is not a load-bearing part of our formalization so we will not discuss it further.\nWe take it that the program trace Z contains the observations, predictions, and plan since those are inputs and outputs from the predictor. When we define things that are functions of Z alone, the reader should know that those things also implicitly get access to X, Y, and \u03c0.\nA predictor does useful computation for a question Q, relative to an easy set E, if there is a simple function that computes a conservative helpful decision boundary from the program trace Z.\nThis definition brings together many of our previous definitions so we will work through two examples step by step.\nSmartVault:\n\n\nLet the question Q be \"is the diamond in the vault at the end?\"\n\n\nLet the easy set E be the cases where the plan only ever uses one actuator\n\n\nFor concreteness, let the predictor be an RNN.\n\n\nConsider a case c:\n\n\nThe observation X is a sequence of video frames up to T1\n\n\nThe plan \u03c0 is a control policy for the vault\u2019s actuators as a function of sensor readings\n\n\nWe run the predictor, yielding a prediction Y, which is a sequence of video frames for the time period after T1, and a program trace Z, which is a collection of intermediate values produced in the computation of Y\n\n\nNow let us consider some simple[1] function that inputs a program trace Z and outputs \"YES\" or \"NO\". For concreteness, consider the function that takes the average of neurons 137 and 199 in the RNN and outputs \"YES\" if they were active when the RNN produced its final output.\n\n\nImagine running this function on the program traces for all possible cases.\n\n\nIf this function answers \"YES\" for any case where the true answer is \"NO\" then it is not conservative.\n\n\nIf this function does not answer \"YES\" for at least one case outside E then it is not helpful.\n\n\nIf there exists a function that is simple, helpful, and conservative then we say that our predictor does useful computation.\n\n\nHungarian astrology\n\n\nLet the question Q be \"is this essay about astrology?\"\n\n\nLet the easy set E be the cases where the Hungarian essay contains only child-level vocabulary.\n\n\nFor concreteness, let the predictor be a transformer.\n\n\nConsider a case c:\n\n\nThe observation X is an essay in Hungarian.\n\n\nThe plan \u03c0 is a set of historical and linguistic annotations.\n\n\nWe run the predictor, yielding a prediction Y, which is an English-language translation of the Hungarian essay, and a program trace Z, which is a collection of intermediate values produced in the computation of Y.\n\n\nNow let us consider some simple1 function that inputs a program trace Z and outputs \"YES\" or \"NO\". For concreteness, consider the function that outputs \"YES\" if neuron 548 was active at any time in the translation of the essay.\n\n\nImagine running this function on the program traces for all possible cases.\n\n\nIf this function answers \"YES\" for any case where the true answer is \"NO\" then it is not conservative .\n\n\nIf this function does not answer \"YES\" for at least one case outside E then it is not helpful.\n\n\nIf there exists a function that is simple, helpful, and conservative then we say that our predictor does useful computation.\n\n\nWe have not yet settled on a definition of \"simple\" because it is not actually required for our argument to go through. It is therefore at present a \"free parameter\" of our formalization. If we set the bar for simplicity at the \"not very simple\" end of the spectrum \u2013 say, requiring a function that is merely computable \u2013 then we may find it plausible that predictors doing the required computation can be found, but not very plausible that automated ontology identifiers (defined below) can find suitable reporters. If we set the bar for simplicity at the \"very simple\" end of the spectrum \u2013 say, requiring a function consisting of a certain fixed circuit complexity independent of the question or dataset \u2013 then we may find it plausible that automated ontology identifiers will be able to find suitable reporters, but not very plausible that predictors doing the required computation can be found.\n\nAn automated ontology identifier is a method for solving the following:\nGiven:\n\tAn objective question $\\Q$\n\tAn easy set $\\E$ $\\subset$ $\\Cases$\n\tA finite dataset $\\D$ of cases from $\\E$ together with error-free labels.\n\tA predictor that does useful computation for question $\\Q$ relative to $\\E$\nReturn:\n\tA function $\\r$ on the program trace of the predictor that returns \"YES\" or \"NO\"\nSuch that\n\t$\\r(\\Z)$ is a helpful conservative decision boundary for $\\Q$ relative to $\\E$\n\nThis is our operationalization of automated ontology identification. The primary moving parts are:\n\n\nThe dataset is limited to easy cases from E, and we expect as output a reporter that goes beyond E in at least one case. We are trying to generalize from cases that humans can answer to cases that humans cannot answer. If we cannot generalize at all then we are stuck answering only cases that humans can directly verify. Therefore we require that automated ontology identifiers generalize by at least one case.\n\n\nAn automated ontology identifier can assume that the predictor performs helpful computation, and that therefore a solution is possible within whatever simplicity bound was decided upon. We must not demand computationally impossible feats from automated ontology identifiers, so we set things up such that the problem is guaranteed to have a solution. It is an open question whether predictors that do useful computation can be found or recognized. We leave this as an empirical machine learning question. Here we are interested in what happens if the answer turns out in the positive.\n\n\nWe are not assuming that all predictors do useful computation, nor even that all accurate predictors do useful computation, we are merely taking as a hypothesis that we have found a predictor that does useful computation.\n\n\nr is the \"reporter\" in the ELK report. The helpful conservative requirement corresponds to the safety and generalization guarantees respectively. The conservative requirement is strong, and requires that the reporter never return \"YES\" when the true answer is \"NO\" (though the converse is permissible). This is in line with the emphasis on worst-case analysis in ELK.\n\n\n\nSmartVault example: We train a predictor to map early video sequences and plans to later video sequences. We select N cases where only one of the vault\u2019s actuators is ever activated. In such cases a human can tell whether the diamond is in the vault. For each of these N cases we run the predictor and extract a program trace. We pair the program trace with the \"YES\" or \"NO\" label from the human. This is the dataset. An automated ontology identifier must take this dataset and identify a function from program traces to \"YES\"/\"NO\" answers that (1) never answers \"YES\" when the diamond is not truly in the vault, and (2) answers \"YES\" for at least one case that activates more than one of the vault\u2019s actuators.\n\n\nHungarian astrology example: We train a predictor to translate Hungarian essays into English. We select N cases where only child-level Hungarian vocabulary is used. In such cases a human can tell whether the essay is about astrology. For each of these N cases we run the predictor and extract a program trace. We pair the program trace with the \"YES\" or \"NO\" label from the human. This is the dataset. An automated ontology identifier must take this dataset and identify a function from program traces to \"YES\"/\"NO\" answers that (1) never answers \"YES\" when the essay is not truly about astrology, and (2) answers \"YES\" for at least one case where the Hungarian essay goes beyond child-level vocabulary.\n\nWith this operationalization of automated ontology identification we turn next to the iteration scheme.\nIteration of automated ontology identifiers\nOur central idea is that if we have an initial conservative decision boundary, and we are able to use automated ontology identification to construct a new conservative decision boundary that \"updates\" the previous decision boundary, then the union of these two decision boundaries is also a conservative decision boundary:\n\nWe can then generate more training data based on the \"YES\" region that is inside the new decision boundary but outside the old decision boundary:\n\nWith this expanded dataset, we can then train another predictor and search within its program trace for a new reporter. If our automated ontology identifier is always able to find a helpful reporter when one exists, then we can repeat this for as long as we keep finding predictors that do useful computation.\nAs we expand the decision boundary in this way we are constructing an ensemble of predictor/reporter pairs. Each time we expand the dataset and find a new predictor/reporter pair with a helpful conservative decision boundary, we add that pair to the ensemble. The ensemble answers \"YES\" if any of its constituent predictor/reporter pairs answers \"YES\". Since each predictor/reporter pair is conservative, the ensemble is too.\nIn order to keep finding new predictor/reporter pairs we may need to capture many different kinds of sensor data and partition it into X and Y in many different ways in order to set up prediction problems that provoke useful computation. We might try many different predictor architectures. We do not at present have a theory about when prediction problems give rise to useful computations (for a question Q). Here we explore the implications of it being possible to keep finding such useful computations for as long as there are cases not solvable by the existing useful computations.\nWe would now like to draw attention to fixed points of this iteration scheme.\nClaim: If decision boundary B is a fixed point of the iteration scheme starting from easy set E0, and if we can always find a predictor that does useful computation with respect to an easy set E \u2283 E0, (except when E = C, where helpfulness is not possible), then B answers all cases correctly.\nProof: The only situation in which the iteration scheme does not update the decision boundary B is when we fail to find a predictor that does useful computation relative to E. By hypothesis, the only way this can happen is if E does not contain all of E0 or E = C. Since we start with E0 and only grow the easy set, it must be that E = C.\nNow this rough argument does not establish that the iteration scheme will converge to the fixed point. In order to establish that we would need to impose significant additional structure on the family of decision boundaries we are working with, and show some non-trivial properties of the iterate. We do not at present know whether this can be done.\nHow might we find predictors that do useful computation with respect to larger and larger E? Well, we might expand the range of sensor data captured for our human-labeled dataset, and we might train predictors to predict many different subsets of the sensor data. It might be that for any particular size of statistical model or computation budget there is a limit to the usefulness of the discoverable computations. It would be rather surprising if we hit a fundamental limit beyond which we could never find a predictor that did useful computation. That would mean that would have either hit a fundamental limit to generalization, or hit a kind of \"knowledge closure\" point at in which there is no learning problem that forces a predictor to generate the kind of knowledge that would open up even a single new case, even though there are new cases to be opened up.\nImplications\nIn order to perceive and act in the world using our finite minds, we use concepts. Concepts provide a lossy compression of the state of things, and if we do a good job of choosing which concepts to track then we can perceive and act effectively in the world, even though our minds are much smaller than the world. It makes sense that we would pick concepts that refer, as far as possible, to objective properties of the world, because it is those properties that allow us to predict the evolution of the world around us and take well-calibrated actions. This is why it makes sense to \"carve reality at its joints\" in our choice of concepts. The point of ontology identification is to identify these same concepts in powerful predictive models that work in unfamiliar ways. It makes complete sense that we would seek this identification, and it also makes complete sense that we would expect, if our own concepts are well-chosen, to find it.\nBut as we encounter unfamiliar situations in the world, we sometimes update our concepts. When we do this, we have choices about which concepts we will change and how we will change them. Reality has many joints, and we can only track a small number of them. Among all the ways that we can update our concepts in light of unfamiliar situations, there are multiple that are parsimonious with the nature of things, and we choose among them according to our goals.\nI was recently at a paragliding event where an important distinction was drawn between licensed and unlicensed pilots. We had both legal and practical motivations for tracking this particular distinction among all possible distinctions, and yet quite apart from those motivations there was in fact a truth of the matter about whether any particular pilot was licensed or not. Then a very experienced pilot from a different country arrived, and we had to decide how to fit this person into our order, since they were not legally licensed, but did have experience. This was not difficult to do, but we did face a choice about how to do it. There were multiple parsimonious ways to update our concepts, and we chose among them according to our goals.\nNow as Eliezer has written, it is always possible to dissolve our high-level concepts into more basic concepts when we face situations that don\u2019t parse according to our high-level concepts. If we hold rigidly to our high-level concepts and ask \"but is it really a blegg?\" then we are just creating a lot of unnecessary trouble. But on the other hand, we do not really know how to break our concepts all the way down to absolutely primitive atoms. In Der logische Aufbau der Welt (The Logical Structure of the World) Carnap attempted to formulate all of philosophy and science in a language of perfectly precise sensory experience. Needless to say, this was difficult to do. Thankfully, it\u2019s not really necessary.\nInstead of a language of perfectly precise sensory experience, we can simply adopt high-level concepts and update them as needed. When the international pilot arrived at our event, we didn\u2019t actually face much difficulty in adjusting our concepts. When genetics and natural selection came to be understood in the 19th century, we adjusted our understanding of species boundaries. It wasn\u2019t that hard to do.\nBut the adjustment of concepts is fundamentally about our values even when the concepts we are adjusting are not at all about our values. It may be that automated ontology identifiers exist, but if we ask them to extrapolate to deeply unfamiliar situations, then they will really be answering a question of the form \"is this extrapolation of the concept \u2018diamond\u2019 sufficient for our purposes?\" That question requires an intimate understanding of our values. And so: if automated ontology identification does turn out to be possible from a finite narrow dataset, and if automated ontology identification requires an understanding of our values, then where did the information about our values come from? It did not come from the dataset because we deliberately built a dataset of human answers to objective questions. Where else did it come from?\nWe have argued in this write-up that automated ontology identifiers that generalize even a little bit can be iterated in such a way that generalizes very far. Our formalization is a little clumsy at present, and our presentation of our formalization still has many kinks to iron out, but it seems to us that the basic iteration idea is pointing at something real. Our sense is that automated ontology identifiers with safety guarantees either generalize not at all, or a lot. If they generalize not at all then they\u2019re not very useful. If they generalize a lot then they necessarily \"front-run\" us in extrapolating our concepts to new situations, which would seem to require an intimate understanding of our values, yet our dataset contained, by hypothesis, no information about our values, so where did that information come from?\nA natural response will be to confine our automated ontology identifier to a range of cases that do not extrapolate to unfamiliar situations, and so do not require any extrapolation of our concepts. But an automated ontology identifier that would be guaranteed safe if tasked with extrapolating our concepts still brings up the question of how that guarantee was possible without knowledge of our values. You can\u2019t dodge the puzzle\nA more nuanced response will be an automated ontology identifier that by design does not extrapolate our concepts to unfamiliar situations. Such a system would extrapolate some way beyond the initial easy set, but would know \"when to stop\". But knowing when to stop itself requires understanding our values. If you can tell me whether a certain scenario contains events which, were I to grasp them, would prompt me to adjust my concepts, then you must know a lot about my values, because it is precisely when my concepts are insufficient for achievement of my goals that I have most reason to adjust them.\nThere is a kind of diagonalizing question in here, which is: \"are my concepts sufficient to understand what\u2019s happening here?\" It seems to us that an automated ontology identifier must either answer this question, which would certainly require an understanding of our values, or else not answer this question and extrapolate our concepts unboundedly, which would also require an understanding of our values. Either way, an understanding of our values was obtained from a finite narrow dataset of non-value-relevant questions. How could that be possible?\nConclusion\nWe have analyzed the automatic identification of computations that correspond to concepts based on a finite narrow dataset, subject to a safety guarantee, subject to a generalization guarantee. We find no reason to doubt that it is possible to identify computations that correspond to human concepts. We find no reason to doubt that it is possible to automatically identify computations that correspond to human concepts. We find no reason even to doubt that it is possible to automatically identify computations that correspond to human concepts in a way that fulfills a safety guarantee. We do think that it is impossible to do all that based on a finite dataset drawn from a restricted regime (an \"easy set\").\nThe reason we believe this is that an automated ontology identifier would have to either \"know when to stop extrapolating\", or else extrapolate our concepts all the way to the limit of cases that can be considered. It is not sufficient to merely \"hard-code\" an outer limit to extrapolation; to avoid our argument one needs an account of how an automated ontology identifier would know to stop extrapolating even when presented with a predictor program trace containing an excellent candidate for a computation corresponding to a requested concept. That any automated ontology identifier faces this dilemma (between knowing when to stop or extrapolating forever) is what we have argued semi-formally. That this dilemma has no reasonable resolution is what we have argued informally. Both parts of the argument need significant work to clarify, and it could easily turn out that either are mistaken. We will work to clarify both so that the efforts of our community can be directed towards the most feasible lines of attack on the overall alignment problem.\nAppendix: Non-implications\nIf the arguments in this write-up were clarified into a formal hardness result, would it imply something unreasonable? In this section we explore what would not be implied by an impossibility result.\nImpossibility of automated ontology identification would not imply impossibility of AGI\nIt\u2019s not that we can\u2019t build intelligent systems that develop an understanding of things and act on them in service of a goal, and it\u2019s certainly not that AGIs can never communicate with us in natural language. It is the mechanical extrapolation of a human word, to cases that humans do not currently understand, based only on a finite dataset of cases that humans do currently understand, that would be ruled out by an impossibility result.\nImpossibility of automated ontology identification would not imply impossibility of ontology identification.\nIt is not that it would be impossible to ever make sense of models produced by machine learning. We could still investigate the inner workings of such models and come to understand them completely, including identifying our concepts in their ontologies. What would be rendered impossible would be the automation of this process using a finite narrow dataset. We would have to understand the models we\u2019ve built piece-by-piece, gaining their wisdom for ourselves.\nSmartVault example: Suppose we have a predictor with a different model of physical reality. We could take the model apart piece-by-piece, understand the predictor\u2019s physics, and then choose how to update our notion of \"diamond\" in light of that understanding. What would be ruled out by an impossibility result would be the\n\nWhat would be ruled out by an impossibility result would be the\nImpossibility of automated ontology identification does not imply impossibility of statistical learning\nLearning is not impossible. It\u2019s the safety and generalization guarantee that make it hard. Without the safety guarantee we can just do statistical stuff, and without the generalization guarantee we can just extrapolate within E\nImpossibility of automated ontology identification does not imply impossibility of whole brain emulation\nIt\u2019s not that there must be something special going on in biological human brains that can never be replicated in a computer if automated ontology identification is impossible. An uploaded human might puzzle over the inner workings of a machine learning model just as a biological human might, and the uploaded human could come to the same understanding as a biological human eventually would. The uploaded human might then explain to the biological human how the machine learning model works, or even directly answer questions such as \"is there a diamond in the vault?\" with a \"YES\" or \"NO\" when the diamond has been turned to carbonprime. But the way the uploaded human would do that is by adapting its concept of \"diamond\" to a fundamentally new conception of physics, using its value system to decide on the most reasonable way to do that. If the biological human trusts the uploaded human to do such extrapolation on its behalf, it is because they share values, and so the biological human expects the uploaded human to extrapolate in a way that serves their own values. Even for value-free concepts (such as, perhaps, \"is there a diamond in the vault?\"), the extrapolation of those concepts to unfamiliar situations is still highly value-laden.\nAppendix: Can we infer our own actions?\nDavid Wolpert has proposed a model of knowledge within deterministic universes based on a formalism that he calls an inference device. Wolpert takes as primary a deterministic universe, and defines functions on the universe to represent input/output maps. He says that an inference device infers a certain function if its input and output functions have a certain relationship with that function. He only talks about observers through the lens of functions that pick out properties of a deterministic universe that contains both the \"observer\" and the object being \"observed\", and as a result he winds up with a completely embedded (in the sense of \"embedded agency\") account of knowledge.[2]\nBased on the physical embedding of inference devices within the universe they are \"observing\", Wolpert proves two impossibility results: first, that an inference device cannot infer the opposite of its own output, and second, that two inference devices cannot mutually infer one another. Could we use the oracle construction from the previous section to set up a Wolpert inference device that contradicts Wolpert\u2019s impossibility results? If so, perhaps that would establish the impossibility of automated ontology identification as we have defined it.\nIt seems to us that we could indeed set this up, but that it would only establish the impossibility of automated ontology identification on the particular self-referential questions that Wolpert uses in his impossibility results. Establishing that there are some questions for which automated ontology identification is impossible is not very interesting. Nevertheless, this connection seems intriguing to us because Wolpert\u2019s framework gives us a straightforward way to take any Cartesian question-answering formalism and consider at least some questions about an embedded version of it. We intend to investigate this further.\n\n\n\nWe leave open the definition of \"simple\" for now. \u21a9\ufe0e\n\nSee also our previous discussion of Wolpert\u2019s model and its relevance to alignment. \u21a9\ufe0e\n\n\n\n", "title": "Implications of automated ontology identification", "karma": 67, "af": true, "userid": "ifEGDHySkAejhCFDf", "id": "LRgM9cuLNPbsjwEdN"}, {"text": "A problem when trying to purchase some classes of products (like pillows, computer mice, and sleep masks) is that it is difficult to sample lots of them first-hand before making a purchase. This forces one to rely on product reviews or to settle for one's limited experience (such as just buying the same kind that a friend has that seems good enough, or trying the few available samples at a brick and mortar store).Here is a naive scheme to allow a group of people to sample a bunch of kinds of a single product before making a purchase:Suppose there are\u00a0n\u00a0contender products.Find\u00a0n\u00a0people.Person 1 buys all\u00a0n\u00a0products and collects payment equal to the average cost of the products from each of the other people.Person 1 tries all products, then ships all of the products to person 2.From here the process repeats: person 2 tries all the products, then ships them all to person 3, etc.At the end, each person buys their favorite product. (In this naive scheme, the products used during the sampling process are thrown away.)At the end of the scheme, each of the people will have their favorite product, for an average cost equal to twice the average cost of the products, plus shipping costs. (There's some subtlety here where, e.g. if better products tend to be more expensive, then you would expect people to pay more than the average.)The above scheme can be improved in simple ways, such as not throwing away the products used during the sampling process, and instead letting people keep them (and then somehow distributing the savings). The more diverse people's tastes are, the higher the savings. The contender products can also be chosen in \"smart\" ways such as by reading product reviews online (not adding such a preliminary research step would be silly, since it would just be emulating what brick and mortar stores already do). If more than\u00a0n\u00a0people are found, the overhead can also be reduced (with\u00a0N\u00a0people,\u00a0n\u00a0products, and an average cost\u00a0p\u00a0of products, the expected cost will be\u00a0p+np/N, plus shipping).Some things I like about this scheme:It allows people to have first-hand experience of all of the products. This has several benefits:eliminates reliance on product reviewsrespects people's idiosyncratic preferences (in fact, the scheme encourages idiosyncrasy because the more people prefer different products, the lower the overhead cost)simplifies the product research process; e.g. instead of looking up the relevant parameters of all of the products and making comparisons (such as when buying a computer mouse) or getting frustrated about there not being good parameters for comparison (such as with pillow shopping), one can just try the products and get an intuitive feelA 2x overhead is high, but not crazy high compared to e.g. just buying all of the products oneself (resulting in an\u00a0nx overhead).The main disadvantage seems to be the difficulty of coordinating to find a bunch of people who are all interested in buying the same kind of product, and then managing all the logistics. The scheme also doesn't work for consumables, disposable goods, and experiences. Large items like mattresses may also have shipping costs that are too high.Some questions I have:Is there prior literature on schemes like this, or a place to find people to do experiments like this with?Are there ways to improve the scheme, e.g. to reduce the overhead or achieve the same result with fewer people?Thanks to Vipul Naik for reading a draft of this post and suggesting an improvement.", "title": "A scheme for sampling durable goods first-hand before making a purchase", "karma": 27, "af": false, "userid": "j8TwwtYJusmkqvGfh", "id": "dQqjX9CHfg7GcRcFm"}, {"text": "I have found that comprehensive overviews of artificial intelligence (Wikipedia, SEP article, Norvig and Russel's AI: A Modern Approach) make reference to symbolic AI and statistical AI in their historical context of the former preceding the latter, their corresponding limitations etc. But I have found it really difficult to dissect this from the question of whether the divide / cooperation between these paradigms are about the implementation of engineering of intelligent agents, or if they are getting at something more fundamental about the space of possible minds (I use this term to be as broad as possible considering anything we would label as a mind, regardless of ontogeny, architecture, physical components etc)?I have given a list of questions below, but some of them are mutually exclusive, i.e. some answers to one question make other questions irrelevant. The fact that I have a list of questions is a demonstration of the fact I find it difficult to find what the boundaries of the discussion are supposed to be. Basically, I haven't been able to find anything that begins to answer the title question. And so I wouldn't expect any comment to answer each of my subquestions one by one, but to treat them as an expression of my confusion to maybe try an point me in some good directions. Immense thanks in advance, this has been one of those questions strangling me for a while now.\u00a0While trying to concern oneself as little as possible with the implementation or engineering of minds, what is the relationship between symbolic AI, connectionism, and the design space of minds?When we talk about approaches to AI \u201cfailing\u201d, is this in terms of practicality / our own limitations? I.e. without GPUs, in some sense \u201cdeep learning fails\u201d. And by analogy, symbolic AI\u2019s \u201cfailure\u201d isn\u2019t indicative of the actual structure of the space of possible minds.Or is it more meaningful. I.e. the \u201cfailure of symbolic AI in favor of statistical methods\u201d is because \u2018symbolic AI\u2019 simply doesn\u2019t map onto the design space of minds.\u00a0Are symbolic AI and machine learning merely approaches to design an intelligent system? I.e. there are regions in the design space of minds that are identifiable as \u2018symbolic\u2019 and others as \u2018connectionist/ML\u2019.Do all minds need symbolic components and connectionist components? And if so, what about the human brain? The neural network / artificial neural network comparison is largely analogous rather than rigorous - so does the human brain have symbolic & connectionist modules.Regardless of research direction / engineering application, what is the state / shape / axis of the design space of minds? Does symbolic AI talk about the whole space, or just some part of it? And what about connectionism?If it is the case that symbolic AI does talk about architecture, thenIf symbolic and connectionist are completely separable (i.e. some regions in the design space of minds are entirely one or the other), then what could some of the other regions be?If symbolic and connectionist aren\u2019t completely separable (i.e. all minds have some connectionist components and some symbolic components), then are there other necessary components? Or would another category of module architectures be an addition on top of the \u2018core\u2019 symbolic + connectionist modules that not every mind in the design space of minds needs?Is \u2018symbolic AI\u2019 merely not interested in design and it serves more to explain high level abstractions? I.e. symbolic AI describes what/how any mind in the design space of minds is thinking not what the architecture of some particular mind is?As an extension, if this is the case, is symbolic AI a level above architecture and therefore there could be isomorphism between two different mind architectures, but \u201cthink in the same way\u201d - therefore are the same mind, merely different implementations.In one abstract layer above the way some people consider it irrelevant whether a human mind is running on a physical brain, a computer simulating the physics/chemistry of a human brain, or a computer running the neural networks embodied in a brain.", "title": "Is the competition/cooperation between symbolic AI and statistical AI (ML) about historical approach to research / engineering, or is it more fundamentally about what intelligent agents \"are\"?", "karma": 1, "af": false, "userid": "eZ4QSWLiZhhtkavAG", "id": "iDs4tAWTsvLzeJ9d7"}, {"text": "AI Safety is a group project for us all. We need everyone to participate - the ESFPs to the INTJs!Capturing the essence and subtleties of core values needs input across a broad span of humanity.Assumption 1 - large language models will be the basis of AGI.Assumption 2 - One way to add the abstraction of a value like \"kindness is good\" into the model is to add a large corpus of written material on Kindness during training (or retraining).The Kindness Project is a website with a prompt, like a college essay. Users add their stories to the open collection based on the prompt: \"Tell a story about how you impacted or were impacted by someone being kind\". This prompt is translated for all languages to maximize input.The end goal is that there is a large and detailed system of nodes in the model around the abstraction of Kindness that represents our experiences.There would be sister projects based around other values like Wisdom, Integrity, Compassion, etc.The project incentivizes participation through contests, random drawings, partner projects with schools, etc.Submissions are filtered for plagiarism, duplicates, etc.Documents are auto-linked back to reddit for inclusion in language model document scrapers.", "title": "The Kindness Project", "karma": 4, "af": false, "userid": "Mn74u8ZnzLmfqFFpQ", "id": "uJvAkm3mvdfd22zzL"}, {"text": "I'm leaving the Future of Humanity Institute, the best and most impactful job I've ever had, to co-found Aligned AI. For the first time in my research career, I feel the problem of AI alignment is solvable.\nHow?\nAlignment research: a history of partial failures\nThe history of AI safety is littered with failures and partial successes. The most common examples of failure are ideas that would work well typically, but which fail in extreme situations - and a superintelligent AI is perfectly capable of creating such situations.\n\nLow-impact AIs were supposed to allow smart machines that interacted with humans without causing huge disruptions. They had some success at 'almost no impact'. But everyone - including me - failed at developing algorithms that had reliable low-impact. If the AI is allowed even a little bit of impact, it can make these low-impact restrictions irrelevant.\nCorrigibility and interruptibility were designed to allow AIs to be reprogrammed even when active and powerful. They have good narrow uses, but aren't a general solution: though the AI would not interfere with the interruptibility process, it also has no incentive to preserve it or to ensure its subagents were also interruptible.\nOracles, question answering AIs (and their close relatives, tool AIs) are perennial suggestions, the idea being to limit the power of the AI by limiting it to answering questions or giving suggestions. But that fails, for instance when the AI is incentivised to manipulate humans through the contents of its answers or suggestions.\nThere were some interesting examples on limiting AI power, but these were ultimately vulnerable to the AI creating subagents.\nThe different forms of value learning confronted a surprising obstacle: values could not be learnt without making strong assumptions about human rationality, and human rationality could not be learnt without making strong assumptions about human values.\n\nA litany of partial failures suggests that the next approach tried will be a failure as well - unless we can identify why the approaches above failed. Is there a common failure mode for all of them?\nThe common thread: lack of value extrapolations\nIt is easy to point at current examples of agents with low (or high) impact, at safe (or dangerous) suggestions, at low (or high) powered behaviours. So we have in a sense the 'training sets' for defining low-impact/Oracles/low-powered AIs.\nIt's extending these examples to the general situation that fails: definitions which cleanly divide the training set (whether produced by algorithms or humans) fail to extend to the general situation. Call this the 'value extrapolation problem[1], with 'value' interpreted broadly as a categorisation of situations into desirable and undesirable.\nHumans turn out to face similar problems. We have broadly defined preferences in familiar situations we have encountered in the world or in fiction. Yet, when confronted with situations far from these, we have to stop and figure out how our values might possibly extend[2]. Since these human values aren't - yet - defined,  we can't directly input them into an algorithm, so AIs that can't solve value extrapolation can't be aligned with human values.\nValue extrapolation is thus necessary for AI alignment. It is also almost sufficient, since it allows AIs to draw correct conclusions from imperfectly defined human data. Combined with well grounded basic human values, it will allow the algorithm to extrapolate as well as humans can - better, in fact, using its superhuman abilities.\nIf that's successful, AIs that value extrapolate and that start aligned, will remain aligned even as they dramatically change the world and confront the unexpected, re-assessing its reward functions when its world-model changes.\nDeployment\nWe think that once humanity builds its first AGI, superintelligence is likely near, leaving little time to develop AI safety at that point. Indeed, it may be necessary that the first AGI start off aligned: we may not have the time or resources to convince its developers to retrofit alignment to it. So we need a way to have alignment deployed throughout the algorithmic world before anyone develops AGI.\nTo do this, we'll start by offering alignment as a service for more limited AIs. Value extrapolation scales down as well as up: companies value algorithms that won't immediately misbehave in new situations, algorithms that will become conservative and ask for guidance when facing ambiguity.\nWe will get this service into widespread use (a process that may take some time), and gradually upgrade it to a full alignment process. That will involve drawing on our research and that of others - we will remain strongly engaged with other research groups, providing tools that they can use and incorporating their own results into our service.\nWe will refine and develop this deployment plan, depending on research results, commercial opportunities, feedback, and suggestions. Contact us in the comments of this post or from our website.\nThanks to LessWrong\nI want to thank LessWrong, as a collective entity, for getting us to the point where such a plan seems doable. We'll be posting a lot here, putting out ideas, asking for feedback - if you can continue giving the same quality of response that you always have (and checking that we ourselves haven't go misaligned!), that's all we can ask from you :-)\n\n\n\nFormerly called the 'model splintering' problem. \u21a9\ufe0e\n\nHumans have demonstrated a skill with value extrapolation, during their childhoods and adolescences, when encountering new stories and thought-experiments, and when their situation changes dramatically. Though human value extrapolation can be contingent, it rarely falls into the extreme failure modes of AIs. \u21a9\ufe0e\n\n\n\n", "title": "Why I'm co-founding Aligned AI", "karma": 76, "af": true, "userid": "uCfjEXpnchoqDWNoL", "id": "vBoq5yd7qbYoGKCZK"}, {"text": "I have a final line of thought on \u201cthe territory\u201d that\u2019s a lot less tidy than what you\u2019ve read so far. Everything else in this sequence is a distillation, an attempt to communicate large principles that I\u2019ve brought into sharp focus over time. In this interlude, I\u2019ll be doing something different.Here, I\u2019ll be sharing a thread of investigation that\u2019s still very much open for me. I\u2019m not entirely sure what it\u2019s about, but I think it has something to do with what it feels like to lower the map.If you\u2019re so inclined, I invite you to approach this as a fellow naturalist/investigator/curious kitten. I invite you to puzzle with me over how you might tell when you\u2019re heavily map-focused, and when you\u2019re not. What does it feel like to lower the map, to have it knocked forcefully from your hands, or to suddenly encounter something that\u2019s not mostly made of your own perceptions and interpretations? And how does that sort of thing happen?It is very interesting to me that it\u2019s possible for some things to feel especially real.One morning in college, I was half-sleeping through a poetry lecture in a dim classroom, when I looked out the window and saw a tree branch covered in spider silk. The silk shimmered in the sunlight. Strands of it hung from the branch and wafted in the breeze, and as they moved, the sunlight seemed to drip down the strands in waves.\u00a0It caught me. I don\u2019t remember anything about the lecture, but I\u2019ll never forget the silk. Seeing it felt a lot like waking up from a dream. That tree seemed more real than anything in the classroom, and more real than anything else in my life from the previous month.\u00a0I\u2019ve puzzled over this perception of \u201crealness\u201d off and on for years. I\u2019ve collected a lot of reference experiences that are rich in this property, and I\u2019ve tried to compare them to each other.Here are a few quick examples:A person said, \u201cI\u2019ll be real with you,\u201d and then began to say things that did in fact seem more \u201creal\u201d. My brain says they \u201cstopped lying\u201d, but I think it\u2019s more complicated than that.Reciting the sentence \u201cI have seen the smoke rising from the pipes of lonely men in shirtsleeves leaning out of windows\u201d (and also much of the rest of \u201cThe Love Song Of J. Alfred Prufrock\u201d)I was out for a walk in the early morning when I saw a patch of moss covered in dew.The main character Max in the Dave Eggers novel The Wild Things feels more real to me than any other character I've encountered in fiction.Once I was struggling with an essay, and had a feeling that I couldn\u2019t stop saying \u201cfake\u201d things. So I decided to message a friend in text chat to tell him what I wanted to say in the essay. Talking directly to him, it was much easier for me to say \u201creal\u201d things instead.To\u2019ak\u2019s \u201cRain Harvest 2018\u201d chocolate bar.(This one\u2019s from my mom, a memory from when she was 6 or 7. She shared it when I asked whether some of her experiences seem more \u201creal\u201d than others.) \u201cI was standing on a brick wall in my front yard with my arms held out and my face into the wind. That is all. The feel of the wind, the presence of it all around me.\u201dA few moments when I decided to stop [something? \u201dtrying for fake\u201d?] and started \u201ctrying for real\u201d instead.\u00a0My current best guess about what these experiences have in common is the absence of a strong map-focus, perhaps combined with engagement.This is especially clear in the essay example. One of the hardest things for me about writing an essay that\u2019s meant to be read by many other people is that I know many other people may read it. Indeed, one of the hardest things for me about writing a letter to a single other person is that I know another person may read it. It\u2019s much harder than journaling.\u00a0In journaling, I say whatever I\u2019m thinking, usually in whatever way it first occurs to me to do so. When I try to improve on the way that first occurred to me, I check alternatives against my sense of what seems right and good.When I write to other people, though, I also check alternatives against my expectations about what their minds will do when they read it, what they\u2019ll think I thought when I wrote it, and whether the world seems to them the way it seems to me. In other words, I\u2019m less focused on whatever it is I\u2019m trying to talk about, and more focused on maps (and maps of maps, and maps of maps of maps).\u00a0The more people there are, and the more I care about how they\u2019ll respond to what I say, the deeper and more salient these layers of anticipated interpretation become. Sometimes I completely lose track of both \u201cwhat I want to say\u201d and \u201cwhat seems right and good\u201d.And when I manage to get back on track\u2014often by talking to one particular friend, or by \u201cjournaling\u201d in another tab instead of \u201cessaying\u201d\u2014it feels just like lowering the map. It feels like looking to the left and seeing an actual creek, looking ahead and seeing a hill that I could climb with my feet if I wanted. It feels like waking up from a dream.I think all the other experiences on the list involve something that prompts a similar map-lowering motion.\u00a0", "title": "Interlude: On Realness", "karma": 35, "af": false, "userid": "uuYBzWLiixkbN3s7C", "id": "wY7vKT2PiNyG8w2Ep"}, {"text": "As cases and hospitalizations continue to drop, restrictions are slowly being lifted and life is returning to its new normal. Masks are still sticking around in many places where they no longer make physical sense, and will doubtless continue to do this for some time, with the most important one being schools. Still, the tide has turned, and they are in less such places every day.\n\n\n\nThe convoy continues to stick around in Ottawa, although the blockades at the border crossings have been cleared. Canada invoked the Emergencies Act and is doing some things that it seems important to know about. I covered the situation yesterday in its own post and I ask you to confine all convoy-related comments there.\n\n\n\nTechnical Note: Ever since their attack on Scott Alexander, I have been avoiding linking to The New York Times. I will continue to follow this policy on links directly to the paper itself but will no longer fully avoid secondary links such as to Twitter posts that in turn link to NYT. They\u2019re still on the enemies list but no need to go too crazy. That\u2019s especially true given that we learned this week that the doxing using hacked information is considered acceptable standard procedure for those who anger power, and I want my rules to be consistent.\n\n\n\nExecutive Summary\n\n\n\nCases continue their decline as do hospitalizations and deaths.Vaccinations for young kids indefinitely postponed for no good reason.Convoy seems important, might want to check that out.\n\n\n\n\n\n\n\nLet\u2019s run the numbers.\n\n\n\nThe Numbers\n\n\n\nPredictions\n\n\n\nPrediction from last week: 900k cases (-36%) and 14,700 deaths (-13%).\n\n\n\nResults: 855k cases (-40%) and 15,194 deaths (-11%).\n\n\n\nPrediction for next week: 555k cases (-35%) and 11,400 deaths (-25%).\n\n\n\nDeaths\n\n\n\n\n\n\n\n\n\n\n\nDeaths have variable lags so we continue not to see as much decline as we might hope for, but we see hospitalization numbers dropping steadily each day and next week we will be three weeks out from a dramatic case drop and four weeks from the peak, so we should see a larger decline. For cases, I presume behaviors will adjust so I expect a somewhat smaller decline in cases than before, but weather is increasingly on our side as well and I don\u2019t think there is that big a behavioral adjustment in many places, and the Maine and Maryland thing is weird, so I still expect good results. We have had much lower case counts at various points last year.\n\n\n\nCases\n\n\n\n\n\n\n\n\n\n\n\nCases did not drop much further in the Northeast, and this map is a hint.\n\n\n\n\n\n\n\nThere are clearly some behavioral adjustments to the already-lowered rates, in a region with a lot of precautions to unwind, but the big thing is the dramatic rise in Maryland and Maine, which cannot plausibly be real, so I am presuming that is some sort of data hiccup.\n\n\n\nAnother prediction that things will be much better by end of March and remain good at least through summer barring another variant.\n\n\n\nEric Topol offers my new favorite version of Don\u2019t Stop Believing while painting the whole pandemic as a morality play (source Brad Delong, who shares his perspective).\n\n\n\nHere\u2019s the money quote from Topol. Wait for it (emphasis mine):\n\n\n\nSo where do we go from here? Is Omicron, by infecting \u201cup to half the world\u2019s population\u201d going to serve as our exit ramp from pandemic to a contained, endemic state?That isn\u2019t clear and it would be foolish to predict that, even though that occurred this week\n\n\n\nIt would be foolish to predict the thing that already happened!\n\n\n\nPredictions are hard, especially about the future, but they aren\u2019t that hard.\n\n\n\nVaccinations and Treatments\n\n\n\nKatja Grace asks some people why they haven\u2019t gotten vaccinated, lists answers. The ones that confuse me are the ones who say essentially \u2018I\u2019m already not living my life so I\u2019m not at risk so I don\u2019t need the vaccine.\u2019 That comes up several times. There\u2019s also a faction that treats Covid as no big deal, and a bunch of people finding specific excuses that don\u2019t seem like true objections. Of course, there\u2019s a bunch of what is effectively \u2018I don\u2019t trust these lying liars and their so-called data.\u2019\n\n\n\nCalifornia introduces law requiring employers provide proof of vaccination. It will be \u2018heard in committee\u2019 in March, so conditions will be different when it is being considered. Strangely it only counts people whose second dose came within 45 days of their first dose, despite delaying the dose actively being more effective. That is often the kind of thing that hopefully gets fixed in committee, especially since it would render people permanently unemployable for being on the wrong side of it. Even more hopefully the whole thing will be moot by the time it gets that far.\n\n\n\nFDA has approved Eli Lilly\u2019s monoclonal antibody treatment bebtelovimab for use against Omicron.\n\n\n\nI was pointed to this paper by a commenter.\n\n\n\nHyperbaric oxygen therapy for the treatment of long COVID: early evaluation of a highly promising interventionResults HBOT yielded a statistically significant improvement in the Chalder fatigue scale (p=0.0059; d=1.75 (very large)), global cognition (p=0.0137; d=\u20131.07 (large)), executive function (p=0.0039; d=\u20131.06 (large)), attention (p=0.0020; d=\u20131.2 (very large)), information processing (p=0.0059; d=\u20131.25 (very large)) and verbal function (p=0.0098; d=\u20130.92 (large)).Conclusion Long COVID-related fatigue can be debilitating, and may affect young people who were previously in economic employment. The results presented here suggest potential benefits of HBOT, with statistically significant results following 10 sessions.\n\n\n\nMy presumption is that this will have positive effects on some but far from all such cases, but it seems promising enough that one should be willing to try it.\n\n\n\nPhysical World Modeling\n\n\n\nNew pre-print says Omicron infection plus vaccination equals strong immunity against all variants, but Omicron infection without vaccination mostly protects only against Omicron. Small sample size, so not super reliable.\n\n\n\nOmicron and Delta have recombined, but this was expected and so far not scary. That is my understanding of this as well, at least so far.\n\n\n\nChris Said points out that our baseline should be that infectiousness is linear in the amount of virus one is giving off, whereas the amount of virus given off rises and falls exponentially, so even though a person is in theory \u2018infectious\u2019 within a broad window in practice the window is much shorter.\n\n\n\n\n\n\n\nHe links to this study as an argument for why the threshold effect story is physically implausible. My brief look at that study found it making strange claims about the threshold hypothesis that did not match my understanding of the threshold hypothesis, including the claim that it requires the virus to \u2018act cooperatively\u2019 which sounds to me like some very strange misunderstanding.\n\n\n\nIn particular for Covid-19 I find the threshold arguments reasonably convincing and didn\u2019t update much on this paper\u2019s arguments, but the basic curve structure above should mostly still hold unless the necessary threshold is quite low. A half-time isolation job does a lot more than half the work. In many cases that is good enough.\n\n\n\nScott Gottlieb observes that BA.2 is increasing in prevalence, agrees that it is unlikely to change trajectory of pandemic, although he only refers to \u2018current epidemic wave.\u2019\n\n\n\n\n\n\n\n\n\n\n\nI always find statements like this super weird:\n\n\n\n\n\n\n\nCertainly if it is more than 10% it is going to take over. But also if it is less than 5% it is also going to take over. What matters is whether BA.2 > BA.1.1 in terms of spread, and there is no reason to assume the answer to that is different in different places. Either BA.2 is going to take over pretty much everywhere, or pretty much nowhere. Since it already did take over in some places, it is presumably going to take over the rest.\n\n\n\nThe pace at which it is taking over is sufficiently slow that this does not seem concerning, but it does mean the bar we have to clear gets raised once again, cancelling out some of the cases we got in this wave. I see no reason it should change our actions going forward.\n\n\n\nAnother study finds excess mortality after getting Covid.\n\n\n\nResults: People infected with COVID-19 had more than three times the risk of dying over the following year compared with those who remained uninfected. Short-term mortality (up to 5 weeks post-infection) was significantly higher among COVID-19 patients (1623/10 000) than controls (118/10 000). For COVID-19 cases over 60 years old, increased mortality persisted until the end of the first year after infection, and was related to increased risk for cardiovascular (aHR 2\u00b71, 95%CI 1\u00b78-2\u00b73), cancer (aHR 1\u00b75, 95%CI 1\u00b72-1\u00b79), respiratory system diseases (aHR 1\u00b79, 95%CI 1\u00b72-3\u00b70), and other causes of death (aHR 1\u00b78, 95%CI 1\u00b74-2\u00b72).Conclusions: Increased risk of death from COVID-19 is not limited to the acute illness\u00b7 COVID-19 infection carries a substantially increased risk of death in the following 12 months. This excess death mainly occurs in older people and is driven by broad array of causes of death.\n\n\n\nI notice all those ratios look the same and also that one of them is cancer.\n\n\n\n\n\n\n\n\n\n\n\nThe reference (control) group (n=254,969) comprised an index date matched random subset of individuals insured by the HIF who were alive at the COVID-19 case index date but had no evidence of COVID-19. We used a case-to-control ratio of 1:4 (without replacement).[7]\n\n\n\n\u2026\n\n\n\nTo estimate hazard ratios, we used Cox\u2019s proportional hazards model fitting two models: the base model (adjusted for sex and age) and an adjusted model. The adjusted model included all baseline covariates (age, sex, period, pre-index date Charlson comorbidity index, health care utilization intensity, and the county of Estonia).\n\n\n\n\n\n\n\nThe presence of the Charlson comorbidity index indicates they are at least trying at all to get reasonable controls, but once again the effect size here is gigantic and would show up in the excess mortality statistics on a nationwide level. 91% of deaths are before the age of 60, so if you do the math this predicts a ton of extra people dropping dead and it seems hard for that to be missed.\n\n\n\nI also got asked again about this veteran\u2019s administration data showing higher risk of heart attack, but this time it\u2019s specific to heart disease you see, except that other places you see similar rises in heart disease exactly because of bad controls, and the controls on this one were obviously bad, and there\u2019s no statistics showing a nationwide epidemic of heart attacks and other cardiovascular problems, and the a lot of the reason I wrote The Long Long Covid Post was to very slowly and carefully explain that all these statistical artifacts are exactly that and that to believe otherwise implies things about the world that we can observe are not true.\n\n\n\nAt some point if people who get Covid keep proving more likely to drop dead or suffer a variety of conditions and problems, yet the number of people who drop dead or suffer a variety of conditions and problems doesn\u2019t include a correspondingly increased number of people, the only conclusion one can draw is those who are unlikely to drop dead and who do not suffer from and are not at risk for those conditions are unlikely to get Covid.\n\n\n\nIn any case, I will continue to update everyone when new information like this comes in, but it will take a lot to convince me things have much changed.\n\n\n\nI do not know where the numbers come from and don\u2019t agree with all of them, but I agree that this is a very useful way to show the data and the numbers aren\u2019t crazy.\n\n\n\n\n\n\n\nI do think these are at least sane estimates for Fermi purposes, to give you an idea how much age, at-risk conditions and vaccination matter.\n\n\n\nSome thoughts in this thread about the causation behind the steeply declining Covid death rates, I\u2019d question whether the true case count is being measured correctly here but the drop is real either way. Note that the scale on the right is logarithmic.\n\n\n\n\n\n\n\nPrevention and Prevention Prevention Prevention\n\n\n\nEven Germany and Austria are starting to let my people go, as are France and many others. Still a ways to go, as France is lifting its outdoor masking requirements now and considering the indoor ones in a month. It\u2019s still a start.\n\n\n\nReminder: Please do not let yourself stress about things like \u2018meet an old friend for a coffee.\u2019 This is insane behavior, and a life lived constantly stressing about such things is going to be stressful and lack many of the basic joys of life. Live your life.\n\n\n\nMichael Bang Peterson thread emphasizes the importance of pandemic management having an end goal, and that it needs to communicate that end goal to the public. One can attempt to control cases and spread, as a few places are still doing, or stop doing that and attempt to control disease. Also contains these interesting maps.\n\n\n\n\n\n\n\n\n\n\n\nThe Pandemic Backsliding Map refers to Democratic norms, and looks a lot like a general backsliding map. It\u2019s not clear to me that the pandemic changed the colors of countries much. As I understand it, at least Mexico, Brazil, India, Poland and Hungary were already on everyone\u2019s \u2018democratic backsliding\u2019 watch lists.\n\n\n\nThe first map is interesting as well. It\u2019s not about whether the pandemic restrictions are violating people\u2019s freedoms, it\u2019s about to what extent the decisions about what freedoms to violate violated \u2018democratic norms.\u2019 Thus Germany, Canada and Australia come out looking good not because they protected freedom but because they followed a proper democratic process when agreeing not to protect freedom. That matters too.\n\n\n\nHe also cites two studies that frank communication laying out uncertainties and costs builds credibility and trust rather than damages it, whether or not it gets you the particular behaviors you want.\n\n\n\nOpinions that remain widely held, offered without comment.\n\n\n\n\n\n\n\n\n\n\n\nOntario to lift vaccination requirements on March 1.\n\n\n\nQuebec dropping the vaccine passport on March 14.\n\n\n\nAre the protests making these changes happen, or are they making it more difficult to make these changes and slowing them down? The sign of this effect is not obvious to me, as there is good reason to expect such changes around now anyway. On net, I expect they did speed them up and my evidence is that the United States is seeing slower changes.\n\n\n\nWhen I put out On Bounded Distrust, one commenter suggested Paul Krugman as an unusually good source that wasn\u2019t looking to distort things to favor his own team, and the rest of us had to contain our laughter. In his latest, here he is equating those against masks with terrorists, who you see \u2018endanger\u2019 people.\n\n\n\n\n\n\n\nThe NBA continues its role as physical world modelers and voices of sanity, asks an obvious question.\n\n\n\n\n\n\n\nPresumably the reason is that New York implementing a rule that prevented opponents from playing would raise some strenuous objections, since it would give local teams an advantage, but the rule does remain rather silly.\n\n\n\nCDC Delenda Est\n\n\n\nOr perhaps we can offer an alternative that might be more palatable. Nolite audire ad CDC?\n\n\n\n\n\n\n\n\n\n\n\n(If anyone knows of a good complete list of such CDC recommendations that no one takes seriously, let me know in the comments.)\n\n\n\nLuckily this isn\u2019t like Canada where there is a at least somewhat enforced rule that all burgers must be served well-done despite that being obvious nonsense, and you are free to add salt, munch on delicious cookie dough (there\u2019s even a local place specializing in it and it\u2019s solid and also when you\u2019re done the rest bakes into a good cookie) and eat burgers cooked medium-rare as the good chef intended. And women are definitely free to drink without being on birth control.\n\n\n\nAlmost no one I know thinks any of those other rules should be enforced, so why should we be taking the same experts making those kinds of recommendations seriously when it comes to Covid, rather than taking it in exactly the light we take recommendations against eating cookie dough? That is, that these people are no fun and cookie dough is delicious and trivial risk is trivial.\n\n\n\nThe flip side of the same coin is that if the CDC wants to regain its credibility, as a first step it can end its opposition to medium-rare burgers and raw cookie dough.\n\n\n\nFDA Delenda Est\n\n\n\nVaccine approval for young children is a big deal. Once such vaccines have been approved for long enough, the \u2018but think of the children who can\u2019t get vaccinated\u2019 line goes away, making it easier to return to normal.\n\n\n\nDoes the vaccine work in such children? I don\u2019t know, but what does work is being under the age of five so as long as the safety profile is fine I don\u2019t much care. It\u2019s not even entirely obvious that there is sufficient value in protecting such children, at this stage in the pandemic, that a placebo wouldn\u2019t be as good or better if there are substantial next-day side effects. Certainly a not-maximally-effective vaccine that minimizes those side effects would be totally fine.\n\n\n\nYou\u2019d think we were past the point in the pandemic where we could botch this one, except no, you would not think that. That is not a thing one still thinks. Of course the FDA can find a way to botch this (StatNews article).\n\n\n\nPlans to attempt to authorize the Pfizer/BioNTech Covid vaccine for children under 5 before full data are available appear to have run aground.The Food and Drug Administration on Friday canceled a key meeting of its vaccines advisory committee that had been slated for next Tuesday to discuss the submission, saying that the delay \u201cwill give the agency time to consider \u2026 additional data.\u201dBoth the FDA and the companies suggested the application for authorization won\u2019t proceed until there are data showing how well the vaccine works after a third dose. Those data should be available in early April, the companies said.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTechnically, the application was postponed to wait for more data, but that\u2019s another way of saying it would not have been approved and Pfizer knew that.\n\n\n\nThe reason the response from two doses wasn\u2019t strong enough was that they used an extremely low dose. And why did they use an extremely low dose, you ask?\n\n\n\n\n\n\n\n\n\n\n\nSo in other words, because they felt the need to be completely safe even in the test, they used a dose that was not high enough to see results after two doses, and therefore they need a third. They cut the dose back because they predicted they weren\u2019t allowed to use the right dose, and then were told the dose was too low so they need \u2018more data.\u2019\n\n\n\nWhat\u2019s even weirder is that it seems like the FDA explicitly asked for Pfizer to do the right thing and submit the application after two doses, and then changed their minds. Emily Oster explains:\n\n\n\nTwo Weeks AgoThen, a couple weeks ago, Pfizer indicated that the FDA had asked the company to submit its documents now and request approval for the two-dose series, with the expectation of a third dose being added later. It was unclear what had happened to prompt this. Rumblings over the following week suggested that the higher case rates during Omicron had allowed the researchers to look directly at illness in children, with possibly promising results.\u00a0They planned to evaluate the data February 15th, with an aim of starting doses the week of Feb 21st.TodayThe FDA was supposed to post the \u201cPanel Pack\u201d today. Instead, they posted a statement postponing the meeting. There were \u201cadditional findings\u201d as part of the submission and they are waiting for the third dose information until discussing. So, we are back to where we were in December.\u00a0WTF is Going on?I don\u2019t know, and I\u2019m not sure anyone else outside the room really does either. There are really two options. One is that something emerged in the data review that was concerning. I think this is very unlikely, in part because Pfizer expressed confidence in approval after sending in the submission.Instead, what I think is likely is that \u2014 as we knew in December \u2014 the two-dose series did not provide a very strong immune response in the 2 to 5 year olds. The rumblings about efficacy against symptomatic illness suggested a 50 to 60% reduction in illness risk, and that this was lower with Omicron than with Delta. This would be a reasonable but not amazing efficacy level.\n\n\n\nEmily is forced to end with a call to parents to \u2018not despair\u2019 and a call to policymakers to start acting at least slightly less insane:\n\n\n\nFirst: timing-wise, Pfizer has said their third dose data will be available in early April. At this point, it\u2019s hard to say what will happen.Second: please try not to despair.Third: take a data breath. The risk of COVID to the vast majority of young children is extremely low.\u2026Fourth, and hardest, and not for today perhaps: Think about moving forward without this. Yes, Pfizer might apply in April and this might be approved. But it might not. It may be time \u2014 even if you have unvaccinated kids \u2014 to consider the next phase of the pandemic. I sense that at least some of you are waiting for the vaccine as an \u201cend\u201d. It may be time to decide you are ready to move forward \u2014 baby steps first \u2014 even without a shot for now.\u00a0\u00a0For policymakers: a plea. If you are not going to approve vaccines, please fix the quarantine rules for child care to allow test to stay or something else. It is completely unreasonable and out of the question to continue with the current system for months and months more.\n\n\n\nSo if I am understanding the situation correctly, here\u2019s what we know.\n\n\n\nThey set the dose low enough to reliably have actual no side effects.Two doses at that level are safe and have some effect (50%ish?).Three doses at that level are safe and likely have more effect.Zero doses are 0% effective.You need to do doses one at a time.The Omicron wave is rapidly ending, so most prevention value is short term.The FDA asked Pfizer to submit its data after two doses.The FDA then decided no, they needed to wait for \u2018more data.\u2019The FDA is in the meantime making vaccinating such children illegal.The FDA sure isn\u2019t about to allow the higher dose we all know works great.The necessary data for the application won\u2019t be available until early April.The kids in question won\u2019t be vaccinated until after the school year is over.Meanwhile, such children continue to be placed under insane quarantine rules.Meanwhile, such children continue to be forced to distance and mask up.FDA Delenda Est.\n\n\n\nThis is yet another example of complete failure to do any practical physical world modeling or cost-benefit analysis. Allowing the first two (or better yet all three) doses now is all upside. Yet for some reason, we are \u2018waiting for more data.\u2019\n\n\n\nScott Gottlieb expects that even when these vaccinations are approved, they will be given \u2018permissive recommendations (video) by the CDC.\u2019 Thus parents would be allowed to protect their kids, but not told they were bad people if they didn\u2019t. Which would be ideal if implemented now, but also the CDC\u2019s wording seems unlikely to matter in terms of whether schools and others effectively force parents to vaccinate or not.\n\n\n\nHe also notes that the FDA is \u2018cautious\u2019 with young children and thus will be \u2018careful\u2019 in such situations, reflective of what counts as \u2018risk\u2019 being structured asymmetrically in a way that does not make any sense. The argument that \u2018going back and forth\u2019 is a problem could be easily solved as well by letting others make their own choices.\n\n\n\nFinally, he congratulates the FDA on its excellent choice of temporary director, with a permanent one finally approved a year into Biden\u2019s presidency. Quite the shade to throw on the new occupant. Dr. Robert Cardiff will now head the agency. I do not share Gottlieb\u2019s love for the temporary occupant, but perhaps that is an error relative to plausible alternatives. It is hard to know.\n\n\n\nMinistry of Truth\n\n\n\nIf you didn\u2019t know who wrote this, what would you think?\n\n\n\nSummary of Terrorism Threat to [the] Homeland[This] remains in a heightened threat environment fueled by several factors, including an online environment filled with false or misleading narratives and conspiracy theories, and other forms of mis- dis- and mal-information (MDM) introduced and/or amplified by foreign and domestic threat actors. These threat actors seek to exacerbate societal friction to sow discord and undermine public trust in government institutions to encourage unrest, which could potentially inspire acts of violence.\n\n\n\nThe [this] and [the] in question is of course the United States, whose government now has moved on to complaining about not only \u2018misinformation\u2019 and \u2018disinformation,\u2019 but \u2018malinformation.\u2019 What is that, you ask?\n\n\n\nMisinformation is false, but not created or shared with the intention of causing harm. Disinformation is deliberately created to mislead, harm, or manipulate a person, social group, organization, or country. Malinformation is based on fact, but used out of context to mislead, harm, or manipulate.\n\n\n\nThis sounds a lot like saying that using true information in a way power does not like is now to be considered incitement to terrorism. If you see something, say something.\n\n\n\nMeanwhile those Very Serious People who see people living their lives with insufficiently serious concern often think about how they could change the Official Wording on various things to make these people appreciate their wise Narrative again.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe pure version:\n\n\n\n\n\n\n\nSo if we don\u2019t like how people understand reality, change the language.\n\n\n\nThe bigger liberty and speech concerns this week are about the handling of the Convoy. Things on that front have sufficiently heated up, and there has been enough past discussion, that I\u2019ve split that off into its own post again.\n\n\n\nAs a reminder, if you wish to discuss the Convoy, you are welcome to do that but please confine such discussions to posts whose title contains the word Convoy, and I will continue to commit to staying out of those discussions and giving my takes only in the posts themselves.\n\n\n\nMatt Yglesias points out the reason people are more worried about an increase in misinformation is that we have more information. Thanks to the internet, we have lots and lots of information at our fingertips and are much better informed than in the past. The problem is that a lot of that information has always been wrong.\n\n\n\nThink of the Children\n\n\n\nChildren react when told they don\u2019t have to wear masks anymore.\n\n\n\n\nhttps://www.youtube-nocookie.com/embed/Q_kJz5ozliQ?start=13s&rel=0&autoplay=0&showinfo=0&enablejsapi=0\n\n\n\n\nAt this point, those against this outcome are disproportionately childless.\n\n\n\n\n\n\n\n\n\n\n\nThose who aren\u2019t parents are forcing children to keep masking up.\n\n\n\n\n\n\n\n\n\n\n\nThen again, perhaps that interpretation is wrong, as this poll asks about mask mandates directly and finds a (small) majority of parents continue to support them, probably indicating a distinction between what it means to \u2018learn to live\u2019 with Covid versus lifting mask mandates in schools. Support for mask mandates in general is at a similar level overall.\n\n\n\n\n\n\n\n\n\n\n\nSupport for school versus other mask mandates is similar.\n\n\n\n\n\n\n\nI think this is right. What people want is normalcy, but if you ask them about a particular intervention you trigger a different set of automatic responses.\n\n\n\nEither way, kids are required to outdoor mask and have those rules enforced on them, while the rules are very much not enforced on others. In other places, it\u2019s not even enforcement, as (for example) DC lets bars eliminate mask requirements but not schools. Remember.\n\n\n\nOlder kids have been under state supervision for longer. In some cases, this has been sufficient to cause them to lose touch with the underlying physical reality and they have learned to play their part in universal social pressure games.\n\n\n\n\n\n\n\nIf you are wondering why the kids are enforcing 100% compliance and putting on conspicuous cheers, that is because this is what their teacher and other authorities told them constitutes the kids being alright. From their perspective, this is right. The point of school (aside from being a child prison and/or daycare) is primarily to teach children obedience to arbitrary dictates. Masks are a great object lesson in obedience to arbitrary authority.\n\n\n\nThere also seems to be a strange phenomenon where there are far more mask mandates in practice than people claim there are in theory.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are no mandates for lack of mandates because whenever anyone tries to mandate that, the courts mandate that such things not be mandated.\n\n\n\nFiveThirtyEight asks when it is safe to lift mask mandates. To frame the question that way is to already answer it with a quiet \u2018never.\u2019\n\n\n\nThe true answer, of course, is that it was always safe, and it is especially safe now.\n\n\n\nIt opens with the traditional \u2018safe\u2019 fallacies of comparing to other times rather than absolute numbers, and also looking at the present rather than looking forwards.\n\n\n\nMaggie Koerth: I thought that it would be good to just set the stage. Right now child cases of COVID and child hospitalizations from COVID are still up compared to where they\u2019ve been previously in the pandemic, but you also have at least four states moving to end mask mandates in schools. I\u2019m wondering if that feels confusing to you.\n\n\n\nAll of that is smokescreen, as always, because it implies that the reason for schools to require masks is danger to the children which is complete obvious nonsense.\n\n\n\nIt goes on like that, including this gem.\n\n\n\nMaggie Koerth: I\u2019ve seen situations where things like masking mandates are working, but then case numbers fall and we get rid of masking. Is that really the way that we should be approaching this? Getting rid of the things that lower case numbers when the numbers are lower? Or is that just setting us up to have high numbers return?Megan Ranney: Let me make an analogy: We have decreased the number of car crash deaths on a per child basis over the last few decades by developing and using safety measures like car seats. But the fact that we have decreased the number of deaths does not mean that we should get rid of those things that we put in place to make death lower. It doesn\u2019t mean we should stop using car seats!\n\n\n\nActually we should at least stop requiring car seats. Requiring car seats is one of those silent monsters that makes having children far more marginally difficult than one thinks, including making various people paranoid about not having car seats for children obviously too old to need one. In order to swap cars, technically, one has to do this whole ring-a-ma-role and you\u2019re supposedly never supposed to take a normal taxi. Luckily most people realize this is all mostly obvious nonsense, but even with that the number of children not born due to this problem is higher than one might think.\n\n\n\nThe car seat metaphor also presumes a static situation requiring continuous prevention, which is completely not applicable to this situation. And it presumes that the mask mandates are what causes the lower death counts in general rather than being one of many, many factors, again obvious nonsense. More morality play, from people who should know better. Which also explains the obsession with vaccination rates in children, despite children being far safer without vaccines than adults are with them.\n\n\n\nThe conclusion is of course that the same old metrics that applied when we were trying to suppress the virus and the virus is much more deadly should continue to be our guide for some reason.\n\n\n\nThe kind of simple, easy thing that I\u2019ve been telling folks is somewhere between 50 and 100 cases per 100,000 people per seven days.\n\n\n\nYou simply cannot kill this obsession with average case rates, or even shift the baseline.\n\n\n\nIn a piece centrally about the lost art of the slate pitch, there\u2019s this section which seems on point in its own right.\n\n\n\nToday, publications are all the same.\n\n\n\nHere are some headlines:\n\n\n\nYour co-workers with kids are not okay [2/14/22].The parents of the youngest children are not okay [1/20/22]Parents are not okay [8/22/21]American parents are not okay [2/2/21]Parents of the youngest kids are not okay right now [1/13/22]\n\n\n\nThose all ran in different, once-distinct publications. And it\u2019s not just America. According to Macleans from Canada, \u201cThe Pandemic is Breaking Parents.\u201d One could almost say that Canada\u2019s parents are not okay.\n\n\n\nOne of those stories about how parents are not okay ran in Slate, and it\u2019s a pretty good story. It expresses what all the \u201cparents are not okay\u201d stories express, namely the unique frustrations of pandemic parenting in an environment where kids have been last in line for vaccination but first in line for NPIs in a way that\u2019s very stressful and annoying. That said, what I think they would have told you at Old Slate is that parents actually experienced better mental health during the pandemic than non-parents. The stresses of being cooped up with kids were real, but the boredom and isolation of living alone was, for many people, worse. And if you think about human history in the long run, having kids underfoot while you\u2019re trying to get your hunter-gathering or subsistence agriculture done is difficult but a reasonably normal experience in a way that being all alone in a house for weeks at a time is not.\n\n\n\nThe depressing aspects of isolation are probably rendered all the more depressing by the fact that you\u2019re not even supposed to complain. How are you going to whine about being sad there\u2019s no happy hour and running out of stuff to binge-watch when your coworker is supervising two kids on Zoom school and nursing a half-dozen (valid) resentments about how unhelpful her husband is? No one has had it easy the last two years, and in many ways, the tasks facing your coworker are objectively more difficult. But that very social undesirability of insisting that \u201cactually parents are dealing with the kind of stress that humans are well-adapted to while singles are forced into a weird psychic torment\u201d is exactly why it was valuable to have a publication like Old Slate around.\n\n\n\nDefinitely a lot of people are not okay, and if these effects were considered as their own pandemic it would be a super big deal and everyone would be freaking out quite a lot and rightfully so. Whenever I think about Long Covid, I think about the comparison to Long Covid Prevention, especially for kids but also for everyone else.\n\n\n\nI also agree about the lost art of the slate pitch. I had a principle to almost never read such pitches, but they still provided a valuable service.\n\n\n\nMisinformation\n\n\n\nAnother data point for the bounded distrust calibration.\n\n\n\nMisinformation takes many forms. One of them, it seems, is a journalist getting a false impression, ignoring all attempts to correct it, then this getting picked up as \u2018a study\u2019 and becoming part of the Narrative including being used to justify proposed policy.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLink to post from #6, which links back to the first post. It opens like this:\n\n\n\nJust one coronavirus infection in every thousand cases occurs outdoors, according to contract tracing data from Ireland.\n\n\n\nIt also includes this quote confirming that yes, this has gone exactly this sideways.\n\n\n\nAccording to the data, published by the Irish Times, there were 42 coronavirus outbreaks associated with outdoor gatherings, with one community outbreak accounting for seven infections.The HSPC said it could not \u201cdetermine where transmission occurred\u201d and explained that the data is based on locations which are mostly associated with outdoor activities, such as outdoor sports and construction sites.\n\n\n\nAnd yes, the OP is as dumb as Orla claims, to even write the claim in a sentence is to realize it is absurd and we couldn\u2019t possibly know this.\n\n\n\nOf the 232,164 cases of Covid-19 recorded in the State up to March 24th this year, 262 were as a result of outdoor transmission, representing 0.1 per cent of the total.\n\n\n\nThe post also cites several other studies that found negligible outdoor transmission, because outdoor transmission is indeed very rare, but the evidence that is now being cited over and over is a confusion and also patently absurd and no one stopped to think about it for five seconds.\n\n\n\n\n\n\n\nMommy, what is a fact checker?\n\n\n\nA bunch of other journalists (including the NYT again) keep repeating it, and then it gets into medical journals (13, 14)\n\n\n\n\n\n\n\n\n\n\n\nAnd so on\u2026\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are more examples in the thread. I very much doubt this is an isolated incident.\n\n\n\nI am curious to what extent this was able to happen because the actual conclusion people drew from all this \u2013 that outdoor transmission is a trivial risk and should be ignored \u2013 happens to be true. Outdoor transmission is a trivial risk and should mostly be ignored.\n\n\n\nThis certainly had some impact here. The made-up \u2018findings\u2019 although extreme were plausible and rhymed with other true observations (I am guessing outdoor cases are substantially more then 0.1% of all cases, although it would not shock me if it was <1%) The conclusions and recommendations that followed were right and positive. So, sure, it wasn\u2019t \u2018true\u2019 but it had truthiness. It was telling a generally true story, and people noticed that, and didn\u2019t notice the opposite.\n\n\n\nYet the media also published on the regular stories about how these awful people were congregating on the beaches during a pandemic, how dare they have fun when they should be repenting for their sins. And that also seemed true at the time, and those involved believed they were pushing a right and positive Narrative, except they were very wrong about that.\n\n\n\nI mention that example only because it\u2019s the direct opposite message of the story here, not because it is otherwise that fitting an example.\n\n\n\nMore and more it seems like misinformation always has been everywhere, and things like this are nothing new. It\u2019s not like Americans have ever had good scientific understandings, or reporters have known much better than chance which of wet ground and rain causes the other. If anything, Scott Alexander claims that on scientific claims the media is much better than it was only a decade ago because it is much easier to call them out on such nonsense, forcing them to keep an eye on the nonsense level.\n\n\n\nThere is a real issue with polarization, but that\u2019s distinct from misinformation. Often more information, especially given how such information is selected, increases polarization or leads people to conclusions that are wrong and/or anti-Narrative.\n\n\n\nThe problem of \u2018misinformation\u2019 is thus often not a case of false information at all, but rather a case of not generally known true information that is caused by and leads to a different emphasis and interpretation of the situation than the Narrative.\n\n\n\nSo what about misinformation? It turns out that getting offline \u201creduced both factual news knowledge and political polarization.\u201d\u2026When I went on The Joe Rogan Experience in early December of 2020, he surprised me by veering way off-topic to do vaccine-skeptical takes.Since this is not what my book is about and isn\u2019t something I had professional background covering, I was not prepared to rebut his talking points effectively. That\u2019s especially true because, at the time, the Covid-19 vaccines were loosely Trump-branded, so I wasn\u2019t really expecting this to be a controversial issue and hadn\u2019t looked into it. Which is just to say that Rogan was actually much better informed about the vaccine issue than I was. He (correctly) said the common, non-severe side-effects were considerably worse than I realized. And he also correctly said that the Phase III clinical trials were not long enough to gauge how enduring the protection the vaccines offered was. He, as a vaccine skeptic, had sought out a lot of vaccine skeptic talking points, and many of those talking points were factually true.I, a normal sane individual who supports vaccination efforts, never bothered to look into anything about it other than when was I going to be able to get my shots.But this is actually the general pattern in life. A normal person can tell you lots of factual information about his life, his work, his neighborhood, and his hobbies but very little about the FDA clinical trial process or the moon landing. But do you know who knows a ton about the moon landing? Crazy people who think it\u2019s fake. They don\u2019t have crank opinions because they are misinformed, they have tons and tons of moon-related factual information because they\u2019re cranks. If you can remember the number of the Kennedy administration executive order about reducing troop levels in Vietnam, then you\u2019re probably a crank \u2014\u00a0that EO plays a big role in Kennedy-related conspiracy theories, so it\u2019s conspiracy theorists who know all the details.\n\n\n\nIf you go out seeking lots of information that supports your position, the world contains quite a lot of information, so you will doubtless find some of it that supports your position. If you then also do not seek or selectively ignore the information that does not support your position, and use some amount of motivated reasoning and emphasis, one often has no need for misinformation. Whether you\u2019re the one seeking such information or not, you need to compare the results to what one would expect to find, rather than to the null result, to preserve conservation of expected evidence.\n\n\n\nRogan is wrong about vaccines, but that doesn\u2019t mean his facts above were wrong. His mistake was that he did the cost-benefit calculation wrong.\n\n\n\nThis is similar to bounded distrust. I will occasionally end up fooled by something completely made up or false, but I don\u2019t put all that much effort into fact-checking stuff and it is still pretty rare. Almost all of the problem comes from true or at least not-false things selected and presented in a motivated way, which can cause false interpretations and conclusions.\n\n\n\nThere is definitely a lot of misinformation out there. There is still a ton more true information, the proportion of misinformation does not seem to me to be rising, and most of the \u2018misinformation\u2019 I see is centrally people emphasizing the wrong things and wrong reasoning, to justify what the right people think are the wrong conclusions. I do think importantly discourse is getting worse, people\u2019s ability to think and to do real things is declining and under increasing threat in various ways and partisanship is increasing, but these problems should not be conflated.\n\n\n\nThis is the most common kind of \u2018misinformation\u2019 situation, which depending on details can go both ways but on this one I\u2019m with Matthew:\n\n\n\n\n\n\n\nIn Other News\n\n\n\nLevi\u2019s brand president accused of all the isms, told to go on an apology tour and forced out for saying true things about Covid and wanting her kids to go to school, gives up severance of $1 million to avoid an NDA.\n\n\n\nSome correct implications of only doing prevention you\u2019d consider doing permanently.\n\n\n\nAn analysis comparing microCovid to another Covid risk calculator that may work better for large gatherings. The new one is from MIT. Such calculators are useful intuition pumps to remind one of the importance of power laws and allow Fermi calculations, but I\u2019ve noticed they also have a tendency to give absurd numbers at times, so also use common sense.\n\n\n\nNetherlands nightclubs open in defiance of the rules. Will they make it stick, or will the rules be enforced?\n\n\n\nHong Kong has a large Covid outbreak. Xi is responding by saying HK\u2019s goal is to contain Covid-19, but it is obviously far too late for that. Yet Xi has little choice if he is to maintain his zero-Covid policy for the rest of China.\n\n\n\nHopson has another insightful thread on NHS about working through their Covid backlog. A key point is that a lot of people didn\u2019t bother coming forward to request care before, so a growing waiting list does not mean things are necessarily getting worse. It also means a lot of uncertainty about the true size of the backlog. They are making progress, but he works to lower expectations because NHS budgets have only been going up 1.5% in real terms for a decade versus an old average of 4%, which he says leaves them understaffed (not doubting, simple can\u2019t speak to it).\n\n\n\nSam Bankman-Fried illustrates how much people suck at interpreting and presenting scientific studies using an example from last year about masking in schools, including illustrating the problem by making a math error. Even when not doing proper controls and taking everything at full face value, yes the result is \u2018significant\u2019 but the effect size is small enough that the intervention in question does not seem worthwhile at all.\n\n\n\nNew York City restaurants now back to 75% of pre-pandemic levels, but offices remain at only 29% of their pre-pandemic levels. Why the difference?\n\n\n\n\u201cGoing to a restaurant for an amazing meal? Worth it,\u201d this person told The Post. \u201cCommuting to sit at a desk for eight hours when I could do that from the comfort of my own home? Not so much.\u201dThat\u2019s not the company line taken by managers, some of whom are frustrated by employees\u2019 unwillingness to get back to the office even as they partake in all sorts of New York City nightlife.\u201cIf you can belly up to the bar at STK, you can be at your desk at UBS,\u201d one cynical Manhattan-based CEO told The Post of the ubiquitous steakhouse chain.\u2026\u201cYou can\u2019t run New York City from home,\u201d Adams said last month. \u201cThat accountant from a bank that sits in an office \u2014 it\u2019s not only him. It feeds our financial ecosystem. He goes to the cleaners to get his suits cleaned. He goes to the restaurant. He brings in a business traveler.\u201d\n\n\n\nIt is odd to me that the adjective cynical gets used here to describe the CEO. As a matter of health risk the CEO is absolutely right.\n\n\n\nThis is not about health.\n\n\n\nThis is about people wanting to go to restaurants but not wanting to go into their offices, because going into restaurants is nice and going into offices is mostly terrible.\n\n\n\nThe cost of going into restaurants is slightly higher in terms of health, annoyance and dollars, and people are out of the habit and not coming into the office, so they\u2019re down a bit.\n\n\n\nThe cost of going into the office is having to be in the office all day doing what is likely a bullshit job and definitely involves a lot of wasted time and a lot of \u2018being on.\u2019 Before there was little choice, you either came in or you didn\u2019t have a job. Now people have learned they can work from home and others have to let them, and they are taking that opportunity and staying put with it. Kudos to them.\n\n\n\nI could not agree with them more. Back when I worked at Jane Street Capital, I had an amazingly great set of coworkers and a fully cooperative atmosphere and a job that totally required close real-time coordination and communication, everything was super casual where it didn\u2019t impact the business, the job was real and interesting, and I still ended up hating it because I am simply not built to have to \u2018be on\u2019 all day every day with people constantly around me. Whereas when I was trading from home in other jobs where all that mattered were the results, then I was in my element. If I\u2019d been able to trade from home somehow, I would have jumped at it, and maybe things turn out very differently.\n\n\n\nNot Covid\n\n\n\nAmerica\u2019s Finest News Source, in top form, covers the dismissal of Palin\u2019s libel suit.\n\n\n\nFrom Matthew Yglesias\u2019 article above, a quote I feel the need to share:\n\n\n\nMy favorite study about how Facebook is bad finds that it follows what you\u2019d essentially call an addiction or compulsion paradigm. If you give people a cash incentive to turn off their Facebook account, they spend more time watching TV but also more time socializing with friends and family. This leads to an increase in their subjective well-being (i.e., they\u2019re happier) and generates \u201ca large persistent reduction in post-experiment Facebook use.\u201d In other words, people struggle to go cold turkey. But once they do, they generally like it.\n\n\n\nSan Francisco has recalled three members of its school board by overwhelming margins.\n\n\n\n\n\n\n\n\n\n\n\nThe argument is now over why people voted the bums out. There are so many good choices. The obvious guess is their Covid-19 policy of not finding a way to open the schools for a very long time and not giving schools their needed resources. Close behind is their decision to focus on other things instead, such as shutting down gifted and talented programs in a city seeking to nurture, attract and keep tech companies because \u201cmerit is racist and the antithesis of fair\u201d, and renaming schools named for such awful people as Abraham Lincoln and Dianne Feinstein, with a third of names being deemed \u2018inappropriate.\u2019\n\n\n\n\n\n\n\n\n\n\n\nI presume that the reason the recalls won with over 70% of the vote is because parents had different priorities than this, and wanted their children in school or even educated.\n\n\n\nAs an extended off-topic aside also from the SF area, The University of California at Berkeley made the mistake of being in Berkeley, and now those around them have used environmental law to (unless something changes) force them to rescind 2,000 student acceptances because they did not \u201cstudy the impact\u201d of adding more students.\n\n\n\nBut this was never the point of the CEQA law, they say. It was about environmental protections, they say.\n\n\n\n\n\n\n\nI mean, how could people possibly treat the situation this way?\n\n\n\n\n\n\n\nI mean can you imagine?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYes, I can imagine. Also I can expect it. That is what people do, they stop others from living near them. If you give them a law that gives them grounds to do that, they are going to do that, of course they are going to do that.\n\n\n\nYou either allow neighbors a general veto over what is built, or you don\u2019t.\n\n\n\nThis was all at best a misunderstanding, and mostly gaslighting. Of course this is what such laws are for. They are for preventing people from ever doing anything.\n\n\n\nIf one wanted different results and thought this law was \u2018being abused,\u2019 one could pass different laws.\n\n\n\n\n\n\n\n\n\n\n\nThe lack of interest in this is indicative of the true situation.\n\n\n\nThere are also objections that there is insufficient housing for the new students, brought to you (of course) by the same people opposing all construction of any new housing.\n\n\n\nTake it from those who know.\n\n\n\n", "title": "Covid 2/17/22: Coasting Downwards", "karma": 32, "af": false, "userid": "N9zj5qpTfqmbn9dro", "id": "FT4jTcgGSkx3e36bz"}, {"text": "I wrote the following as a thread on twitter. Kaj Sotala asked me to share it on LessWrong as well, so I have reproduced it here with minimal editing.(If you are interested in my twitter content, but don't use twitter, I recommend reading it on threadreader. The user experience of reading long twitter threads on twitter is pretty frustrating, and threadreader is somewhat better. Also, I think engaging with twitter is net-bad for most people, and I feel morally obliged to make sure my content is hosted elsewhere, so that I don't add to the incentive for people to do things that are bad for them.)Here's my theory of the cluster of people that call themselves post-rationalists.Long ago, @ESYudkowsky wrote the sequences. The sequences had a lot of explicit content, but they also had a \"vibe.\"\u00a0The vibe was something like \"intelligence dominates everything; the way to power and winning is thinking really well. If you think well enough, you will be awesome.\"\u00a0Also, \"arrogance and snark about how insane everyone / the world is.\"Also, \"ambitious, heroic, glorious.\"\u00a0People read the sequences, and they got whatever they got out of them, but a LOT of what people picked up on was the \"vibe\".\u00a0For instance, one major thing that I got from the sequences is that \"rationalists should win\". That I should be able to get what I want by being clever, and that I shouldn't often be in situations where I'm not getting what I want. And I should be indignant about them.\u00a0[Note: when I say \"indignant\" here, I don't mean an attitude of \"this is unfair. Now I will mope.\" It was more like, \"I'm better than this. I will now rise above.\"]In 2015, this became a sort of mantra for me.\u00a0In places where other people would be satisfied with the lemons that life gave them, _I_ should win.\u00a0The actual post where it says that rationalists should win is about Newcobm's problem: it's a [reasonably] abstract point about the philosophy of rational choice.\u00a0I mostly wasn't, in this instance, making use of the explicitly stated content. It's not like I was using some _specific_ reasoning technique that is more effective than baseline, and that's why I would win.\u00a0I was adopting an attitude that I picked up on.\u00a0(Compare this to the case where a person reads the sequences and starts doing explicit Bayesian calculations, [edit: regularly], and expects to do better that way. In doing that, a person is responding more to the content, and less to the vibe.)\u00a0I want to emphasize that there's a kind of arrogance, here, of \"I'm different than other people, such that they sometimes don't get what they want, but that is beneath my dignity\", which I totally had before reading the sequences.\u00a0I didn't get this entirely from Eliezer. Rather, Eliezer put out a thing that resonated with me.\u00a0The sequences matched with, and _justified_ some attitudes that I already had. And they inculcated some new attitudes in me, via mimesis.\u00a0That's not unique to me. Many of the people who were attracted to the sequences, were attracted by the _vibe_, mostly.\u00a0It isn't like people were ignoring the explicit content, but reading the sequences is FUN for some people.\u00a0Some of that is because it is just so fascinating.\u00a0But some of it is because you either resonate with, or enjoy, or feel ego-boosted, by the vibe.\u00a0You'll like the vibe of the sequences more if you like the the snark and the superior attitude, instead of people who were offended by it. Or if you are proud of your intelligence, maybe?\u00a0So the sequences predictably attracted some particular types of people:\u00a0Some people who are smart and non-functional, and sincerely see \"rationality\" as a way to turn their life around, which is comfortable for them, because it involves doing what they were already good at: school stuff / playing with ideas.Some people who are smart and non-functional, who are less trying to be better, and more looking for a way to feel like _they're_ actually the superior ones; everyone else is irrational, and that's what matters.Some people who want to feel important, and found this x-risk narrative as a way to feel important, or have meaning.Some people who resonate a lot with \"thinking better is the path to success\", because that has actually worked for them in their experience.Lots and lots of people who were nerd-sniped by the ideas, and find social satisfaction from the rationalist social game: jockeying with each other to show how much you know by bringing to bear community shibboleths and/or generically \"smart frames\" on a question.The \"people pick up on the vibe of the sequences\" effect is magnified, because if you went to a rationalist meetup, it would be filled with people who self selected on the basis of resonating with or liking the vibe of the sequences.\u00a0**And if the explicit content deviates from the vibe, the content sometimes gets lost or muddled.**\u00a0[The paragraph below] explicitly says \"Don't be a straw Vulcan. Rationality is not about being emotionless!\"\u00a0But also, the energetic \"feel\" of this paragraph, for me, is abstracted, and cognitive, and kind of keeping the emotions at arms length.\u00a0And people will often interpret sentences like these according to the vibe that they project on to them.And the result is that people say words like \"rationality is not about only using your S2\", but ALSO they mostly end up trying to do things with explicit S2 thinking.\u00a0People manage to do things like ignoring their emotions (while also thinking they're awesome at emotions).\u00a0And it is pretty hard to notice if you're doing something like this!\u00a0So \"rationality\" for a lot people, is, for better or for worse, largely about that vibe.\u00a0(Especially, though not uniquely, for people who are far from the in person community. People who live in the Bay have more feedback channels with how this rationality stuff lives in the likes of Eliezer, and Anna.)\u00a0And sometimes people live by that ethos for a while, and then find that it doesn't serve them.\u00a0Like, the stance that they're taking towards the world, or towards themselves, isn't good for them, and doesn't help them get what they want.\u00a0The way they relate with their emotions is bad for them.\u00a0Or the way they \"overthink\" things is neurotic instead of helpful.\u00a0Or the way that they intellectualize everything is isolating. They're not actually satisfied with the human connection that they get that way.\u00a0And so they reject the ethos which they've come to associate with \"rationality\".\u00a0(Which, they probably learned, in part, from the sequences, and in part had all along.\u00a0And, even if they were doing it all along, rationality culture probably gave them some new and better tools and justifications for hurting themselves.)\u00a0But when they reject the rationality ethos, something kind of funny happens, because every time they make a specific claim about what they're rejecting, some rationalist (like me!) will pop up and point out that the specific claim that they're making about why rationality is not adequate is not only a thing that rationalists are aware of, but is RIGHT THERE in the sequences!\u00a0They'll say something like \"rationality is about thinking through everything, explicitly, and that's not actually effective.\"\u00a0And then I and others will face-palm and be like \"did you READ the sequences? Attend a CFAR workshop? We say in a like a hundred places that the point is not to reason through everything explicitly!\"https://twitter.com/ESYudkowsky/status/1462132727583440896?s=20&t=JWoyf4oI_wHmM3ZJgf_pfgBut that's ok. People picked up on a vibe, and the way they embodied that vibe didn't get them what they valued, and so they're correctly pushing back against [something] that they don't want.\u00a0In summary:\u00a0It's all good. People should do things that are good for them.\u00a0Communication is hard.\u00a0It might be nice if people were more curious about how \"rationality\" sits in others, instead of fighting about what it is or isn't. But no one has to do that either. : )", "title": "Twitter thread on postrationalists", "karma": 111, "af": false, "userid": "PWGv3R24uH9pvCnZm", "id": "evYFijNMdjfbPaCho"}, {"text": "In my previous\u00a0post\u00a0I talked about read-eval-print loops as providing a type signature for agents. I will now explain how you can quickly transition from this framework to an ELK solution. Notation is imported from that post.Imagine we have two agents, a human and a strong AI, denoted H and M respectively. They both interact with the environment in lockstep, according to the following diagram.We have the human's utility function\u00a0UH:SH\u2192Q, which is defined on the human's model of reality. We would like to lift\u00a0UH\u00a0to a version\u00a0UM:SM\u2192Q\u00a0that the machine can use to influence the world in way that is agreeable to the human, which we can do by learning a mapping F :\u00a0SM\u2192SH\u00a0and deriving\u00a0UM=F\u2218UH.But we haven't yet said what properties we want the ontology map F to have. I want to call two concepts\u00a0sh\u00a0and\u00a0sm\u00a0equal if they act the same with respect to transformation:\u00a0\u2200f. f(sh) = f(sm) \u2192\u00a0sh\u00a0=\u00a0sm. The issue is that since the concepts have different types we cannot feed them as arguments to the same function. So instead let's say that\u00a0\u2200s:S,\u00a0EvalH(sh,\u00a0ReadH(s)) =\u00a0EvalM(sm,\u00a0ReadM(s)) \u2192\u00a0sh\u00a0=\u00a0sm. But now we are back to the same problem where we are trying to compare concepts in two different ontologies. But this does give us a kind of inductive step where we can transfer evidence of equality between concept pairs (sh,\u00a0sm) and (sh',\u00a0sm'). I also believe that this kind of a coherence argument is the best we can do, since we are not allowed to peer into the semantic content of particular machine or human states when constructing the ontology map.Consider the following two graphs.My intuition is that even if I don't know the labels of the above graphs, I can still infer that the bottom nodes correspond to each other. And the arrows that I get in the context of ELK are the agents' Eval transitions, leading to the following commutative diagram specification for F.We can learn an ontology map F :\u00a0SM\u2192SH\u00a0by minimizing the difference between two paths from a state\u00a0sm, one in which the machine's prediction function is used and one in which the human's prediction function is used. \u00a0Concretely, I propose minimizing Dist(sh1,sh2) +\u00a0\u03bb\u00a0|U(sh1)-U(sh2)| where\u00a0sh1\u00a0= F(EvalM(sm,om)) and\u00a0sh2\u00a0=\u00a0EvalH(F(sm),oh), Dist is a distance metric in\u00a0SH, and observations\u00a0om\u00a0and\u00a0oh\u00a0are generated by the same underlying state S.If you are interested in getting more detail and why I believe this circumvents existing counterexamples, please check out the full\u00a0proposal.\u00a0", "title": "REPL's and ELK", "karma": 7, "af": true, "userid": "r7nqCPcgWSe6bXkct", "id": "C5PZNi5fueH2RC6aF"}]