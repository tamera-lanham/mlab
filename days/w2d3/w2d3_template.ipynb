{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking encoding:\n",
      "Congrats! You've passed the test!\n",
      "Checking new key and value:\n",
      "Congrats! You've passed the test!\n",
      "Congrats! You've passed the test!\n",
      "Checking logits:\n",
      "Congrats! You've passed the test!\n",
      "Checking final encodings:\n",
      "Congrats! You've passed the test!\n",
      "Congrats! Your GPT returns the same results with and without cache.\n",
      "It took 1.764s to generate a 500-token sentence without cache and 0.559s with cache.\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import einops\n",
    "import gpt_tests\n",
    "from gpt_sol import UniAttention as UniAttentionSolution\n",
    "from _gpt_sol import _UnidirectionalAttention, _GPT2Block, _GPT2\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from torchtyping import TensorType\n",
    "\n",
    "import sys\n",
    "sys.path.append('../w2d1')\n",
    "from bert_sol import Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upper_right_mask(x: t.Tensor):\n",
    "    return t.tril(x, 0) + (t.triu(t.ones(x.shape), 1) * -1e4)\n",
    "\n",
    "class UniMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.attn_ll = nn.Linear(hidden_size, hidden_size*3)\n",
    "        self.output_ll = nn.Linear(hidden_size, hidden_size)\n",
    "        self.head_size = hidden_size // num_heads\n",
    "        self.hidden_size = hidden_size # embedding size\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        \n",
    "    def forward(self, x: t.Tensor): # [batch, seq_len, hidden_size]\n",
    "        batch, seq_len, _ = x.shape\n",
    "        qkv = self.attn_ll(x) # [batch, seq_len, 3 * hidden_size]\n",
    "        \n",
    "        q, k, v = einops.rearrange(qkv, 'b s (three e) -> three b e s', three=3) # e for embedding size (which is hidden size)\n",
    "        #q, k, v = t.split(qkv, self.hidden_size, dim=-1)\n",
    "        q, k, v = [einops.rearrange(m, 'b (n h) s -> b n h s', n=self.num_heads) for m in (q, k, v)]\n",
    "        \n",
    "        raw_score = t.einsum('bnhs,bnhz->bnsz', k, v)\n",
    "        \n",
    "        scaled_score = raw_score / math.sqrt(self.head_size)\n",
    "        \n",
    "        masked_score = upper_right_mask(scaled_score) \n",
    "        \n",
    "        softmaxed_score = masked_score.softmax(-1) # batch, num_heads, seq_len, seq_len\n",
    "        \n",
    "        Z = t.einsum('bnsz,bnhz -> bnhs', softmaxed_score, v)\n",
    "        Z = einops.rearrange(Z, 'b n h s -> b s (n h)')\n",
    "        \n",
    "        output = self.output_ll(Z)\n",
    "        \n",
    "        return output\n",
    "        # WhatWeWant = Z * WO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, seq_len, hidden_size = 1, 4, 100\n",
    "x = t.randn((batch, seq_len, hidden_size))\n",
    "\n",
    "module = UniMultiHeadAttention(hidden_size, 5)\n",
    "output = module(x)\n",
    "\n",
    "out, out2 = gpt_tests.test_unidirectional_attn(UniMultiHeadAttention)\n",
    "#output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.allclose(out, out2, atol=1e-4, rtol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.equal(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT Block\n",
    "\n",
    "# The GPT-2 order is: LayerNorm -> Attention -> Residual -> LayerNorm \n",
    "# -> Linear[hidden_size, hidden_size * 4] -> GELU -> Linear[hidden_size * 4, hidden_size] -> \n",
    "# Dropout -> Residual.\n",
    "\n",
    "import pdb\n",
    "\n",
    "class GPTBlock(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_heads: int, dropout: float, layer_norm_epsilon: float, uni_attn_block=UniAttentionSolution): \n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = hidden_size // num_heads\n",
    "        self.dropout = dropout\n",
    "        self.layer_n_eps = layer_norm_epsilon\n",
    "        \n",
    "        self.layer_norm_1 = nn.LayerNorm(normalized_shape = (self.hidden_size,), eps = self.layer_n_eps)\n",
    "        self.attention = uni_attn_block(self.hidden_size, self.num_heads)\n",
    "        self.layer_norm_2 = nn.LayerNorm(normalized_shape = (self.hidden_size,), eps = self.layer_n_eps)\n",
    "        \n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, self.hidden_size * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.hidden_size * 4, self.hidden_size),\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: t.Tensor): # [batch, seq_len, hidden_size]\n",
    "        \n",
    "        x_norm = self.layer_norm_1(x)\n",
    "        x_attn = self.attention(x_norm)\n",
    "        x_resid = x + x_attn\n",
    "        x_norm2 = self.layer_norm_2(x_resid)\n",
    "        x_mlp = self.MLP(x_norm2)\n",
    "        x_dropout = self.dropout(x_mlp)\n",
    "        output = x_resid + x_dropout \n",
    "\n",
    "        return output\n",
    "\n",
    "num_heads = 5 \n",
    "dropout = 0.05\n",
    "# Parameters all screwed\n",
    "#gpt_block = GPTBlock(hidden_size, num_heads, dropout, layer_norm_epsilon = 1e-5, uni_attn_block=UniAttentionSolution)\n",
    "#norm_x_attn = gpt_block.forward(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gpt_block(GPT2Block):\n",
    "    kwargs = dict(hidden_size=48, layer_norm_epsilon=1e-4, dropout=0.0, num_heads=4)\n",
    "    x = t.randn(1, 5, 48)\n",
    "    \n",
    "    t.manual_seed(710)\n",
    "    _block = _GPT2Block(**kwargs)\n",
    "    _out = _block(x)\n",
    "    \n",
    "    t.manual_seed(710)\n",
    "    block = GPT2Block(**kwargs)\n",
    "    ours = block(x)\n",
    "    \n",
    "    return _out, ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "gpt_tests.test_gpt_block(GPTBlock)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2\n",
    "GPT-2 has the following architecture:\n",
    "Embedding -> Dropout -> N times GPT-2 Block -> LayerNorm -> Unembedding\n",
    "\n",
    "The embedding layer takes into account only input_ids and position (i.e. no token_type_id).\n",
    "We will not have an explicit unembedding layer. Instead, weâ€™ll use the same matrix as in the embedding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://realpython.com/python-data-classes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPT2Output:\n",
    "    logits: TensorType[\"batch_size\", \"vocab_size\"]\n",
    "    final_encoding: TensorType[\"batch_size\", \"hidden_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem = lambda n=3: t.cuda.memory_allocated() / 2**(10*n)\n",
    "\n",
    "mem(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem = lambda n=3: 5*2**30 / 2**(10*n)\n",
    "mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2(nn.Module):\n",
    "    def __init__(self, num_layers, num_heads, vocab_size, hidden_size, max_position_embeddings, dropout, layer_norm_epsilon):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.num_heads = num_heads\n",
    "        self.norm_eps = layer_norm_epsilon\n",
    "        self.num_blocks = num_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = hidden_size\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.positional_embedding = nn.Embedding(self.max_position_embeddings, self.embedding_size)\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(self.dropout)\n",
    "        self.GPT_blocks = nn.Sequential(\n",
    "            *[GPTBlock(self.embedding_size, self.num_heads, self.dropout, self.norm_eps) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm( (self.embedding_size,) ,eps = self.norm_eps)\n",
    "        \n",
    "    def forward(self, input_ids): # batch, seq_len\n",
    "        device = input_ids.device\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        # Create embeddings\n",
    "        token_embeddings = self.token_embedding(input_ids) \n",
    "        positional_embeddings = self.positional_embedding(t.arange(seq_len).repeat((batch_size, 1)).to(device))\n",
    "        embedded = token_embeddings + positional_embeddings\n",
    "        \n",
    "        dropped_embedded = self.dropout_layer(embedded)\n",
    "        pre_norm = self.GPT_blocks(dropped_embedded)\n",
    "        pre_unembedding = self.layer_norm(pre_norm)\n",
    "        \n",
    "        self._enc = pre_norm\n",
    "        \n",
    "        unembedded = pre_unembedding @ self.token_embedding.weight.T\n",
    "        return GPT2Output(unembedded[:,-1,:], pre_unembedding[:,-1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Embedding` layer has two parts:\n",
    "    - One-hot encoding\n",
    "    - `nn.Linear`\n",
    "    \n",
    "One-hot encoding needs to know the max value of any number in the input tensor\n",
    "    - Given an input of shape (batch_size, seq_len) -> output of shape (batch_size, seq_len, max_value)\n",
    "    \n",
    "`nn.Linear` does the rest\n",
    "    - (batch_size, seq_len, max_value) -> (batch_size, seq_len, embedding_size)\n",
    "    - The weight matrix for this layer is what Illustrated GPT2 calls (wte) or (wpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(num_layers=2, num_heads=4, vocab_size=100, hidden_size=64,\n",
    "              max_position_embeddings=32, dropout=0.0, layer_norm_epsilon=1e-4)\n",
    "\n",
    "gpt_ours = GPT2(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking logits:\n",
      "Congrats! You've passed the test!\n",
      "Checking final encodings:\n",
      "Congrats! You've passed the test!\n"
     ]
    }
   ],
   "source": [
    "gpt_tests.test_gpt(GPT2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gpt = GPT2(num_layers=12, num_heads=12, vocab_size=50257, hidden_size=768, max_position_embeddings=1024, dropout=0.1, layer_norm_epsilon=1e-5)\n",
    "\n",
    "pretrained_gpt, common_tokenizer = gpt_tests.get_pretrained_gpt_and_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _ in enumerate(tuple(zip(pretrained_gpt.state_dict(),my_gpt.state_dict()))):\n",
    "#     print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_texts = [\n",
    "\"My life motto:\",\n",
    "\"My life motto: Fortune\",\n",
    "\"My life motto: Fortune favors\",\n",
    "\"My life motto: Fortune favors the\",\n",
    "\"My life motto: Fortune favors the bold\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common_tokenizer = pretrained_gpt.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common_tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rich', 'wealthy', 'good', 'underdog', 'few']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def ascii_art_gpt(model, text, k=5, tokenizer=common_tokenizer):\n",
    "    tokenized_text = common_tokenizer(text)\n",
    "    preprocessed = t.tensor(tokenized_text['input_ids']).unsqueeze(0)\n",
    "        \n",
    "    output = model(preprocessed)\n",
    "    top_k_list = t.topk(output.logits[0,-1], k).indices.squeeze().tolist()  \n",
    "\n",
    "    tokens = common_tokenizer.convert_ids_to_tokens(top_k_list)\n",
    "    return [token.replace('Ä ', '') for token in tokens]\n",
    "\n",
    "ascii_art_gpt(pretrained_gpt, 'My life motto: Fortune favors the')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/transformers/issues/9362\n",
    "\n",
    "https://discuss.huggingface.co/t/tokenizer-taking-lot-of-memory/8597"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_bert = Bert(num_layers=12, num_heads=12, vocab_size=50257, hidden_size=768, max_position_embeddings=1024, dropout=0.1, type_vocab_size = 2, intermediate_size = 3072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = common_tokenizer(generated_texts)['input_ids']\n",
    "ten_zeros = [0] * 10\n",
    "\n",
    "for el in input_ids: \n",
    "    remaining_zeros = 10 - len(el)\n",
    "    for z in range(remaining_zeros):\n",
    "        el.append(0)\n",
    "        \n",
    "final_input_ids = t.Tensor(input_ids).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3063, -1.0202, -1.2014, -0.5042, -0.5798, -1.3563, -1.3400, -0.2594,\n",
       "         -0.1752, -0.7772],\n",
       "        [-0.2854, -1.0073, -1.1958, -0.5106, -0.8399, -1.3810, -1.3516, -0.2324,\n",
       "         -0.1648, -0.7812],\n",
       "        [-0.1917, -0.9239, -1.1180, -0.4416, -0.7737, -1.4737, -1.3105, -0.1507,\n",
       "         -0.0885, -0.6993],\n",
       "        [-0.0911, -0.8879, -1.0305, -0.3618, -0.6880, -1.4176, -1.7094, -0.0575,\n",
       "          0.0053, -0.6132],\n",
       "        [-0.1158, -0.9190, -1.0935, -0.4143, -0.7246, -1.4794, -1.7706, -0.2269,\n",
       "         -0.0032, -0.6541]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_bert.eval()(final_input_ids)\n",
    "our_bert._enc[:,:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1982e+00,  7.1118e-01, -1.3565e+00, -1.3015e+00, -2.1480e-01,\n",
       "          5.3286e-01, -3.6640e-01, -9.0771e-01, -9.0394e-01,  2.4541e-01],\n",
       "        [-1.1982e+00,  7.1118e-01, -1.3565e+00, -1.3015e+00,  1.3649e+00,\n",
       "          8.2624e-01, -1.4388e-01, -7.0532e-01, -6.9136e-01,  3.9569e-01],\n",
       "        [-1.1982e+00,  7.1118e-01, -1.3565e+00, -1.3015e+00,  1.3649e+00,\n",
       "          1.6376e-04, -9.0492e-02, -6.4807e-01, -5.9624e-01,  4.0160e-01],\n",
       "        [-1.1982e+00,  7.1118e-01, -1.3565e+00, -1.3015e+00,  1.3649e+00,\n",
       "          1.6376e-04,  1.3555e+00, -4.7671e-01, -5.1044e-01,  4.3125e-01],\n",
       "        [-1.1982e+00,  7.1118e-01, -1.3565e+00, -1.3015e+00,  1.3649e+00,\n",
       "          1.6376e-04,  1.3555e+00,  1.4337e+00, -3.0476e-01,  5.8670e-01]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_gpt.eval()(final_input_ids)\n",
    "my_gpt._enc[:,:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now in the next iteration, when the model processes the word robot, it does not need to \n",
    "# generate query, key, and value queries for the a token. It just reuses the ones it saved from the first iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Bert(nn.Module):\n",
    "#     def __init__(self, vocab_size, hidden_size, max_position_embeddings, type_vocab_size,\n",
    "#                  dropout, intermediate_size, num_heads, num_layers):\n",
    "#         super().__init__()\n",
    "#         self.embed = BertEmbedding(vocab_size, hidden_size, max_position_embeddings,\n",
    "#                                    type_vocab_size, dropout)\n",
    "#         self.blocks = nn.Sequential(*[\n",
    "#             BertBlock(hidden_size, intermediate_size, num_heads, dropout)\n",
    "#             for _ in range(num_layers)\n",
    "#         ])\n",
    "#         self.lin = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "#         self.unembed = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "\n",
    "#     def forward(self, input_ids):\n",
    "#         token_type_ids = t.zeros_like(input_ids, dtype=int)\n",
    "#         emb = self.embed(input_ids, token_type_ids)\n",
    "#         self._enc = enc = self.blocks(emb)\n",
    "#         enc = self.lin(enc)\n",
    "#         return self.unembed(self.layer_norm(F.gelu(enc)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
