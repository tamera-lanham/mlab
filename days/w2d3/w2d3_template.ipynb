{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import einops\n",
    "import gpt_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "def upper_right_mask(x: t.Tensor):\n",
    "    return t.tril(x, 0) + (t.triu(t.ones(x.shape), 1) * -1e4)\n",
    "\n",
    "def lower_left_mask(x: t.Tensor):\n",
    "    return t.triu(x, 0) + (t.tril(t.ones(x.shape), -1) * -1e4)\n",
    "\n",
    "class UniMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.attn_ll = nn.Linear(hidden_size, hidden_size*3)\n",
    "        self.output_ll = nn.Linear(hidden_size, hidden_size)\n",
    "        self.head_size = hidden_size // num_heads\n",
    "        self.hidden_size = hidden_size # embedding size\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        \n",
    "    def forward(self, x: t.Tensor): # [batch, seq_len, hidden_size]\n",
    "        batch, seq_len, _ = x.shape\n",
    "        qkv = self.attn_ll(x) # [batch, seq_len, 3 * hidden_size]\n",
    "        \n",
    "        q, k, v = einops.rearrange(qkv, 'b s (three e) -> three b e s', three=3) # e for embedding size (which is hidden size)\n",
    "        q, k, v = [einops.rearrange(m, 'b (h n) s -> b n h s', n=self.num_heads) for m in (q, k, v)]\n",
    "        #pdb.set_trace()\n",
    "        \n",
    "        raw_score = t.einsum('bnhs,bnhz->bnsz', k, v)\n",
    "        \n",
    "        scaled_score = raw_score / math.sqrt(self.head_size)\n",
    "        \n",
    "        masked_score = upper_right_mask(scaled_score) \n",
    "        \n",
    "        softmaxed_score = masked_score.softmax(-1) # batch, num_heads, seq_len, seq_len\n",
    "        \n",
    "        Z = t.einsum('bnsz,bnhz -> bnhs', softmaxed_score, v)\n",
    "        Z = einops.rearrange(Z, 'b n h s -> b s (n h)')\n",
    "        \n",
    "        output = self.output_ll(Z)\n",
    "        \n",
    "        return output, qkv, k, q, v, masked_score, softmaxed_score, Z\n",
    "        # WhatWeWant = Z * WO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = t.arange(1, 26).reshape((5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5],\n",
       "        [ 6,  7,  8,  9, 10],\n",
       "        [11, 12, 13, 14, 15],\n",
       "        [16, 17, 18, 19, 20],\n",
       "        [21, 22, 23, 24, 25]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04],\n",
       "        [ 6.0000e+00,  7.0000e+00, -1.0000e+04, -1.0000e+04, -1.0000e+04],\n",
       "        [ 1.1000e+01,  1.2000e+01,  1.3000e+01, -1.0000e+04, -1.0000e+04],\n",
       "        [ 1.6000e+01,  1.7000e+01,  1.8000e+01,  1.9000e+01, -1.0000e+04],\n",
       "        [ 2.1000e+01,  2.2000e+01,  2.3000e+01,  2.4000e+01,  2.5000e+01]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_right_mask(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, seq_len, hidden_size = 1, 4, 100\n",
    "x = t.randn((batch, seq_len, hidden_size))\n",
    "\n",
    "module = UniMultiHeadAttention(hidden_size, 5)\n",
    "output = module(x)\n",
    "\n",
    "(output, qkv, k, q, v, masked_score, softmaxed_score, Z), (true_output, qkv2, k2, q2, v2, attn_scores, attn_prob, combined_v) = gpt_tests.test_unidirectional_attn(UniMultiHeadAttention)\n",
    "#output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 24, 5])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv.shape\n",
    "einops.rearrange(qkv, 'b s (three e) -> three b e s', three=3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv2.shape\n",
    "t.split(qkv2, 24, dim=-1)[0].equal(q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "einops.rearrange(k, 'b n h s -> b n s h').equal(k2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.5433, -0.3950,  0.0243,  0.1633, -0.4108],\n",
       "          [-0.6218,  0.1771, -0.1220,  0.0649,  0.8572],\n",
       "          [-0.1799, -1.3417, -0.3185,  0.5328,  0.1256],\n",
       "          [-0.1425, -0.0530, -0.4618, -0.2510,  1.0349],\n",
       "          [-0.4637,  0.9413, -0.5275,  0.0479, -0.7248],\n",
       "          [ 0.6505,  0.5064, -0.6061, -0.4649,  0.7123]],\n",
       "\n",
       "         [[ 0.6235, -0.0756, -0.1040,  0.4833,  0.7994],\n",
       "          [-0.5811, -0.9073, -0.0224,  0.3460, -0.0421],\n",
       "          [ 0.1105, -0.3790, -0.0938, -0.8144,  0.4575],\n",
       "          [ 0.0680,  0.6321,  0.4341,  0.0670,  0.4790],\n",
       "          [-0.3622, -0.2336,  0.8287,  0.1245,  0.1725],\n",
       "          [ 0.6786,  0.2461, -0.2254,  0.1167, -0.1402]],\n",
       "\n",
       "         [[-1.0921, -0.0949,  0.8208,  0.6795,  0.4688],\n",
       "          [-0.2159,  0.2922,  1.1232,  0.1396,  0.1746],\n",
       "          [-0.4640, -0.0530, -0.6113, -0.0201,  0.2239],\n",
       "          [ 0.0427, -1.4614, -0.2217, -0.0060,  0.2423],\n",
       "          [-0.8837, -0.4219, -1.8101, -1.1027, -0.0943],\n",
       "          [ 0.0610, -0.6829,  0.0539, -0.4457, -0.1325]],\n",
       "\n",
       "         [[ 0.6205,  0.3078, -1.2910, -0.2533,  0.4010],\n",
       "          [ 0.4255, -0.3060, -0.3564,  1.6150,  0.2288],\n",
       "          [ 0.6904, -0.0963, -0.1102,  1.2167,  0.3959],\n",
       "          [-0.1137, -1.6315,  1.5536,  0.7757, -0.7889],\n",
       "          [-0.1159, -0.5503,  0.3296, -0.1090, -0.3597],\n",
       "          [-0.5852, -1.5254,  0.9447,  0.4367, -0.1468]]]],\n",
       "       grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.5433,  0.6235, -1.0921,  0.6205, -0.6218, -0.5811],\n",
       "          [-0.3950, -0.0756, -0.0949,  0.3078,  0.1771, -0.9073],\n",
       "          [ 0.0243, -0.1040,  0.8208, -1.2910, -0.1220, -0.0224],\n",
       "          [ 0.1633,  0.4833,  0.6795, -0.2533,  0.0649,  0.3460],\n",
       "          [-0.4108,  0.7994,  0.4688,  0.4010,  0.8572, -0.0421]],\n",
       "\n",
       "         [[-0.2159,  0.4255, -0.1799,  0.1105, -0.4640,  0.6904],\n",
       "          [ 0.2922, -0.3060, -1.3417, -0.3790, -0.0530, -0.0963],\n",
       "          [ 1.1232, -0.3564, -0.3185, -0.0938, -0.6113, -0.1102],\n",
       "          [ 0.1396,  1.6150,  0.5328, -0.8144, -0.0201,  1.2167],\n",
       "          [ 0.1746,  0.2288,  0.1256,  0.4575,  0.2239,  0.3959]],\n",
       "\n",
       "         [[-0.1425,  0.0680,  0.0427, -0.1137, -0.4637, -0.3622],\n",
       "          [-0.0530,  0.6321, -1.4614, -1.6315,  0.9413, -0.2336],\n",
       "          [-0.4618,  0.4341, -0.2217,  1.5536, -0.5275,  0.8287],\n",
       "          [-0.2510,  0.0670, -0.0060,  0.7757,  0.0479,  0.1245],\n",
       "          [ 1.0349,  0.4790,  0.2423, -0.7889, -0.7248,  0.1725]],\n",
       "\n",
       "         [[-0.8837, -0.1159,  0.6505,  0.6786,  0.0610, -0.5852],\n",
       "          [-0.4219, -0.5503,  0.5064,  0.2461, -0.6829, -1.5254],\n",
       "          [-1.8101,  0.3296, -0.6061, -0.2254,  0.0539,  0.9447],\n",
       "          [-1.1027, -0.1090, -0.4649,  0.1167, -0.4457,  0.4367],\n",
       "          [-0.0943, -0.3597,  0.7123, -0.1402, -0.1325, -0.1468]]]],\n",
       "       grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv.equal(qkv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "einops.rearrange(k, 'b n h s -> b n s h')#.equal(k2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = t.tensor([-6.7706e-01, -1e-4, -1e-4, -1e-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example.softmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, k, v = einops.rearrange(qkv, 'b s (three h) -> three b s h', three=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
