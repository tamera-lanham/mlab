{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from run import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_batches(list_of_tensors, batch_size):\n",
    "    n_batches = math.ceil(len(list_of_tensors) / batch_size)\n",
    "    batches = [list_of_tensors[i:i+batch_size] for i in range(n_batches)]\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = import_lesswrong_corpus()\n",
    "list_of_tensors = corpus_to_tensor_list(corpus)\n",
    "\n",
    "mini_batch_size = 16\n",
    "batch_size = mini_batch_size*2\n",
    "batches = to_batches(list_of_tensors, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = t.stack(batches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(batches) == list\n",
    "assert len(batches) == math.ceil(len(list_of_tensors) / batch_size)\n",
    "assert len(batches[0]) == batch_size\n",
    "assert batches[0][0].shape == list_of_tensors[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3718 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "corpus = import_lesswrong_corpus('small_lw_corpus.json')\n",
    "list_of_tensors = corpus_to_tensor_list(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_jsonl(filename='lw_corpus.jsonl'):\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line.strip())\n",
    "    \n",
    "def corpus_tokens(corpus, tokenizer):\n",
    "    for d in corpus:\n",
    "        yield tokenizer(d['text']).input_ids\n",
    "\n",
    "def buffer(iterable_of_lists):\n",
    "    iterator = iter(iterable_of_lists)\n",
    "    buffer_storage = []\n",
    "    \n",
    "    def add_to_buffer():\n",
    "        nonlocal buffer_storage\n",
    "        try: buffer_storage += next(iterator)\n",
    "        except: return True\n",
    "    \n",
    "    def get(n):\n",
    "        nonlocal buffer_storage\n",
    "        while len(buffer_storage) < n:\n",
    "            done = add_to_buffer()\n",
    "            if done: break\n",
    "            \n",
    "        n_elements, buffer_storage = buffer_storage[:n], buffer_storage[n:]\n",
    "        return n_elements\n",
    "    \n",
    "    return get\n",
    "\n",
    "def corpus_to_tensors(corpus, max_tokens=1024):\n",
    "    tokenizer = transformers.GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "    token_buffer = buffer(corpus_tokens(corpus, tokenizer))\n",
    "    \n",
    "    while True:\n",
    "        tokens = token_buffer(max_tokens)\n",
    "        if not tokens: break\n",
    "        yield t.tensor(tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "corpus = corpus_jsonl()\n",
    "corpus_2 = import_lesswrong_corpus()\n",
    "t_iter = corpus_to_tensors(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_iter_2 = corpus_to_tensors(corpus_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3712 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "t_list = [next(t_iter_2) for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024]),\n",
       " torch.Size([1024])]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in t_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = token_buffer(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   40,  1101,   257,  1987,   614,  1468, 31350, 33013,  1762,   287,\n",
       "         2831,    11,   290,   625,   262,  1613,   614,   314,   423, 49237,\n",
       "          284,   257,   517, 10373,    12, 11813,  2597,    13,   314,  1975,\n",
       "          326,  9552, 19114,   318,   845,  1593,    11,   290,   616,   890,\n",
       "         3381,  3451,  3061,   318,   284,  1037,  8676,   284, 19114,  2267,\n",
       "           13,    40,  1053,   655,   587,  4438,   281,   406,    19,  4572,\n",
       "         4673, 11949,  2292,   379,  3012,    11,   290,   423,  5982,   262,\n",
       "         1074, 12336,  7108,    13, 11259,   616,   890,  3381,  3061,    11,\n",
       "          389,   612,  3489,  7747,   329,  3466,   326,   561,   307,   749,\n",
       "         5035,    30,  3666,  1459,  3164,   318,   284,  2989,   262,  3012])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = corpus_iter(corpus[:20], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(ci)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer('hello world')['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1, 2, 3, 4, 5]\n",
    "l[8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from run import corpus_to_tensor_list, import_lesswrong_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = import_lesswrong_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_list = corpus_to_tensor_list(corpus, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tensor in t_list:\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tensor)\n",
    "    print([token.replace('Ä ', '') for token in tokens], len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cuda.get_device_name(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
